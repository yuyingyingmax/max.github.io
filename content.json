[{"title":"【LeetCode】5250.检查好数组","date":"2019-11-07T14:45:18.000Z","path":"2019/11/07/leetcode-5250/","text":"给你一个正整数数组 nums，你需要从中任选一些子集，然后将子集中每一个数乘以一个 任意整数，并求出他们的和。 假如该和结果为 1，那么原数组就是一个「好数组」，则返回 True；否则请返回 False。 示例 1： 输入：nums = [12,5,7,23]输出：true解释：挑选数字 5 和 7。53 + 7(-2) = 1 示例 2： 输入：nums = [29,6,10]输出：true解释：挑选数字 29, 6 和 10。291 + 6(-3) + 10*(-1) = 1 示例 3： 输入：nums = [3,6]输出：false 提示： 1 &lt;= nums.length &lt;= 10^51 &lt;= nums[i] &lt;= 10^9 题解分析这是第161场周赛的第3题。没来得及做。 本次周赛一道都没做出来（真菜）。 论文看完了，来做最后一题了，联合抽取好难啊….. 解法1 - 裴蜀定理这道题真是没想明白，刚好看了LeetCode的官方解，总结搬砖一下。 这是一道数论的题目，同学们平时一般接触比较少，这也能理解为什么这道题会被放到 hard 难度。但是一旦知道了知识点，这道题目马上就退化成 easy 了。 我们知道，对于两个不为 1 的正整数，如果他们互斥，则他们的最大公约数为 1。但是如果你了解 裴蜀定理[1]，就会知道还有另外一个定理是 如果两个数 x，y 互斥，则存在整数 a，b，使得 ax + by = 1。 而推广到 n 个整数，则： 如果 n 个数的最大公约数为 1，则存在整数 a，b，…使得 ax + by + … = 1。 那么问题就很简单了，我们直接求 n 个数的最大公约数，然后判断是不是等于 1 就好了。 求最大公约数时间复杂度为 O(m)（辗转相除法），要求 n 次，所以总体时间复杂度为 O(nlog(m))，其中 m 为数字大小。 ps. 辗转相除法的时间复杂度为$log(m)$，可以用数学公式证明。 代码 1234567891011public boolean isGoodArray(int[] nums)&#123; int result = nums[0]; for(int i = 1; i &lt; nums.length; i++)&#123; result = gcd(result,nums[i]); &#125; return result == 1; &#125; private int gcd(int a, int b)&#123; return b == 0 ? a : gcd(b, a % b); &#125; 复杂度分析 时间复杂度：$O(nlog(m))$ ,如上所述空间复杂度：$O(1)$"},{"title":"【LeetCode】5249.移除无效的括号","date":"2019-11-03T08:29:36.000Z","path":"2019/11/03/leetcode-5249/","text":"5249.移除无效的括号给你一个由 ‘(‘、’)’ 和小写字母组成的字符串 s。 你需要从字符串中删除最少数目的 ‘(‘ 或者 ‘)’ （可以删除任意位置的括号)，使得剩下的「括号字符串」有效。 请返回任意一个合法字符串。 有效「括号字符串」应当符合以下 任意一条 要求： 空字符串或只包含小写字母的字符串 可以被写作 AB（A 连接 B）的字符串，其中 A 和 B 都是有效「括号字符串」 可以被写作 (A) 的字符串，其中 A 是一个有效的「括号字符串」 示例 1： 输入：s = “lee(t(c)o)de)”输出：”lee(t(c)o)de”解释：”lee(t(co)de)” , “lee(t(c)ode)” 也是一个可行答案。 示例 2： 输入：s = “a)b(c)d”输出：”ab(c)d” 示例 3： 输入：s = “))((“输出：””解释：空字符串也是有效的 示例 4： 输入：s = “(a(b(c)d)”输出：”a(b(c)d)” 提示： 1 &lt;= s.length &lt;= 10^5s[i] 可能是 ‘(‘、’)’ 或英文小写字母 题解分析这是第161场周赛的第3题。没来得及做。 本次周赛一道都没做出来（真菜）。 其实前三题都不算难，最后一道题看完论文做（Flag） 论文看完了，来做最后一题了，联合抽取好难啊….. 解法1 - 括号匹配还是用栈来做匹配，遇到左括号入栈，遇到右括号时如果栈不为空则出栈，如果栈为空，那么此时对应的右括号需要被移除。遍历完字符串，如果栈不为空，那么栈中对应的左括号也需要被移除。 ps : 我们在栈中只需要存放左括号的索引即可。 代码 12345678910111213141516171819202122public String minRemoveToMakeValid(String s) &#123; Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(); char[] str = s.toCharArray(); for (int i = 0; i &lt; s.length(); i++) &#123; char ch = s.charAt(i); switch (ch) &#123; case '(': stack.push(i); break; case ')': if (!stack.isEmpty()) stack.pop(); else str[i] = ' '; break; &#125; &#125; while (!stack.isEmpty()) &#123; str[stack.pop()] = ' '; &#125; String res = new String(str); res = res.replace(\" \", \"\"); return res;&#125; 复杂度分析： 时间复杂度：$O(n)$空间复杂度：$O(n)$"},{"title":"【LeetCode】5248.统计优美子数组","date":"2019-11-03T08:19:53.000Z","path":"2019/11/03/leetcode-5248/","text":"5248.统计优美子数组给你一个整数数组 nums 和一个整数 k。 如果某个子数组中恰好有 k 个奇数数字，我们就认为这个子数组是「优美子数组」。 请返回这个数组中「优美子数组」的数目。 示例 1： 输入：nums = [1,1,2,1,1], k = 3输出：2解释：包含 3 个奇数的子数组是 [1,1,2,1] 和 [1,2,1,1] 。 示例 2： 输入：nums = [2,4,6], k = 1输出：0解释：数列中不包含任何奇数，所以不存在优美子数组。 示例 3： 输入：nums = [2,2,2,1,2,2,1,2,2,2], k = 2输出：16 提示： 1 &lt;= nums.length &lt;= 500001 &lt;= nums[i] &lt;= 10^51 &lt;= k &lt;= nums.length 题解分析这是第161场周赛的第2题。我的解法超时。 本次周赛一道都没做出来（真菜）。 其实前三题都不算难，最后一道题看完论文做（Flag） 解法1 - 前缀和将数组中的奇数都当作1，偶数当作0，那么寻找子数组的问题可以转化为前缀和问题，具体地，用map来统计前缀和等于s的个数，就等于统计了子数组的个数。 具体过程模拟代码就能理解，（看完论文补图） 代码 1234567891011121314public int numberOfSubarrays2(int[] nums, int k) &#123; int s = 0;//前缀和 Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); map.put(s,1); int res = 0; for (int num : nums) &#123; if (num % 2 == 1) s++; if (s - k &gt;= 0) &#123; res += map.get(s - k); &#125; map.put(s,map.getOrDefault(s,0) + 1); &#125; return res; &#125; 复杂度分析： 时间复杂度：$O(n)$空间复杂度：$O(n)$"},{"title":"【LeetCode】5247.交换字符使得字符串相同","date":"2019-11-03T08:02:43.000Z","path":"2019/11/03/leetcode-5247/","text":"5247.交换字符使得字符串相同有两个长度相同的字符串 s1 和 s2，且它们其中 只含有 字符 “x” 和 “y”，你需要通过「交换字符」的方式使这两个字符串相同。 每次「交换字符」的时候，你都可以在两个字符串中各选一个字符进行交换。 交换只能发生在两个不同的字符串之间，绝对不能发生在同一个字符串内部。也就是说，我们可以交换 s1[i] 和 s2[j]，但不能交换 s1[i] 和 s1[j]。 最后，请你返回使 s1 和 s2 相同的最小交换次数，如果没有方法能够使得这两个字符串相同，则返回 -1 。 示例 1： 输入：s1 = “xx”, s2 = “yy”输出：1解释：交换 s1[0] 和 s2[1]，得到 s1 = “yx”，s2 = “yx”。 示例 2： 输入：s1 = “xy”, s2 = “yx”输出：2解释：交换 s1[0] 和 s2[0]，得到 s1 = “yy”，s2 = “xx” 。交换 s1[0] 和 s2[1]，得到 s1 = “xy”，s2 = “xy” 。注意，你不能交换 s1[0] 和 s1[1] 使得 s1 变成 “yx”，因为我们只能交换属于两个不同字符串的字符。 示例 3： 输入：s1 = “xx”, s2 = “xy”输出：-1 示例 4： 输入：s1 = “xxyyxyxyxx”, s2 = “xyyxyxxxyx”输出：4 提示： 1 &lt;= s1.length, s2.length &lt;= 1000s1, s2 只包含 ‘x’ 或 ‘y’。 题解分析这是第161场周赛的第一题。 本次周赛一道都没做出来（真菜）。 其实前三题都不算难，最后一道题看完论文做（Flag） 解法1 - 贪心法由于s1和s2需要完全相同，那么不相同时字符只有以下两种对应情况： x - yy - x 当我们有两个 x - y 或 y - x时，只需要一次变换就可以让他们相等： xxyy交换 s1[0] 和 s2[1] yyxx交换 s1[0] 和 s2[1] 当我们有1个x - y和一个y - x时，需要两次变换： xyyx交换s1[0]和s2[0]yyxx再做一次变换，交换s1[0]和s2[1] 如果我们只有一个x - y 或 y - x，那么不能完成变换，return -1 因此算法如下： 1.统计x - y 和 y - x的个数2.贪心查找2k个匹配的x- y 和 y - x3.接下来可能有3种情况： 两种模式各有一个；两种模式都没有；两种模式中只有一种还剩1。4.对于这种情况：各有一个的+2；都没有不用计算；只有一种还剩1，return - 1; 代码 1234567891011121314151617181920public int minimumSwap(String s1, String s2) &#123; int result = 0; int[] count = new int[2]; for (int i = 0; i &lt; s1.length(); i++) &#123; if (s1.charAt(i) == 'x' &amp;&amp; s2.charAt(i) == 'y') &#123; count[0]++; &#125; else if (s1.charAt(i) == 'y' &amp;&amp; s2.charAt(i) == 'x') &#123; count[1]++; &#125; &#125; //step1 find two x-y or y-x int res1 = count[0] / 2; count[0] -= res1 * 2; int res2 = count[1] /2; count[1] -= res2 * 2; result = res1 + res2; if(count[0] == 1 &amp;&amp; count[1] == 1) result += 2; else if (count[0] == 1 || count[1] == 1) return -1; return result;&#125; 复杂度分析： 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【论文笔记】End-to-End Relation Extraction using LSTMs on Sequences and Tree","date":"2019-11-02T06:24:19.000Z","path":"2019/11/02/Miwa-2016-note/","text":"摘要提出了新颖的端到端神经模型来提取实体及其之间的关系。我们的基于RNN的模型通过在双向sequence LSTM-RNNs上堆叠双向树状LSTM-RNNs来捕获单词序列和依存树结构信息。这使模型能够在单个模型通过共享的参数同时表示实体和关系。我们进一步鼓励在训练过程中检测实体，并通过实体预训练和计划抽样在关系抽取中使用实体信息。我们的模型在最先进的基于特征的端到端关系抽取模型之上进行改进，分别在ACE2005和ACE2004的F1评分达到了12.1%和5.7%的误差降低。在名词关系分类（SemEval-2010 Task 8）上，基于LSTMRNN的模型与基于CNN的最新模型相比具有优势。最后我们对几种模型组件进行了广泛的消融分析。 想解决什么问题？ question pipeline性能不够好；在关系分类任务中，基于LSTM的系统使用了有限的语言结构及神经结构，并且没对实体和关系联合建模，性能没有RNN好； 通过什么理论/模型来解决的？ method 双向LSTM （BiLSTM） 给出的答案是什么？ answer 这种模型能够在单个模型上通过共享参数同时表示实体和关系； 在基于特征的end2end关系抽取模型之上改进，降低了误差。 基于LSTMRNN的模型在名词关系分类上要好于CNN的； 结论我们提出了一种新颖的端到端关系抽取模型，该模型通过使用双向顺序和双向树结构的LSTM-RNN来同时表示单词序列和依存树结构。这使得我们能够在单个模型中表示实体和关系，在端到端关系抽取中，相对于最新的基于特征的系统有所改进(ACE04,ACE05)，并且比最新的基于CNN的名词关系分类有更好的性能。 我们的评估和消融实验得出3个重要的发现： 同时使用单词序列和依存树结构是有效的。 使用共享参数进行训练可以提高关系抽取的准确性，尤其是在与实体预训练，计划采样以及标签embedding配合使用时。 在关系分类中广泛使用的最短路径也适用于在LSTM模型中表示树结构。 不足之处 (补） 引言在信息抽取和NLP中，提取文本中实体间的语义关系是一项重要且经过充分研究的任务。 传统系统将此任务视为两个独立任务的pipeline，即命名实体识别(NER)(Nadeau and Sekine,2007; Ratinov and Roth,2009)以及关系抽取（Zelenko,2003;Zhou,2005）。但是最近的研究表明，实体和关系的end2end（联合）建模对于实现高性能非常重要，因为关系与实体信息是密切交互的（Li and Ji,2014; Miwa and Sasaki, 2014)。例如，要学习句子Toefting transferred to Bolton中Toefting和Bolton之间的Organization-Affiliation(ORG-AFF)关系, 实体信息Toefting是Person实体 和 Bolton 是 Organization实体就很重要。这些实体的提取反过来又受到上下文单词transfer to的鼓励，这些上下文词语表示了雇佣关系。之前的联合模型引入基于特征的结构化学习。这种end2end关系抽取任务的替代方法是通过基于神经网络(NN)模型自动学习特征。 有两种使用神经网络表示实体间关系的方式：RNN和CNN。其中，RNNs能够直接 表示基本的语言结构，即单词序列(Hammerton,2001)和组成/依赖树(Tai,2015)。尽管具有这种表示能力，对于关系分类任务来说，先前报告中使用基于RNN的LSTM(Xu,2015;Li,2015)性能要比CNNs(dos Santos, 2015)性能差。以前的这些基于LSTM的系统主要包括有限的语言结构以及神经结构，并且没有联合对实体和关系建模。通过基于包含补充语言结构的更丰富的LSTM RNN结构的实体和关系end2end建模，我们能够对最新模型进行改进。 单词序列和树结构是用来在抽取关系时做补充信息的。例如，在句子：&quot;This is...&quot;, one U.S. source said中，单词间的依赖不足以预测source 和 U.S.之间有 ORG-AFF关系，预测需要上下文单词said。 许多传统的基于特征的关系分类模型同时从序列和解析树中抽取特征（Zhou,2005)。然而，先前的基于RNN的模型只关注了这些语法结构中的一种。 我们提出了新颖的end2end模型，同时在单词序列和依存树结构上抽取实体间的关系。我们的模型允许通过使用双向序列（左到右，右到左）和双向树结构（下到上，上到下)的LSTM-RNN在单个模型中对实体和关系进行联合建模。模型首先检测实体，然后使用单个增量解码的神经网络结构提取检测到的实体间的关系，并且使用实体和关系标签联合更新神经网络参数。与传统增量式end2end关系抽取模型不同，我们的模型在训练中进一步整合了两个增强：实体预训练（对实体模型进行预训练）以及计划抽样（Bengio，2015），将（不可靠的）预测标签替换为有一定概率的gold标签。这些增强缓解了训练早期阶段实体探测的低性能问题，并允许实体信息进一步帮助下游关系分类。 在end2end关系抽取中，我们改进了最新的基于特征的模型，在F1得分上减少了12.1%(ACE2005),5.7%(ACE2004)的相关误差。在名词关系分类（SemEval-2010 Task 8）上，与最新的基于RNN的模型相比，在F1值上具有优势。最后，我们还消融和比较了我们的各种模型组件，这引出了一些关于贡献和有效性的关键发现（正面和负面）：不同RNN结构，输入依赖关系结构，不同解析模型，外部资源，联合学习设置。（在共享和有效性方面）。 为什么研究这个课题？ 1.传统pipeline分为NER和关系抽取，最近研究表明实体和关系的end2end建模对性能很重要，因为关系与实体式密切交互的。2.虽然RNN能表示语言结构（单词序列和组成/依赖树），但在关系分类任务中，之前使用基于RNN的LSTM性能没CNN好，这是因为LSTM没有联合对实体和关系建模，只包含有限的语言结构和神经结构。 因此我们通过包含补充语言结构的BiLSTM来做end2end的改进。 这个课题研究进行到哪一阶段？ 目前看来应该是做了这个双向LSTM（BiLSTM）并有性能提升。 作者使用理论基于哪些假设？ (补） 模型我们使用代表单词序列和依赖关系树结构的LSTM-RNN设计模型，并在这些RNN之上执行实体之间关系的端到端提取。图1展示了该模型的概况。 该模型主要由三个表示层组成：1.词嵌入层（embedding layer)2.基于LSTM-RNN的单词序列层（sequence layer）3.基于LSTM-RNN的依存子树层（dependency layer） 在解码期间，我们在序列层（sequence layer）建立贪婪的，从左到右的实体检测，并在依赖层（dependency layer）实现关系分类，其中每个基于LSTM-RNN的子树对应于两个检测到的实体之间的候选关系。 解码整个模型结构后，我们通过反向传播（BPTT）同时更新参数（Werbos，1990）。依赖层堆叠在序列层之上，因此实体检测和关系分类共享嵌入层和序列层，并且共享参数受实体和关系标签两者共同影响。 嵌入层 Embedding Layer嵌入层处理嵌入表示。$n_w,n_p,n_d,n_e$维向量$v^{(w)},v^{(p)},v^{(d)},v^{(e)}$分别嵌入单词(words),POS tags,依赖类型(dependency types)以及实体标签(entity labels). 嵌入层主要有4部分(1) word embedding(2) part-of-speech (POS,词性) embedding(3) dependency types embedding(4) entity labels embedding 序列层 Sequence Layer序列层使用嵌入层的表示形式以线性序列表示单词。该层表示句子上下文信息并且维护实体，如图一左下所示。 我们使用双向LSTM-RNNs表示句子中的单词序列（Graves，2013）。第t个单词处的LSTM单元由$n_{l_s}$维的向量集合组成：一个输入门$i_t$,一个遗忘门$f_t$,一个输出门$o_t$,一个记忆单元$c_t$, 隐藏层状态$h_t$。该单元接收一个n维输入向量$x_t$,前一时刻的隐层状态$h_{t-1}$，以及上一时刻记忆单元$c_{t-1}$，使用下列公式计算新的向量： 其中$\\sigma$表示logistic函数，$\\bigodot$表示逐元素乘法，$W$和$U$是权重矩阵，$b$是偏置向量。第t个单词处的LSTM单元接受单词和POS嵌入的串联作为其输入向量：$x_t = \\left[ v_t^{(w)};v_t^{(p)} \\right]$。我们还将每个单词对应的两个方向的LSTM单元的隐层状态向量（表示为 $\\overrightarrow{h_t}$ 和 $\\overleftarrow{h_t}$）串联起来作为输出向量，$s_t = \\left[\\overrightarrow{h_t};\\overleftarrow{h_t}\\right] $，并将它传递到随后的层。 实体检测 Entity Detection我们将实体检测作为序列标注任务。 我们使用BILOU编码模式(Begin, Inside, Last, Outside, Unit)为每个单词分配实体标签，该模式中每个实体标签表达了实体类型以及实体中单词的位置。例如，在图一中，我们将B-PER和L-PER（分别表示person实体类型的起始单词和末尾单词）分配给Sidney Yates的每一个单词，以将该短语表示为PER(person)实体类型。 我们在序列层（sequence layer）的顶层执行实体检测。我们引入了由$n_{h_e}$维的隐藏层$h^{(e)}$和softmax输出层组成的2层的神经网络来做实体检测。 这里，$W$是权重矩阵，$b$是偏置向量。 我们以贪婪的，从左到右的方式为单词分配实体标签（也考虑过集束搜索，不过没有太大改进）。在解码中，我们使用单词的预测标签来预测下一个单词的标签，以便考虑标签的依赖性。 上面的神经网络接收其在序列层中的相应输出的级联以及前一个单词的标签嵌入。 小结Sequence层主要是对word embedding 和 POS embedding进行编码，使用BILOU编码模式，将实体检测作为序列标注任务，使用BiLSTM，然后接入softmax进行标签分类。这里使用到了pretrain entity使用预训练的实体的标签，使用Sheduled sampling计划采样，训练时以概率p选择模型自身的输出作为下一个预测的输入，以1-p选择真实标记作为下一个预测的输入，采样过程中p是变化的，一开始训练不充分时可以小一点，尽量使用真实label，随着训练进行，将p增大，多采用自身输出作为下一个预测的输入。 依赖层 Dependency Layer依赖层通过依存树表示一对目标单词间的关系（对应关系分类中的候选关系），并负责关系特定表示，如图1右上所示。该层主要关注依存树中一对目标单词间的 最短路径 （即，最小公共节点与两个目标词之间的路径），因为这些路径在关系分类中是有效的（Xu，2015a)。例如，我们在图1下部展示了Yates和Chicago间的最短路径，这条路径很好地捕获了表达它们关系的关键短语，即born in。 我们引入双向树结构的LSTM-RNN(即，自下而上和自上而下），通过捕获目标单词对周围的依赖结构来表达候选关系。这个双向结构向每个节点传播来自叶子节点的信息以及来自根节点的信息。这对关系分类来说很重要，它利用树底部附近的参数节点，而我们自上而下的LSTM-RNN将信息从树顶部发送到此类近叶节点（这和标准自下而上的LSTM-RNN不同）。注意，Tai等人（2015）的树形LSTM-RNN的两种变体无法代表我们的目标结构，目标结构中有可变数量的类型化children：Child-Sum Tree-LSTM不能处理类型，N-ary Tree假设固定数量的子类型。因此我们提出了一种树形LSTM-RNN的变体，该变体为相同类型的孩子共享权重矩阵$U$s，并且允许可变数量的孩子。对于该变体，我们使用以下方程以$C(t)$个子级在第t个节点的LSTM单元上计算$n_{l_t}$维向量： 其中$m(\\cdot)$是类型映射函数。 为了研究合适的结构来表示两个目标单词对之间的关系，我们尝试了3种可选结构。我们首先使用了最短路径结构（SP-Tree），这种结构能够捕捉目标单词对之间和核心依存路径，并且在关系分类模型中广泛的使用，例如（Bunescu and Mooney，2005；Xu，2015a）。我们也尝试了其他两种依赖结构：SubTree 以及 FullTree。SubTree是目标单词对的最低公共祖先的子树。这为SPTree中的单词对和路径提供了额外的修饰信息。FullTree是完全依存树。这从整个句子中捕获上下文。虽然我们为SPTree使用一种节点类型，但我们为SubTree和FullTree定义了两种节点类型。即，一种用于最短路径上的节点，另一种用于所有其他节点。我们用类型映射函数$m(\\cdot)$来区别这两种节点类型。 小结这部分主要用于抽取目标实体对的关系，方法是在依赖树中找到两个目标实体的最短路径（shortest path）。文中使用了 bidirectional tree-structured LSTM-RNNs的方式，这种双向（从上往下 + 从下往上）的方式，使得每个节点包含的信息也是双向的。而且文中的这种方式可以应对不同类型和数量的子节点，对同种类型的子节点共享权重矩阵$U$s。 堆叠序列层和依赖层我们将依赖层（对应于候选关系）堆叠在序列层之上，以便将单词序列和依存树结构的信息包括进输出中。依赖层中，第t个单词的LSTM单元接受输入$x_t = \\left[ s_t;v_t^{(d)};v_t^{(e)}\\right]$，即串联sequence层的隐层状态向量$s_t$，依存类型嵌入$v_t^{(d)}$(表示对parent的依存类型)，以及标签嵌入$v_t^{(e)}$(对应于预测的实体类型)。 关系分类在解码期间，我们使用检测到的实体的最后单词的所有可能组合来逐步构建候选关系（即，在BILOU模式中有L或U标签的单词）。例如，我们使用带L-PER标签的Yates以及带U-LOC标签的Chicago构建候选关系。对于每个候选关系，我们实现了在候选关系中单词对$p$之间的路径相对应的依赖层$d_p$，并且神经网络接收由依存树层的输出构建的候选关系向量，然后预测它的关系标签。当检测到的实体是错误的或者当这对实体没有关系时，我们将这对实体视作负关系。我们通过类型和方向表示关系标签（除去负关系没有方向） 候选关系向量是由级联的$d_p$构建的，$d_p = \\left[\\uparrow h_{p_A};\\downarrow h_{p_1}; \\downarrow h_{p_2}\\right]$，其中$\\uparrow h_{p_A}$是自底向上LSTM-RNN（表示目标单词对的最低公共祖先）的上层LSTM单元的隐藏层状态向量。$\\downarrow h_{p_1}, \\downarrow h_{p_2}$是两个LSTM单元的隐藏层向量，表达自顶向下的LSTM-RNN中的第一个和第二个目标单词。所有相应的箭头都在图一表示了。 和实体检测类似，我们也引入两层神经网络，$n_{h_r}$维隐藏层$h_{(r)}$以及softmax输出层（权重矩阵$W$，偏置向量$b$) 我们通过从树结构LSTM-RNNs堆叠在序列LSTM之上，来为关系分类构建输入$d_p$,因此sequence层对输入的贡献是间接的。此外，我们的模型使用单词表示实体，因此它不能完全使用实体信息，为减轻这个问题，我们直接从序列层到输入$d_p$串联每个实体的平均隐层状态向量进行关系分类，即： 其中$I_{p_1}$和$I_{p_2}$代表第一和第二实体的索引集合。 另外，由于考虑了从左到右和从右到左的方向，因此在预测中为每个单词对分配了2个标签。当预测标签不一致时，我们选择positive以及confident 的标签，类似(Xu,2015). 训练我们通过BPTT和Adam(Kingma and Ba, 2015)来更新模型参数(weights, biases, embeddings)，并使用了梯度裁剪，参数平均和L2正则化（我们规范权重$W$和$U$，而不是偏置项$b$)。我们还将dropout应用在embedding层和最终隐藏层，以进行实体检测和关系分类。 我们引入2个增强：计划采样以及实体预训练，以解决训练早期实体的预测标签不可信的问题，并且鼓励从检测到的实体中鼓励构建positive的关系实例。在计划采样中，我们以概率$\\epsilon_i$使用gold labels作为预测（如果gold label合法），该概率依赖于训练过程中epochs的数量i。至于$\\epsilon_i$，我们选择反sigmod衰减 其中$k (\\geq 1)$是一个超参数，用来调整我们使用gold label做预测的频率。 实体预训练是由(Pentina,2015)激发的灵感，并且，我们在训练整个模型参数之前，使用训练数据来预训练实体探测模型。 小结 dependency layer采用了依存树结构，树中的节点通过Bi-LSTM连接。遍历所有预测出的实体的最后一个word（BIL的最后一个word或U对应的word）的所有组合，做为候选的word-pair。预测时，word-pair中的每个实体由三个向量拼接表示：对应的label-embedding,对应的sequence layer的hidden unit，与父节点的dependency embedding。然后在依存树中找到连接这两个实体的最短路（sptree）（有论文证实最短路有利于分析实体间的关系）。然后利用最短路通过softmax输出它们之间的关系，比如图中的Yates和Chicage就是一个word-pair，其中Yates的label是预测出的BL的组后一位L，Chicage的label是U，在依存树中它们的最短路如图所示。这也是本文的核心内容，看似依旧是pipeline式的抽取，但由于sequence layer的参数和embedding层的参数是用到了dependency layer的，就是说参数是共享的，所以实体抽取和关系抽取是互相影响的，但是并没有做到真正的联合抽取，因为是先抽取出实体，然后得到所有实体对之间的关系。 总结本文论文题目是端到端的关系抽取，但利用了NER作为辅助任务帮助关系抽取，可以说是实体和关系联合抽取的开山之作，所以往后的论文都会拿出来吊打一波。预测实体时忽略了标签之间的长依赖关系， (Y. H. Suncong Zheng n.d.)利用LSTM解码器解决了标签的长依赖问题[1]；预测好实体后，两两配对后输入到关系分类模块中识别它们之间的关系，因为不存在关系的实体对也输入到关系分类模块中，造成信息冗余，增加了错误率， (Suncong Zheng 2017)提出了新颖的标注机制解决了信息冗余的问题[2]。 这篇文章有哪些缺点 1.预测实体时忽略了标签之间的长依赖关系.2.预测好实体后，两两配对后输入到关系分类模块中识别它们之间的关系，因为不存在关系的实体对也输入到关系分类模块中，造成信息冗余，增加了错误 作者的构思有哪些 在关系抽取中词序信息和树结构信息是可以互补的。因此，作者提出了一种新颖的端到端模型，基于词序信息和依存树结构信息来抽取实体间的关系。该模型使用了双向的LSTM和tree-LSTM结构，同时抽取实体及其关系。 后来的改进有哪些 [1]. Suncong Zheng, Yuexing Hao, Dongyuan Lu, Hongyun Bao, Jiaming Xu, Hongwei Hao, Bo Xu. “Joint entity and relation extraction based on a hybrid neural network.” Neurocomputing 257, n.d.: 59-66 (2017).利用LSTM解码器解决了标签的长依赖问题[2]. Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, Bo Xu. “Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.” ACL (1), 2017: 1227-1236.提出了新颖的标注机制解决了信息冗余的问题 链接：https://blog.csdn.net/guolindonggld/article/details/79547284https://blog.csdn.net/qq_21460525/article/details/74918242https://www.cnblogs.com/robert-dlut/p/7710735.htmlhttps://blog.csdn.net/bobobe/article/details/82867239https://www.cnblogs.com/theodoric008/p/7874373.htmlhttps://www.sohu.com/a/165856071_465975https://blog.csdn.net/qq_32782771/article/details/89404573"},{"title":"【LeetCode】89.格雷编码","date":"2019-10-29T14:07:59.000Z","path":"2019/10/29/leetcode-89/","text":"89.格雷编码格雷编码是一个二进制数字系统，在该系统中，两个连续的数值仅有一个位数的差异。 给定一个代表编码总位数的非负整数 n，打印其格雷编码序列。格雷编码序列必须以 0 开头。 示例 1: 输入: 2输出: [0,1,3,2]解释:00 - 001 - 111 - 310 - 2 对于给定的 n，其格雷编码序列并不唯一。例如，[0,2,3,1] 也是一个有效的格雷编码序列。 00 - 010 - 211 - 301 - 1 示例 2: 输入: 0输出: [0]解释: 我们定义格雷编码序列必须以 0 开头。 给定编码总位数为 n 的格雷编码序列，其长度为 2n。当 n = 0 时，长度为 20 = 1。 因此，当 n = 0 时，其格雷编码序列为 [0]。 题解分析解法1 - 动态规划动态规划需要找到状态转移方程，如何找？ 看例子，当n = 2时，gray code为0,1,3,2，用二进制表示为： 0：001：013：112：10 当n = 3时，一共有$2^3$个数字，那么需要将上面的4个数字（2位）扩展成8个数字(3位），怎么扩展？ 首先，在原数字的起始位置添加0，这样不会改变原数字的值： 0:0001:0013:0112:010 另外四个可以添加1： 4:1005:1017:1116:110 这种方法仔细去看是不对的，因为此时0:000和6:110相差了两位。 考虑第一组的第四个数字2:010，下一个数只要和它有一位不同即可，因此在首位加1后，直接把后面的位数挪下来：6:110。那么再往下一个数也只要和6:110有一位不一样即可，由于6:110是由2:010推出来的，2:010和3:011不同，因此只需要对3:011将第一位变为1，得到7:111，即可。 由此我们可以发现，我们的n每增加一次，我们只要把之前的结果拿下来，再加上相应的数即可，那么加多少呢？ 上一个例子我们加了4，也就是$2^{3 - 1}$ ,即1 &lt;&lt; (n - 1)。 代码 123456789101112public List&lt;Integer&gt; grayCode(int n) &#123; int size = (int)Math.pow(2,n); List&lt;Integer&gt; result = new ArrayList&lt;&gt;(size); result.add(0); for(int i = 0; i &lt; n; i++)&#123; int add = 1 &lt;&lt; i; for(int j = result.size() - 1; j &gt;= 0; j--)&#123; result.add(result.get(j) + add); &#125; &#125; return result;&#125; 复杂度分析 时间复杂度：$O(2^n)$空间复杂度：$O(1)$ 解法2 - 推导该方法参考了LeetCode题解 详细通俗的思路分析，多解法 看下维基百科提供的一个生成格雷码的思路。 以二进制为 0 值的格雷码为第零项，第一项改变最右边的位元，第二项改变右起第一个为1的位元的左边位元，第三、四项方法同第一、二项，如此反复，即可排列出n个位元的格雷码。 以 n = 3 为例。 0 0 0 第零项初始化为 0。 0 0 1 第一项改变上一项最右边的位元 0 1 1 第二项改变上一项右起第一个为 1 的位元的左边位 0 1 0 第三项同第一项，改变上一项最右边的位元 1 1 0 第四项同第二项，改变最上一项右起第一个为 1 的位元的左边位 1 1 1 第五项同第一项，改变上一项最右边的位元 1 0 1 第六项同第二项，改变最上一项右起第一个为 1 的位元的左边位 1 0 0 第七项同第一项，改变上一项最右边的位元 思路有了，代码自然也就出来了。 作者：windliang链接：https://leetcode-cn.com/problems/gray-code/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by--12/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 123456789101112131415161718192021222324252627public List&lt;Integer&gt; grayCode2(int n) &#123; List&lt;Integer&gt; gray = new ArrayList&lt;Integer&gt;(); gray.add(0); //初始化第零项 for (int i = 1; i &lt; 1 &lt;&lt; n; i++) &#123; //得到上一个的值 int previous = gray.get(i - 1); //同第一项的情况 if (i % 2 == 1) &#123; previous ^= 1; //和 0000001 做异或，使得最右边一位取反 gray.add(previous); //同第二项的情况 &#125; else &#123; int temp = previous; //寻找右边起第第一个为 1 的位元 for (int j = 0; j &lt; n; j++) &#123; if ((temp &amp; 1) == 1) &#123; //和 00001000000 类似这样的数做异或，使得相应位取反 previous = previous ^ (1 &lt;&lt; (j + 1)); gray.add(previous); break; &#125; temp = temp &gt;&gt; 1; &#125; &#125; &#125; return gray;&#125; 复杂度分析： 时间复杂度：$O(n2^n)$,每添加两个数需要找第一个为 1 的位元，需要$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】125.验证回文串","date":"2019-10-29T09:00:01.000Z","path":"2019/10/29/leetcode-125/","text":"125.验证回文串给定一个字符串，验证它是否是回文串，只考虑字母和数字字符，可以忽略字母的大小写。 说明：本题中，我们将空字符串定义为有效的回文串。 示例 1: 输入: “A man, a plan, a canal: Panama”输出: true 示例 2: 输入: “race a car”输出: false 题解分析在此贡献调库侠解法，腰疼无法思考只能调库这样子。 解法1 - 双指针调库大法题目要求只考虑数字和字母，并且大小写不敏感，但字符串可能出现其他字符，因此我们先replaceAll()，把数字和字母以外的字符全替换成&quot;&quot;,我们还可以接着用toLowerCase()把大写也转换成小写，这么处理完，接下来就很简单了，两个指针分别指向头部和尾部，依次比较，如果不同直接返回false，如果相同就继续，直到两个指针相遇的时候，说明该字符串是回文，结束。 代码 12345678910111213public boolean isPalindrome(String s) &#123; if (s == null) return false; if (s.length() == 0) return true; String str = s.toLowerCase().replaceAll(\"[^a-z^0-9]\",\"\"); int left = 0, right = str.length() - 1; while (left &lt; right) &#123; char lch = str.charAt(left); char rch = str.charAt(right); if(lch != rch) return false; else left++;right--; &#125; return true;&#125; 复杂度分析 时间复杂度：$O(n^2)$空间复杂度：$O(n)$"},{"title":"【LeetCode】259.较小的三数之和","date":"2019-10-29T08:39:11.000Z","path":"2019/10/29/leetcode-259/","text":"259.较小的三数之和给定一个长度为 n 的整数数组和一个目标值 target，寻找能够使条件 nums[i] + nums[j] + nums[k] &lt; target 成立的三元组 i, j, k 个数（0 &lt;= i &lt; j &lt; k &lt; n）。 示例： 输入: nums = [-2,0,1,3], target = 2输出: 2解释: 因为一共有两个三元组满足累加和小于 2: [-2,0,1] [-2,0,3] 进阶：是否能在$O(n^2)$的时间复杂度内解决？ 题解分析这道题有很多种方法，类似于三数之和15题和16题的解法可以解出这道题。这里有一个很巧妙的方法，记录一下。 解法1 - 双指针先把问题简化成寻找全部两数之和小于target，即a + b &lt; target求全部a和b的组合。 首先类似于三数之和，我们对数组按升序进行排序，结合例子来分析。 给定nums = [3,5,2,9,4,1]，排序后 nums = [1,2,3,4,5,9]。target = 7。 设定双指针left和right分别指向数组的头部和尾部，算法过程如下： 此时已经解出较小的2数之和了，怎么扩展到3数之和呢？ 每取数组中的一个元素，就对它之后的元素进行上述算法，newTarget = target - nums[i]，就把问题化简成两数的问题了。 代码 12345678910111213141516171819202122public int threeSumSmaller(int[] nums, int target) &#123; Arrays.sort(nums); int sum = 0; for(int i = 0; i &lt; nums.length - 2; i++)&#123; sum += twoSumSmaller(nums,i + 1, target - nums[i]); &#125; return sum;&#125;private int twoSumSmaller(int[] nums, int start, int target)&#123; int sum = 0; int left = start, right = nums.length - 1; while(left &lt; right)&#123; if(nums[left] + nums[right] &lt; target)&#123; sum += right - left; left++; &#125;else&#123; right--; &#125; &#125; return sum;&#125; 复杂度分析： 时间复杂度：$O(n^2)$，排序的复杂度近似$O(nlogn)$，寻找两数之和的复杂度为$O(n)$，对于每个元素都寻找两数之和，因此复杂度为$O(n^2)$空间复杂度：$O(1)$"},{"title":"【论文笔记】Modeling Joint Entity and Relation Extraction with Table Representation","date":"2019-10-28T12:56:25.000Z","path":"2019/10/28/Miwa-2014-note/","text":"摘要本文提出了一种基于历史的结构化学习方法，该方法可以联合抽取句子中的实体和关系。我们介绍了一种新的实体和关系的table表示形式。我们在table上不精确搜索的情况下，调查了几种特征设置，搜索顺序和学习方法，实验结果表明，联合学习方法通过合并全局特征并选择合适的学习方法和搜索顺序，大大优于pipeline方法。 结论本文提出了基于历史的结构化学习方法，可以共同检测实体和关系。我们介绍了一种新的实体和关系table，可以联合表示实体和关系，并且展示如何将实体和关系抽取任务映射到一个简单的表填充问题。我们调查了先前研究中确定了的搜索顺序和学习方法。实验结果表明，联合学习方法比pipeline方法更优秀，正确选择学习方法和搜索顺序对于在此任务上产生高性能至关重要。在将来的工作中，我们计划将这种方法应用到其他关系抽取任务中并且为关系抽取任务探索更合适的搜索顺序。 引言传统上从文本中抽取实体和关系被视为两个单独的子任务的pipeline：实体识别和关系抽取。这种分离使得任务易于处理，但忽略了子任务之间和内部的潜在依赖性。首先，由于实体识别不受关系抽取的影响，因此实体识别中的错误将会传播到关系抽取。其次，关系抽取通常被当做对成对实体的多分类问题，因此pairs之间的依赖被忽略了。 这种依赖关系如图1所示，对于子任务之间的依赖，Live_in关系需要实体PER和LOC，反之亦然。对于子任务内的依赖，“Mrs.Tsutayama”和“Japan”间的关系Live_in可以从其他两个关系中推断出来。 图1还显示了该任务具有灵活的图形结构，与其他NLP任务（POS标记和依存关系分析）不同，此结构通常不会覆盖句子中所有单词，因此在该任务中，局部约束被认为更重要。 联合学习方法在模型中合并了这些依赖以及局部约束(Yang,2013;Singh,2013)，但是，大多数方法很耗时，并且采用由多个模型组成的复杂结构。Li和Ji（2014）最近提出了一种基于历史的结构化学习方法，该方法比其他方法更简单且计算效率更高。尽管该方法很有希望，但它仍然具有搜索的复杂性，并且由于其semi-Markov表示，部分地限制了搜索顺序，因此，尚未充分研究基于历史的学习的潜力。 本文中，我们介绍了一个实体和关系table来解决在表示任务上的困难。我们提出实体和关系的联合抽取，使用基于历史的结构在table上学习。这种table表示将任务简化为表填充问题，并使任务具有足够的灵活性，以合并以前基于历史纪录的方法未解决的一些增强功能，例如解码中的搜索顺序，从关系到实体的全局特征以及不精确搜索的几种学习方法。 研究方法本节中，首先介绍实体和关系table，该table用于表示句子中的整个实体和关系结构。然后，在该table上概述我们的模型。最后解释模型中的decoding，learning，search order和features。 实体和关系表(Entity and realtion table)在这项工作中解决的任务是从句子中提取实体和他们之间的关系。实体是类型化的并且可能跨越多个单词，关系是类型化并且直接的。 我们使用单词来表示实体和关系。假设实体不重叠。我们采用BILOU（Begin，Inside，Last，Outside，Unit）编码方案，该方案已证明优于传统的BIO方法（Ratinov and Roth，2009），并且我们将证明该方法会引起单词之间以及单词和关系间的标签依赖（label dependencies）。 根据与单词对应的实体的相对位置和实体类型，将标签分配给单词。关系用类型和方向表示。⊥表示非关系对，→表示从左到右，←表示从右到左。关系定义在单词上而非实体，因为抽取关系时并不总是给出实体。实体上的关系被映射到实体最后一个词上的关系。 基于这种表示，我们提出实体和关系table，能够联合表示句子中的实体和关系。图2阐述了与图1中例子相关的实体和关系table。由于表格对称，我们只用下三角，因此当句子中有$n$个单词时，共有$n(n+1)/2$个单元格。通过这种实体和关系table的表示，联合抽取问题被映射为表填充问题，因为标签被分配给table的单元格。 稍微解释一下表格的意义，对角线是用BILOU编码表示每个单词的编码，如Mrs.是人名的起始，Tsutayama是人名的结束，is和from是实体之外的词（用O表示），Japan是用一个单词就能表示的实体，因此用Unit来表示。 通过Tsutayama能够推出与Kumamoto Prefecture的Live_in关系，以此类推。 模型我们通过基于历史的结构化学习方法处理表填充问题，该方法一个一个地为单元格分配标签。除了table表示之外，这和传统基于历史的模型（Collins，2002）很相似。 假设$\\bf x$是输入表，$\\bf Y(x)$是对该表的所有可能赋值，$s(\\bf x,y)$是一个打分函数，用于评估$y \\in \\bf Y(x)$对$\\bf x$的取值。根据这些定义，我们定义了模型来预测最有可能的分配： 这个打分函数是可分解的，每个分解函数都会对分配给表中单元格的标签进行评估。 此处，$i$代表单元格的索引，将在Table-to-sequence mapping一节解释。分解函数$s(\\bf x,y,\\it 1,i)$对应第$i$个单元格。分解函数表示为线性模型，即，特征及其对应权重的内积。$s(\\bf x,y, \\it 1,i) = \\bf w\\cdot f(x,y,\\it,1,i) \\tag{3}$ 打分函数随后被划分为如下两个函数：$s(\\bf x,y, \\it 1,i) = s_{local}(\\bf x,y,\\it i) + s_{global}(\\bf x,y,\\it 1,i) \\tag{4}$ 此处，$ s_{local}(\\bf x,y,\\it i)$是局部打分函数，它在不考虑其他分配的情况下评估了对第$i$个单元的分配。而$ s_{global}(\\bf x,y,\\it 1,i)$是全局打分函数，它在第1个到第$i-1$个分配的上下文中评估每个分配。该全局打分函数表示实体之间，关系之间以及实体和关系之间的依赖性。同样，特征$\\bf f$被划分为局部特征$\\bf f_{local}$以及全局特征$f_{global}$,他们被定义在目标单元格以及周围的上下文中。特征将在Features部分解释。权重$\\bf w$也能划分，但是在学习中可以共同调整他们。 解码公式(2)中的打分函数$s(\\bf x,y,\\it 1,i)$使用所有之前的分配并且不依赖于马尔可夫假设，因此我们不能使用动态规划。 相反，我们使用集束搜索来找到得分最高的最佳分配（Collins and Roark,2004)。在从一个单元格移动到下一个单元格时，集束搜索依次为单元格分配标签，并保持最好的K个分配。用于解码的集束搜索伪代码如图3所示： 我们将在下面的子章节解释如何将table映射到序列(第2行)，以及如何计算可能的分配（第6行）。 table 到 sequence的映射输入表的单元格最初是通过二维进行索引的。为了将模型部分介绍的的模型应用到单元格，我们需要将二维table映射到一维的sequence。这等价于在table中定义搜索顺序，因此我们将交替使用术语映射和搜索顺序。 由于尝试所有可能的映射是不可行的，因此我们定义了6个有希望的静态映射（搜索顺序），如图4所示。注意，标题中的left和right对应的是表格而不是单词顺序。定义了两个映射(a)和(b)在up to down顺序上有最高优先级，它从句子开头检查一个句子。同样，定义了两个映射(c)和(d)，并在right to left顺序有最高优先级，它从句尾开始检查句子。 从另一个角度来看，(b)和(c)中，实体在关系之前被检测到，而在(a)和(d)中优先考虑句子的顺序。 进一步定义了close-first的映射，因为实体比关系更容易找到，近的关系比远的关系更容易找到。 我们还研究了采用easy-first策略（Goldberg和elhadad，2010）的动态映射。动态映射与静态映射不同，因为会在每次decoding之前对单元进行重排序（在decoding中进行重排序也是有必要的，但它很大地加大了计算开销）。我们用局部打分函数评估单元格，并且为单元格分配索引，以使得分更高的单元格具有更高优先级。除了这种简单的easy-first策略外，还定义了两个动态映射，通过将esay-first策略与以下两个策略之一组合来限制重排序：entity-first实体优先,所有实体在关系之前被探测和close-first 近距离优先，较近的单元格在较远的之前被探测。 标签依赖 label dependencies为了避免对表的非法分配，必须根据前面的分配将可能的分配限制到单元格，该限制还能减少计算成本。我们考虑单元格之间的所有依赖，以允许以任意顺序将标签分配给单元。在Entity and realtion table这一部分引入了实体间的依赖以及实体和关系间的依赖。 表1-3总结了句子中第i个词$w_i$上的这些依赖。如果某些实体类型涉及有限数量的关系类型，则我们可以进一步利用实体类型和关系类型之间的依赖关系，反之亦然。我们注意到实体类型和关系类型之间的依赖性不仅包括参与关系的词，还包括其周围的单词。例如$w_{i-1}$上的标签可以限制涉及$w_i$的关系类型，我们在评估中采用了这些类型依赖，但是由于这些依赖取决于任务，因此在此省略这些依赖。 学习学习目标是通过调整公式3的打分函数权重$\\bf w$来最小化预测分配y和最佳分配$y^gold$间的误差。 这篇文章的问题：需要设计复杂的特征工程，严重依赖于NLP工具，这可能会导致错误传播问题。 链接： https://www.cnblogs.com/theodoric008/p/7874373.html https://www.jianshu.com/p/4b78e109811ahttps://zhuanlan.zhihu.com/p/23635696https://www.sohu.com/a/165856071_465975https://blog.csdn.net/qq_32782771/article/details/89404573"},{"title":"【论文笔记】Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions","date":"2019-10-28T03:47:18.000Z","path":"2019/10/28/sentence-level-attention-entity-description-2017-note/","text":"关于这篇论文在做什么这篇文章是在之前的两篇基础上进行扩展（PCNN和Attention这两篇），将实体信息融入到Attention Weights的计算中。 在关系抽取中，实体对之间的关系与实体本身所携带的信息具有很大的关系，如何得到准确的实体描述是一个非常关键的问题。如果只利用词向量作为实体信息的描述会存在局限性： （1）词向量很难学出专有名词，类似人名，地名的准确向量表示； （2）人名经常作为实体出现，词向量无法给具有相同姓名的不同人物不同的向量表示； （3）对于预训练词向量矩阵中没有出现过的实体，只能随机初始化，该向量无法携带任何有用信息。针对以上问题，本文利用Freebase和Wikipedia提供的实体的描述信息，生成特定语境下的实体的准确的向量表示。 总结&amp;思考针对弱监督关系抽取数据集中的噪声数据问题，本文通过加入实体描述信息，提出了更鲁棒的Attention机制。实体描述信息的向量表示结合了知识图谱构建的TransE模型的思路，非常具有说服力。 因为Freebase中的实体多为著名的人物或地点，所以可以找到准确的描述信息，如果对于没有可以直接利用的描述的实体，该如何生成准确的实体描述？ 更多参考这篇笔记：Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions 关于TransE：TransE模型：知识图谱的经典表示学习方法"},{"title":"【论文笔记】Neural Relation Extraction with Selective Attention over Instances","date":"2019-10-26T11:19:51.000Z","path":"2019/10/26/Lin-2016-note/","text":"摘要为解决远程监督关系抽取中的wrong label问题，提出句子级基于注意力的模型。模型中，使用CNN来嵌入句子的语义。之后在多个实例上建立句子级别的注意力，有望动态减少噪声实例的权重。在真实数据集上的结果表明，该模型可以充分利用所有有效的句子并有效减少错误标记实例的影响。与基线相比，该模型在关系提取方面实现了显著且一致的改进。 代码在这里：https://github.com/thunlp/NRE 结论和未来工作开发了具有句子级别的注意力的CNN。模型可以充分利用有效的句子，减轻了远程监督关系抽取中wrong label问题。在实验中，我们评估了关系抽取任务的模型，实验结果表明，我们的模型显著且始终优于最新的基于特征的方法和神经网络方法。 引言关系抽取是从纯文本中生成关系数据的过程，是NLP中的关键任务。 之前的方法（远程监督mintz-&gt;多实例多标签, 用神经网络的），这些方法基于句子级别的注释数据构建分类器，由于缺少人工标注的训练数据，因此无法在大规模知识库中应用。因此Zeng（2015）将多实例学习和神经网络模型结合，可以基于远程监督数据构建关系提取器。 尽管该方法在关系抽取方面取得了显著改进，但远不能令人满意。该方法假定至少一个提到这两个实体的句子将表答他们的关系，并且仅在训练和预测中为每个实体对选择可能性最高的句子。显然该方法将丢失大量包含在被忽略句子中的丰富信息。 本文提出句子级别的基于注意力的CNN，用于远程监督关系抽取。 如图1，我们使用CNN来嵌入句子的语义。随后，为了利用包含信息的句子，我们将关系表示为句子嵌入的语义组成。为了解决wrong label问题，我们在多实例上建立了句子级别的注意力机制，这有望动态减少噪声的权重。最后，利用句子级别attention加权的关系向量抽取关系。我们在真实数据集上评估模型，实验结果表明，与最新方法相比，模型在关系抽取方面实现了显著且一致的改进。 论文贡献如下： 与现有神经关系抽取模型相比，我们的模型可以充分利用每个实体对的所有包含信息的句子 为了解决wrong label问题，我们提出选择性的注意力机制来淡化噪声实例。 实验中，我们表明选择性注意力机制在关系抽取任务中对两种CNN模型有利。 研究方法给定一组句子$\\{x_1,x_2,\\cdots,x_n\\}$和两个相应实体，模型评估每个关系$r$的概率。这一部分从两个主要方面介绍模型： 句子编码器（Sentence Encoder）：给定句子$x$个两个目标实体，CNN用来构造句子的分布式表示$\\bf x$. 实例的选择性注意机制（Selective Attention over Instances)。当学习了所有句子的分布式向量表示，我们使用句子级别的attention来选择真正表达相应关系的句子。 句子编码器 Sentence Encoder如图2，我们将句子$x$通过CNN转换成分布式表示$\\bf x$。首先，句子中的单词被转换成密集的实值向量。然后，卷积层，最大值池化层和非线性转换层用来构建句子的分布式表示，例如$\\bf x$。 输入表示CNN的输入是句子$x$的原始单词。 首先将单词转换为低维向量。这里每个输入单词通过word embedding矩阵转换为向量。此外,为了明确每个实体对的位置，我们也为句子中的所有单词使用了position embedding。 Word Embeddings.将单词转换为分布式表示，其能捕获句法和语义信息。给定包含$m$个单词的句子$x$，每个单词都被表示成实值向量。单词表示通过embedding矩阵中的列向量$\\bf V$编码，$\\bf V$是固定大小的词汇表。 Position Embeddings。关系抽取中，离目标实体近的单词通常有助于确定实体间的关系。沿用(Zeng,2014)的方法使用position embeddings，它能帮助CNN跟踪每个单词与头部实体或尾部实体的接近程度。它定义为当前单词到头或尾实体的相对距离的组合。 如图2，假定word embedding的维数$d^a$是3，position embedding的维数$d^b$是1。最终将所有单词的word embeddings和position embeddings串联起来，并将其表示为向量序列$\\bf w = \\{w_1,w_2,\\cdots,w_m\\}$，其中$\\bf w_i \\in \\Bbb R^d \\it (d= d^a+d^b \\times 2)$ 卷积层，最大池化层和非线性层关系抽取中，主要存在的挑战就是句子长度可变，且重要信息可能会出现在句子的任何位置。因此我们应该利用所有局部特征并全局地执行关系预测。 这里用卷积层合并这些特征。 卷积层首先用长度为$l$的滑动窗口在句子上抽取局部特征。图2的例子中，假设滑动窗口长度$l$是3。然后，通过最大值池化操作来结合所有局部特征，以获得输入句子对应的固定大小的向量。 卷积被定义为向量序列$\\bf w$和卷积矩阵$\\bf W \\in \\Bbb R^{d^c \\times (l \\times d)}$，其中$d^c$是句子的embedding大小。定义$qi \\in \\Bbb R^{l \\times d}$为第ige窗口内$w$单词嵌入序列的串联： $\\bf q_i = w_{\\it i-l+1:i} \\qquad \\text{$1 \\leq i \\leq m+l-1$} \\tag{1}$ 由于窗口在滑动到边界附近可能会在句子边界之外，因此我们为句子设置了特殊的padding标记。我们将所有超出范围的输入向量$w_i(im)$作为零向量。 因此，卷积层的第i个过滤器计算为： $\\bf p_i = [W_q + b]_i \\tag{2}$ 其中$\\bf b$是偏置向量。向量$\\bf x \\in \\Bbb R^{d^c}$的第i个元素如下： $[\\bf x]_i = max(p_i) \\tag{3}$ 此外，PCNN（Zeng，2015）作为CNN的变体，在关系抽取中采用分段最大池化。每个卷积过滤器$\\bf p_i$通过首部和尾部的实体被划分为3个部分（$\\bf p_i1,p_i2,p_i3 $），最大池化处理分别在3个部分中执行。 最后，我们在输出端使用非线性方程，例如tanh。 实例的选择性注意机制（Selective Attention over Instances)引入attention机制，给不同的语料赋予不同的权重，隐式地摒弃一些噪声语料，以此提升分类器的性能。 给包含某一实体及其关系的所有句子都分配权重，这个权重的大小代表着我们是否可以认为该句子包含着该种关系。 给定包含一对实体和n个句子的集合$S = \\{x_1,x_2,\\cdots,x_n\\}$,在预测关系$\\bf r$时，模型将集合$S$表示为实值向量$\\bf s$。集合$S$的表示取决于所有句子表示$\\bf x_1,x_2,\\cdots,x_n$。每个句子表示$\\bf x_i$包含了这样的信息，实体对(head，tail)是否包含句子$x_i$的关系$r$。 集合向量$s$计算为这些句子向量$\\bf x_i$的加权和： $\\bf s = \\sum_{i}{\\alpha_ix_i} \\tag{5}$ 其中$\\alpha_i$是每个句子向量$x_i$的权重。 本文以两种方式定义$\\alpha_i$: 平均值：假设集合中所有句子对集合表示都有相同贡献，那么集合S的embedding就是所有句子向量的平均值。这是我们attentino的最初基线。 Selective Attention：使用均值的话，wrong label的句子会在训练和测试中引入大量噪声，因此使用注意力机制来减小噪声句子的影响。因此$\\alpha_i$定义为： 其中$e_i$称为基于查询的函数，可以对输入句子$x_i$和预测关系关系$r$的匹配程度进行评分。我们选择双线性的形式，该形式在不哦那个的选择中均能达到最佳性能。 $e_i = \\bf x_iAr \\tag{8}$ 其中$\\bf A$是加权对角矩阵，$\\bf r$是与关系$r$相关联的查询向量，指示关系$r$的表示。显然$e$的大小取决于$x$在$r$上的映射的大小。与该实体关系更加密切的句子可以取得更大的取值。 最后，通过softmax层定义条件概率$p(r|S,\\theta)$,那么所要最大化的的就是的就是在网络参数下某实体关系的概率： 其中$n_r$是关系总数，$\\bf o$是神经网络最后的输出，它对应于与所有关系类型相关的分数，定义如下： $\\bf o = Ms + d \\tag{10}$ 其中$d$是偏置向量，$\\bf M$是关系的表示矩阵。 （Zeng，2015）遵循这样的假设，即一对实体至少有一个提及将反映它们之间的关系，并且仅使用每组中概率最高的句子进行训练。 因此，当将概率最高的句子的权重设置为1并将其他概率设置为0时，他们在多实例学习中采用的方法可以被视为我们的选择性注意力特例。 选取交叉熵函数并利用随机梯度下降进行优化最后便可以学得网络的所有参数： 模型分析Attention 从上面两张图可以看出 ONE模型（at-least-one multi-instance,每次只选取最后可能的一个句子进行训练和预测）效果要优于裸CNN模型，证明了过滤掉噪音语料是有效的； AVE模型（sentence-level attention’s naive version）效果优于裸CNN模型，证明了减弱噪音语料是有效的； 而AVE模型和ONE模型的效果接近，因为对噪音语料的处理都不是十分恰当； 实验也证明了文章对于噪音语料处理的有效性。 句子数目使用CNN/PCNN+ONE,CNN/PCNN+AVE和CNN/PCNN+ATT三种模型在所有句子上训练，然后在每个实体对对应２个以上句子的实例中进行测试，分别使用随机选取１个句子，随机选取２个句子和选取所有句子的方式进行测试。 从上图可以看出 搭载ATT的模型在所有实验条件下都表现最好； 随机选取一个句子测试时，搭载AVE的模型和搭载ATT的模型性能接近，但当选取句子数目上升时，搭载AVE的模型的性能不再增加，说明将每个句子等同对待时，噪音语料会对关系提取有负面影响； 在选取一个句子做预测时，CNN+AVE和CNN+ATT的模型的性能明显优于CNN+ONE，由于这只跟训练有关，说明尽管一些噪音语料会带来负面影响，但训练时考虑的语料越多，总的来说，还是对关系提取有帮助的； 搭载ATT的模型要明显优于其余2种模型，说明当考虑的语料越多的同时，考虑的语料的质量越高，对关系提取的帮助越大。 对比人工提取特征的方法 从上图可以看出 基于attention机制的神经网络模型要明显优于手工提取特征的方法，不仅性能曲线整体更高，同时，下降得也更平缓； 同为使用神经网络的模型，PCNN+ATT要明显优于CNN+ATT，这说明attention机制只考虑了全局的句子层面的信息，而没有考虑到句子内部的信息，也就是说还可以通过使用更为强劲的句子编码神经网络获得更好的句子内信息，以此来提高关系提取的性能。 小结这篇文章的思路十分简洁，面对误标签传播的问题，引入attention来评价句子的贡献度是一种十分自然而舒服的思路。注意力模型之前在很多自然语言处理场景中都得到了很好的应用，那么显然在关系抽取中也可以取得相应的效果。这篇文章的想法对于以后的工作会有很多的借鉴意义。 是否可以将Attention机制和其他的神经网络结合？"},{"title":"【LeetCode】209.长度最小的子数组","date":"2019-10-26T03:39:21.000Z","path":"2019/10/26/leetcode-209/","text":"209.长度最小的子数组给定一个含有 n 个正整数的数组和一个正整数 s ，找出该数组中满足其和 ≥ s 的长度最小的连续子数组。如果不存在符合条件的连续子数组，返回 0。 示例: 输入: s = 7, nums = [2,3,1,2,4,3]输出: 2解释: 子数组 [4,3] 是该条件下的长度最小的连续子数组。 进阶: 如果你已经完成了O(n) 时间复杂度的解法, 请尝试 O(n log n) 时间复杂度的解法。 题解分析$O(n)$的方法已解出。flag留给$O(nlogn)$的。 plus，今天一定要把论文看完(ㄒoㄒ) 看完了！ 解法1 - 双指针滑动窗口用两个指针left和right模拟一个滑动窗口，right扩张窗口，left缩小窗口。 用sum来累计窗口内值的总和，当sum &lt; s时，right++;当sum &gt;= s时，更新minLen，尝试缩小left，直到窗口中的sum &lt; s时结束。 注意的是，初始化minLen = nums.length + 1,那么当数组中所有元素之和都小于s，那我们没有找到这样的子数组，需要返回0，需要在return的时候做一个处理。 代码 123456789101112131415161718public int minSubArrayLen(int s, int[] nums) &#123; if (nums == null || nums.length == 0) return 0; int left = 0, right = 0; int minLen = nums.length + 1; int sum = 0; while (right &lt; nums.length) &#123; sum += nums[right]; if (sum &gt;= s) &#123; while (sum &gt;= s &amp;&amp; left &lt;= right) &#123; minLen = Math.min(minLen, right - left + 1); sum -= nums[left]; left++; &#125; &#125; right++; &#125; return minLen == nums.length + 1 ? 0 : minLen;&#125; 复杂度分析： 时间复杂度：$O(n)$,最坏情况需要遍历数组中的元素两次。空间复杂度：$O(1)$"},{"title":"【LeetCode】88.合并两个有序数组","date":"2019-10-26T03:28:00.000Z","path":"2019/10/26/leetcode-88/","text":"88.合并两个有序数组给定两个有序整数数组 nums1 和 nums2，将 nums2 合并到 nums1 中，使得 num1 成为一个有序数组。 说明: 初始化 nums1 和 nums2 的元素数量分别为 m 和 n。你可以假设 nums1 有足够的空间（空间大小大于或等于 m + n）来保存 nums2 中的元素。 示例: 输入:nums1 = [1,2,3,0,0,0], m = 3nums2 = [2,5,6], n = 3 输出: [1,2,2,3,5,6] 题解分析解法1 - 双指针用两个指针i和j分别指向num1和nums2的末尾，对于nums1来讲应该是指向最后一个元素，因为num1需要能容纳得下两个数组的元素，因此后面会有空位，用k指向num1的末尾，然后通过比较i和j指向元素的大小，通过k来从后向前填充nums1。 代码 12345678910public void merge(int[] nums1, int m, int[] nums2, int n) &#123; int i = m - 1; int j = n - 1; int k = nums1.length - 1; while(i &gt;= 0 &amp;&amp; j &gt;= 0)&#123; if(nums1[i] &gt; nums2[j]) nums1[k--] = nums1[i--]; else nums1[k--] = nums2[j--]; &#125; while(j &gt;= 0) nums1[k--] = nums2[j--];&#125; 复杂度分析： 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】159.至多包含两个不同字符的最长子串","date":"2019-10-24T07:02:30.000Z","path":"2019/10/24/leetcode-159/","text":"159.至多包含两个不同字符的最长子串给定一个字符串 s ，找出 至多 包含两个不同字符的最长子串 t 。 示例 1: 输入: “eceba”输出: 3解释: t 是 “ece”，长度为3。 示例 2: 输入: “ccaabbb”输出: 5解释: t 是 “aabbb”，长度为5。 题解今天是1024，程序员节。 分析用两个指针控制一个滑动窗口，使得窗口中包含的字符最多出现两种。 解法1 - 双指针滑动窗口left指针缩小窗口，right指针扩大窗口。 用Map保存窗口中字符以及其出现的位置。Map最多不超过3种字符。 算法： 1.如果字符串的长度小于 3 ，返回其长度 。2.将左右指针都初始化成字符串的左端点 left = 0和 right = 0 ，且初始化最大子字符串为 maxLen = 0;3.当右指针小于字符串长度时： 如果map包含小于 3 个不同字符，那么将当前字符 str[right] 放到map中并将右指针往右移动一次。 如果map包含 3个不同字符，将最左边的字符从map中删去，并移动左指针，以便滑动窗口只包含 2 个不同的字符。 更新maxLen 代码 1234567891011121314151617181920public int lengthOfLongestSubstringTwoDistinct(String s) &#123; if(s == null || s.length() == 0)return 0; if(s.length() &lt; 3) return s.length(); char[] str = s.toCharArray(); int left = 0, right = 0; int maxLen = 0; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); while(right &lt; str.length)&#123; if(map.size() &lt; 3) &#123; map.put(str[right], right++); &#125; if(map.size() == 3)&#123; int index = Collections.min(map.values()); map.remove(str[index]); left = index+1; &#125; maxLen = Math.max(maxLen,right - left); &#125; return maxLen;&#125; 复杂度分析 时间复杂度:$O(n)$，遍历字符串1遍空间复杂度:$O(1)$，map中不超过3个元素"},{"title":"【LeetCode】75.颜色分类","date":"2019-10-24T06:45:08.000Z","path":"2019/10/24/leetcode-75/","text":"75.颜色分类给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。 此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。 注意:不能使用代码库中的排序函数来解决这道题。 示例: 输入: [2,0,2,1,1,0]输出: [0,0,1,1,2,2] 进阶： 一个直观的解决方案是使用计数排序的两趟扫描算法。首先，迭代计算出0、1 和 2 元素的个数，然后按照0、1、2的排序，重写当前数组。你能想出一个仅使用常数空间的一趟扫描算法吗？ 题解今天是1024，程序员节。 分析三色国旗问题，也叫荷兰国旗问题。 可以用计数排序来做。 关于计数排序，今天读完paper总结。 解法1 - 双指针设置一个左指针left，左指针左边的元素都存放0。 设置一个右指针right，右指针右边的元素都存放2。 设置一个指针cur用来遍历元素，遇到0就和left交换，遇到2就和right交换，遇到1就继续遍历。 代码 12345678910111213141516171819202122public void sortColors(int[] nums) &#123; if(nums == null || nums.length == 0) return; int left = 0, right = nums.length - 1; int cur = 0; while(cur &lt;= right)&#123; if(nums[cur] == 0)&#123; swap(nums,left,cur); left++; cur++; &#125;else if(nums[cur] == 1) cur++; else&#123; swap(nums,cur,right); right--; &#125; &#125;&#125;private void swap(int[] nums, int i, int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp;&#125; 复杂度分析 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】349.两个数组的交集","date":"2019-10-23T02:06:57.000Z","path":"2019/10/23/leetcode-349/","text":"349.两个数组的交集给定两个数组，编写一个函数来计算它们的交集。 示例 1: 输入: nums1 = [1,2,2,1], nums2 = [2,2]输出: [2] 示例 2: 输入: nums1 = [4,9,5], nums2 = [9,4,9,8,4]输出: [9,4] 说明: 输出结果中的每个元素一定是唯一的。我们可以不考虑输出结果的顺序。 题解分析今天过生日，就不做那么难的题了，做两道简单题度过开心的一天。 解法1 - 双指针将两个数组排序，分别用两个指针来遍历。比较到相同的元素就加入Set中。 算法如下： 1.排序nums1，nums2.设置两个指针p、q分别指向nums1和nums2.2.如果nums1[p] &lt; nums2[q],则p++;3.如果nums1[p] &gt; nums2[q],则q++;4.如果nums1[p] == nums2[q]，则加入set中5.直到遍历完其中一个为止。 其实去重的部分也可以不放在set中做，只要多加两个判断条件即可。 代码 1234567891011121314151617public int[] intersection(int[] nums1, int[] nums2)&#123; Arrays.sort(nums1); Arrays.sort(nums2); int p = 0, q = 0; Set&lt;Integer&gt; res = new HashSet&lt;&gt;(); while(p &lt; nums1.length &amp;&amp; q &lt; nums2.length)&#123; if(nums1[p] &lt; nums2[q])p++; else if(nums1[p] &gt; nums2[q] )q++; else if(nums1[p] == nums2[q])&#123; res.add(nums1[p]); p++; q++; &#125; &#125; int[] result = res.stream().mapToInt(Number::intValue).toArray(); return result;&#125; 复杂度分析： 假设nums1长度为n，nums2长度为m 时间复杂度：主要是Arrays.sort()的复杂度。空间复杂度：当nums1和nums2完全相等且不包含重复元素时，$O(n)$ 解法2 - Set大法将nums1放入set1，遍历nums2，判定是否出现在set1中，如果出现则加入set2，最终set2中的元素就是我们求的交集。 123456789101112131415161718public int[] intersection(int[] nums1, int[] nums2)&#123; Set&lt;Integer&gt; set1 = new HashSet&lt;&gt;(); Set&lt;Integer&gt; set2 = new HashSet&lt;&gt;(); for(int num : nums1)&#123; set1.add(num); &#125; for(int num : nums2)&#123; if(set1.contains(num))&#123; set2.add(num); &#125; &#125; int[] res = new int[set2.size()]; int i =0 ; for(int num : set2)&#123; res[i++] = num; &#125; return result;&#125; 复杂度分析: 假设nums1长度为n，nums2长度为m 时间复杂度：$O(max(n,m))$空间复杂度：$O(max(n,m))$"},{"title":"【LeetCode】345.反转字符串中的元音字母","date":"2019-10-23T01:57:23.000Z","path":"2019/10/23/leetcode-345/","text":"345.反转字符串中的元音字母编写一个函数，以字符串作为输入，反转该字符串中的元音字母。 示例 1: 输入: “hello”输出: “holle” 示例 2: 输入: “leetcode”输出: “leotcede” 说明: 元音字母不包含字母”y”。 题解分析今天过生日，就不做那么难的题了，做两道简单题度过开心的一天。 解法1 - 双指针设定头尾两个指针，遇到所指向的元素都是元音字母的就交换一下位置，很自然又简单的思路。 代码 1234567891011121314151617181920212223242526272829303132public String reverseVowels(String s) &#123; Set&lt;Character&gt; set = new HashSet&lt;&gt;(); set.add('a');set.add('A'); set.add('e');set.add('E'); set.add('i');set.add('I'); set.add('o');set.add('O'); set.add('u');set.add('U'); int left = 0, right = s.length() - 1; char[] array = s.toCharArray(); while(left &lt; right)&#123; if(set.contains(array[left]) &amp;&amp; set.contains(array[right]))&#123; swap(array,left,right); left++; right--; &#125;else if(set.contains(array[left]))&#123; right--; &#125;else if(set.contains(array[right]))&#123; left++; &#125;else&#123; left++; right--; &#125; &#125; return String.valueOf(array);&#125; private void swap(char[] array, int i, int j)&#123; char temp =array[i]; array[i] = array[j]; array[j] = temp;&#125; 复杂度分析: 时间复杂度：最坏情况下遍历一遍字符串，$O(n)$空间复杂度：用了额外的set来保存元音，$O(10)= O(1)$"},{"title":"【论文笔记】Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks","date":"2019-10-22T09:22:13.000Z","path":"2019/10/22/Zeng-2015-note/","text":"摘要使用远程监督进行关系抽取时存在2个问题。 1.wrong label 问题 2.NLP工具带来的误差传递问题 本文提出分段卷积神经网络(PCNNs)和多实例学习来解决这两个问题。 对于第一个问题，远程监督关系抽取被当做多分类问题，其中考虑了实例标签的不确定性。 对于第二个问题，避免了特征工程，采用具有分段max pooling的卷积架构自动学习相关特征。 实验证明，该方法优于很多竞争性的基线方法，且是有效的。 结论本文提出的方法，特征能够自动学习无需NLP预处理。成功地在提出的网络中配置了分段最大池化层(max pooling)来捕获结构信息并且结合多实例学习来解决wrong label问题。实验结果证明提出的方法在比较的方法中有重要的改进。 引言关系抽取中，构建机器学习系统面临的一个问题就是生成训练数据，这个问题的常用解决方法是远程监督（Mintz提出），远程监督假设：在已知的知识库中，如果两个实体存在关系，那么所有提到这两个实体的句子都已某种方式表达了这个关系。 图1是通过远程监督自动打标签的例子。其中第二个句子就没表达这个relation。 远程监督用在关系抽取中，也存在两个问题。 1.远程监督的假设太强了，造成了wrong label问题。 2.Mintz、Riedel、Hoffman等人提出的方法通过远程监督获取标记数据时使用监督模型来精心设计特征。这些特征来自已有的NLP工具，由于工具总是存在误差，所以存在误差传播问题。 远程监督从Web获取语料，包括非结构文本。图2展示了句子长度的分布。（benchmark远程监督数据集）。超过一半的句子都超过40个词。McDonald和Nivre提出，句法解析的准确率随着句子长度的增长而显著降低。因此，使用传统特征时，误差传播或积累的问题不仅存在，还会变得更严重。 本文提出了新模型，叫做PCNNs和多实例学习，来解决上面描述的这俩问题。 对于第1个问题，远程监督关系抽取被当做多分类问题，和之前的研究相似(Riedel、Hoffman、Surdeanu）。在多分类问题中训练集被当做很多bag，每个bag有很多实例。bag的标签已知，bag中的实例的标签未知。我们设计了bag级的目标函数。在学习过程中，实例标签的不确定性会被考虑。这样缓解了wrong label问题。 对于第二个问题，我们使用卷积架构来自动学习相关特征，无需复杂的NLP预处理。该方法是对Zeng(2014)提出方法的扩展（自己改进自己~），之前的方法利用单个max pooling操作来定义最重要的特征。尽管这个方法有效，但它过快地缩小了隐藏层的大小，不能捕获两个实体间的结构信息(Graham，2014）. 例如，要定义Steve Jobs 和 Apple的关系，需要指明实体并提取他们之间的结构特征。有几种方法采用了人工创建特征来为结构信息建模。这些方法通常考虑内部和外部的上下文。根据两个给定实体，句子被固定地划分为3个部分。内部上下文考虑两个实体之间的字符，外部上下文包括2个实体周围的字符(Zhang,2006).显然，单个 max pooling不足以捕捉这样的结构信息。 为了捕捉结构和其他潜在信息，基于两个实体的位置将卷积的结果分成3段，并且设计出分段的max pooling层取代单个max pooling层。分段max pooling返回每个segment的最大值而不是整个句子的单一最大值。因此期望比传统方法表现更佳。 本文贡献如下： 探讨了在不需要设计特征的情况下进行远程监督关系抽取的可行性。PCNNs不需要复杂的NLP预处理就可以自动学习特征。 为了解决wrong label问题，开发了创新的解决方案，将多实例学习融入PCNNs中，用于远程监督关系抽取。 在该网络中，我们涉及了一个分段最大池化层，其目的是捕获两个实体之间的结构信息。 研究方法远程监督关系抽取是一个多实例问题。在这一部分，我们提出将多实例学习融入卷积神经网络中来解决这个问题。PCNNs 用来自动学习特征。 图3展示了远程监督关系抽取的神经网络结构。解释了处理一个bag中一个实例的过程。该过程包括4个主要的部分： 向量表示(Vector Representation) 卷积 分段最大值池化 Softmax输出 接下来详细解释这4个部分。 向量表示网络的输入是原始单词标记。使用神经网络时，我们通常将单词标记转换为低维向量。在我们的方法中，通过查询预训练的词嵌入，将每个输入的单词标记转换成一个向量。此外，我们使用位置特征（PFs）来指定实体对，这也是通过查询位置嵌入(position embeddings)进行的。 词嵌入词嵌入是单词的分布式表示，它将文本中的每个单词映射为k维的实值向量。在捕捉单词的语义和语法信息上十分有效。使用预先训练好的词嵌入已经成为解决其他NLP问题的常用方法(Parikh,2014;Huang,2014). 训练神经网络的通常方法是随机初始化所有参数，然后再通过优化算法来优化它们。最近的研究（Erhan，2010）表明，使用词嵌入初始化时，神经网络可以收敛到更好的局部极小值。词嵌入通常是通过利用未标记文本中单词的共现结构，以完全无监督的方式学习的。研究人员已经提出一些训练词嵌入的方法（Bengio，2003；Collobert，2011；Mikolov，2013）,本文使用Skip-gram模型（Mikolov，2013）来训练词嵌入。 位置嵌入在关系抽取中，我们专注于为实体对打标签。和Zeng（2014）相似，我们使用PFs来指定实体对。一个PF被定义为当前单词与$e_1$和$e_2$的相对距离的组合。 举例来讲，son和$e_1$(Kojo Annan）与$e_2$(Kofi Annan)的相对距离分别是3和-2。 两个位置嵌入矩阵（$PF_1$和$PF_2$）是随机初始化的。然后通过查找位置嵌入矩阵将相对距离转换成实值向量。在图3的例子中，假设词嵌入的大小是$d_w = 4$，位置嵌入的大小是$d_p = 1$.在组合的词嵌入和位置嵌入中，向量表示部分将实例转换成矩阵 $S \\in \\Bbb R^{s \\times d}$,其中$s$是句子长度，$d = d_w + d_p \\times 2$;然后矩阵S就作为卷积部分的输入。 卷积在关系抽取中，标记为包含目标实体的输入句子仅对应于关系类型。它不会为每个单词预测标签。因此，可能有必要利用所有局部特征并全局地执行此预测。当使用神经网络时，卷积方法是很自然地用来合并所有特征的方法(Collobert,2011)。 卷积是一个在权重向量$\\bf w$和输入向量之间的操作，输入向量被当做一个序列$\\bf q$。权重向量$\\bf w$被当做卷积的过滤器。在图3的例子中，假设过滤器的长度是$w(w=3)$；因此，$\\bf w$ $\\in \\Bbb R^m$ $(m = w*d)$。我们认为$\\bf S$是一个序列$\\{\\bf q_1,q_2,\\cdots,q_s\\}$，其中$\\bf q_i$ $\\in \\Bbb R^d$。通常$\\bf q_{i:j}$指的是$\\bf q_i$到$\\bf q_j$的串联。卷积操作涉及对序列$\\bf q$中的每个$w$-gram取$\\bf w$的点积，以获得另一个序列$\\bf c$ $\\in \\Bbb R^{s+w-1}$:$c_j = \\bf wq_{\\it j-w+1:j} \\tag{1}$ 其中索引$j$的范围从1到$s+w-1$,超出范围的输入值$\\bf q_i$取0，其中$is$。 捕获不同特征通常需要在卷积中使用多个过滤器（或特征映射）。在该假设下，我们使用$n$个过滤器（$\\bf W = \\{ w_1,w_2,\\cdots.w_n\\}$),卷积操作可以表达如下：$c_ij = \\bf w_i q_{i-w+1:j} \\qquad \\text{$1 \\leq i \\leq n$} \\tag{2}$ 卷积后的结果是矩阵$\\bf C = \\{c_1,c_2,\\cdots,c_n\\} \\in \\Bbb R^{\\it n \\times (s+w-1)}$ . 图3是使用3个过滤器进行卷积过程的例子。 分段最大值池化卷积输出矩阵$\\bf C$（$\\bf C \\in \\Bbb R^{\\it n \\times (s + w -1}$）的大小取决于输入网络中的句子里标记$s$的数量。为了应用后续的层，必须将卷积层抽取的特征进行组合，以使其与句子长度无关。在传统卷积神经网络(CNNs)中，最大值池化操作通常用于此目的（Collobert,2011;Zeng,2014）。这种类型的池化方案自然可以解决可变句子的长度问题。该想法是捕获每个特征映射中最重要的特征（最大值）。 然而，尽管最大值池化的使用非常广泛，但此方法不足以进行关系抽取。如第一部分所述，单个最大值池化会过快地减小隐藏层地大小，并且过于粗糙，无法捕获用于关系提取的细粒度特征。此外，单个最大值池化不足以捕捉两个实体之间的结构信息。在关系抽取中，输入的句子能够根据2个被选择的实体被划分为3个部分。因此，我们提出分段最大值池化过程，该过程将返回每个segment的最大值而不是单个最大值。如图3所示，每个卷积过滤器$c_i$的输出，通过Kojo Annan和Kofi Annan被划分为3个部分$\\{c_i1,c_i2,c_i3\\}$。分段最大值池化过程表达如下： $p_ij = max(\\bf c_ij) \\qquad \\text{$1 \\leq i \\leq n, 1 \\leq j \\leq 3$} \\tag{3}$ 对于每个卷积过滤器的输出，能够得到一个3维向量$\\bf p_i = \\it \\{p_i1,p_i2,p_i3\\}$。然后串联所有向量$\\bf p_{1:n}$，使用非线性方程，例如双曲正切（tanh）。最后，分段最大值池化处理输出一个向量：$\\bf g = \\it tanh(\\bf p_{1:n}\\it) \\tag{4}$其中$\\bf g \\in \\Bbb R ^{\\it 3n}$,$\\bf g$的大小是固定的，并且不再与句子长度相关。 Softmax输出为计算每个关系的置信度，将特征向量$\\bf g$输入softmax分类器。 $\\bf o = W_1g + \\it b \\tag{5}$ $\\bf W_1 \\in \\Bbb R ^{n_1 \\times 3n}$是转换矩阵，$\\bf o \\in \\Bbb R^{n_1}$是网络最终的输出，其中$n_1$等于关系抽取系统中可能的关系类型的数量。 我们在倒数第二层使用dropout进行正则化。Dropout 通过在正向计算的过程中让隐藏层的节点以$p$的概率不工作，来防止共适应(co-adaptation)。首先在$\\bf g$上使用掩蔽(masking)操作($\\bf g \\circ r$)，其中$r$是概率为p的伯努利随机变量的向量。等式(5)变为：$\\bf o = W1(g \\circ r) + b \\tag{6}$ 然后每个输出都可以被解释为对应关系的置信度得分。通过应用softmax操作，可以将此分数解释为条件概率（下一节）。在测试的过程中，学习的权重向量通过$p$进行缩放，以使$\\hat{W_1} = pW_1$，并用于（没有dropout）对未见的实例进行评分。 多实例学习我们在PCNNs上使用多实例学习，缓和wrong label问题。基于PCNN的关系提取可以表示为五元组$\\theta = (\\bf E,PF_1,PF_2,W,W_1)^2$。 网络的输入是一个bag。假设有$T$个bag$\\{M_1,M_2,\\cdots,M_T\\}$,第i个bag包含$q_i$个实例$M_i = \\{ m_{i}^{1}, m_{i}^{2}, \\cdots, m_{i}^{q_i} \\}$。多实例学习的目的是预测看不见的bag的标签。本文中认为一个bag中的所有实例都是独立的。给定输入实例$m_i^j$，参数为$\\theta$的网络输出向量$\\bf o$,其中第r个分量$o_r$对应与关系$r$相关的分数。为了获得条件概率$p(r\\mid m,\\theta)$ $p(r \\mid m_i^j;\\theta) = \\frac {e^{o_r}}{\\sum_{k = 1}^{n_1} {e^{o_k}} } \\tag{7}$ 多实例学习的目的是区分bags而不是实例。为此，我们必须在bags上定义目标函数。给定所有($T$)bags($M_i,y_i$)，我们可以在bag级别使用交叉熵定义目标函数，如下： 使用此定义的目标函数，我们通过我们使用Adadelta（Zeiler，2012）更新规则通过shuffled mini-batches上的随机梯度下降来最大化$J(\\theta)$,整个训练过程在算法1中进行了描述： 通过以上介绍，我们知道传统的反向传播算法会根据所有训练实例修改网络，而使用多实例学习的反向传播会修改基于bag的网络。 因此，我们的方法捕获了远程监督关系提取的本质，其中不可避免地会错误地标记一些训练实例。 当使用训练好的PCNN进行预测时，当且仅当网络的输出至少在bag中的一个实例上被标记为positive，才会对bag打positive标签。 实验实验为了证明以下假设：使用PCNNs和多实例学习来自动学习特征能够提升性能。为此，首先介绍使用的数据集和评估指标。接下来通过交叉验证测试几种变体以确定实验中使用的参数。然后将我们的方法的性能与几种传统方法的性能进行比较。最后评估分段最大值池化和多实例学习的效果。 数据集:使用了一个公开的数据集，KB是Freebase，而metric为precision/recall。实验Baseline部分包含：传统的Distant Supervision RE Baseline、包含Multiple-Instance改进的，同时包含Multiple-Instance+Multiple Labels改进的。实验显示该方法均高于Baseline模型不少。"},{"title":"【LeetCode】42.接雨水","date":"2019-10-22T07:06:48.000Z","path":"2019/10/22/leetcode-42/","text":"42.接雨水给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 感谢 Marcos 贡献此图。 示例: 输入: [0,1,0,2,1,0,1,3,2,1,2,1]输出: 6 题解分析解法1 - 按列求和求每一列的水，我们只需要关注当前列，以及左边最高的墙，右边最高的墙就够了。 装水的多少，当然根据木桶效应，我们只需要看左边最高的墙和右边最高的墙中较矮的一个就够了。 所以，根据较矮的那个墙和当前列的墙的高度可以分为三种情况。 较矮的墙的高度大于当前列的墙的高度 那么储水量就是高度差 较矮的墙的高度小于当前列的墙的高度 那么这一列就无法储水 较矮的墙的高度等于当前列的墙的高度 和上面一样，无法储水 那么算法就很简单啦，对于每一列，记录其左边和右边最高的墙，然后在这两个高度中选一个最小的和当前列进行对比，按照上面的情况进行计算即可。 代码 12345678910111213141516171819public int trap(int[] height) &#123; int sum = 0; int[] leftMax = new int[height.length]; int[] rightMax = new int[height.length]; for (int i = 1; i &lt; height.length - 1; i++) &#123; leftMax[i] = Math.max(leftMax[i - 1], height[i - 1]); &#125; for(int i = height.length - 2; i &gt;= 0; i--)&#123; rightMax[i] = Math.max(rightMax[i + 1], height[i + 1]); &#125; for(int i = 1; i &lt; height.length - 1; i++)&#123; int min = Math.min(leftMax[i],rightMax[i]); if(min &gt; height[i])&#123; sum += min - height[i]; &#125; &#125; return sum;&#125; 复杂度分析 假设height数组长度为n 时间复杂度：需要遍历3遍height数组，$O(3n)$空间复杂度：用了两个辅助数组存左右的最大值，$O(2n)$"},{"title":"【LeetCode】76.最小覆盖子串","date":"2019-10-22T03:41:15.000Z","path":"2019/10/22/leetcode-76/","text":"76.最小覆盖子串给你一个字符串 S、一个字符串 T，请在字符串 S 里面找出：包含 T 所有字母的最小子串。 示例： 输入: S = “ADOBECODEBANC”, T = “ABC”输出: “BANC” 说明： 如果 S 中不存这样的子串，则返回空字符串 “”。如果 S 中存在这样的子串，我们保证它是唯一的答案。 题解分析现在困难题也能秒A了，膨胀了哈哈哈。就是运行时间有点慢，改进改进，多向厉害的人学习。 解法1 - 双指针的滑动窗口还是和5232题的思路相似，可以用两个指针来模拟一个窗口，right指针用来扩充窗口，left指针用来缩小窗口，保证在窗口内的子串是满足要求的，然后求一个最小的窗口。 代码123456789101112131415161718192021222324252627282930313233343536373839404142public String minWindow(String s, String t) &#123; if(s == null || s.length() == 0 || t == null || t.length() == 0) return \"\"; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); for(int i = 0; i &lt; t.length(); i++)&#123; char ch = t.charAt(i); if(map.containsKey(ch))&#123; map.put(ch,map.get(ch) + 1); &#125;else&#123; map.put(ch,1); &#125; &#125; String res = \"\"; int minLen = s.length(); int left = 0, right = 0; while(left &lt;= right &amp;&amp; right &lt; s.length())&#123; char ch = s.charAt(right); boolean find = true; if(map.containsKey(ch))&#123; map.put(ch,map.get(ch) - 1); while(find)&#123; for(Map.Entry&lt;Character, Integer&gt; entry : map.entrySet())&#123; if(entry.getValue() &gt; 0)&#123; find = false; break; &#125; &#125; if(find)&#123; if(minLen &gt;= right - left + 1)&#123; minLen = right - left + 1; res = s.substring(left,right+1); &#125; if(map.containsKey(s.charAt(left)))&#123; map.put(s.charAt(left), map.get(s.charAt(left)) + 1); &#125; left++; &#125; &#125; &#125; right++; &#125; return res;&#125; 复杂度分析 时间复杂度：$O(|S| + |T|)$,$|S|$和$|T|$是$S$和$T$的长度，最坏情况下可能要对$S$的元素遍历两遍，左右指针各一遍。空间复杂度：$O(|S| + |T|)$。 解法2 - 解法1的优化解法1虽然AC了，但运行时间太长了，看了2ms的答案以后，感觉比我的简洁很多。 用数组来替代HashMap，因为只有英文字符。用count来指示窗口中包含的元素个数。也是遍历S，当count==0时验证长度并更新最小长度，每遍历一个字符都修改map和count，比我上面的方法少了很多循环验证，自然快了不少。 代码 1234567891011121314151617181920public String minWindow(String s, String t)&#123; int[] map = new int[128]; for(char ch : t.toCharArray()) map[ch]++; int count = t.length(); int minLen = s.length() + 1; int i = 0, j = 0, left = 0, right = 0; while(j &lt; s.length())&#123; if(map[s.charAt(j++)]-- &gt; 0) count--; while(count == 0)&#123; if(j - i &lt; minLen)&#123; minLen = j - i; left = i; right = j; &#125; if(++map[s.charAt(i++)] &gt; 0 ) count++; &#125; &#125; return minLen == s.length() + 1 ? \"\" : s.substring(left,right); &#125; 复杂度分析： 时间复杂度：$O(|S| + |T|)$空间复杂度：只用了一个固定长度的数组，$O(L)$，其中L为固定的长度。"},{"title":"【LeetCode】5231.删除子文件夹","date":"2019-10-21T07:40:56.000Z","path":"2019/10/21/leetcode-5231/","text":"5231.删除子文件夹你是一位系统管理员，手里有一份文件夹列表 folder，你的任务是要删除该列表中的所有子文件夹，并以任意顺序返回剩下的文件夹。 我们这样定义「子文件夹」： 如果文件夹 folder[i] 位于另一个文件夹 folder[j] 下，那么 folder[i] 就是 folder[j] 的子文件夹。 文件夹的「路径」是由一个或多个按以下格式串联形成的字符串： / 后跟一个或者多个小写英文字母。 例如，/leetcode 和 /leetcode/problems 都是有效的路径，而空字符串和 / 不是。 示例 1： 输入：folder = [“/a”,”/a/b”,”/c/d”,”/c/d/e”,”/c/f”]输出：[“/a”,”/c/d”,”/c/f”]解释：”/a/b/“ 是 “/a” 的子文件夹，而 “/c/d/e” 是 “/c/d” 的子文件夹。 示例 2： 输入：folder = [“/a”,”/a/b/c”,”/a/b/d”]输出：[“/a”]解释：文件夹 “/a/b/c” 和 “/a/b/d/“ 都会被删除，因为它们都是 “/a” 的子文件夹。 示例 3： 输入：folder = [“/a/b/c”,”/a/b/d”,”/a/b/ca”]输出：[“/a/b/c”,”/a/b/ca”,”/a/b/d”] 提示： 1 &lt;= folder.length &lt;= 4 * 10^4 2 &lt;= folder[i].length &lt;= 100 folder[i] 只包含小写字母和 / folder[i] 总是以字符 / 起始 每个文件夹名都是唯一的 题解分析这是第159场周赛的最后一题。 本次周赛做出了第1和第4题（菜的抠脚),中间那两道字符串的题没AC。 看完论文做那两个字符串题。（此处立flag） 论文看完回来把这两道题做了，其实也没那么难…… 解法1 - 没什么技巧的暴力解法首先对folder按字典序排序，那么子文件夹就会排列在父文件夹的后面，然后我们就遍历folder进行判断，这里使用indexOf()进行判断，对于文件夹/a，它肯定是子文件夹的子串，并且出现位置为0. 一种特殊情况是，/a/b/c和/a/b/cd在上面的判断中也会被判定为子文件夹，然而并不是，因此我们使用split对他俩进行分割，根据分割后的长度进行判定，因为子文件夹肯定比父文件夹在分割之后长度要长(数组长度）。 代码 123456789101112public List&lt;String&gt; removeSubfolders(String[] folder)&#123; Arrays.sort(folder); List&lt;String&gt; result = new ArrayList&lt;&gt;(); String cur = \"\"; for(String f : folder)&#123; if(result.isEmpty() || f.indexOf(cur) != 0 || (f.indexOf(cur) == 0 &amp;&amp; f.split(\"/\").length &lt;= cur.split(\"/\").length))&#123; result.add(f); cur = f; &#125; &#125; return result; &#125; 复杂度分析 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】5232.替换子串得到平衡字符串","date":"2019-10-21T06:54:19.000Z","path":"2019/10/21/leetcode-5232/","text":"5232.替换子串得到平衡字符串有一个只含有 ‘Q’, ‘W’, ‘E’, ‘R’ 四种字符，且长度为 n 的字符串。 假如在该字符串中，这四个字符都恰好出现 n/4 次，那么它就是一个「平衡字符串」。 给你一个这样的字符串 s，请通过「替换子串」的方式，使原字符串 s 变成一个「平衡字符串」。 你可以用和「待替换子串」长度相同的 任何 其他字符串来完成替换。 请返回待替换子串的最小可能长度。 如果原字符串自身就是一个平衡字符串，则返回 0。 示例 1： 输入：s = “QWER”输出：0解释：s 已经是平衡的了。 示例 2： 输入：s = “QQWE”输出：1解释：我们需要把一个 ‘Q’ 替换成 ‘R’，这样得到的 “RQWE” (或 “QRWE”) 是平衡的。 示例 3： 输入：s = “QQQW”输出：2解释：我们可以把前面的 “QQ” 替换成 “ER”。 示例 4： 输入：s = “QQQQ”输出：3解释：我们可以替换后 3 个 ‘Q’，使 s = “QWER”。 提示： 1 &lt;= s.length &lt;= 10^5s.length 是 4 的倍数s 中只含有 ‘Q’, ‘W’, ‘E’, ‘R’ 四种字符 题解分析这是第159场周赛的最后一题。 本次周赛做出了第1和第4题（菜的抠脚),中间那两道字符串的题没AC。 看完论文做那两个字符串题。（此处立flag） 论文看完回来把这两道题做了，其实也没那么难…… 解法1 - 双指针滑动窗口明确几个点，字符串中只包含Q,W,E,R。并且每个字母出现次数为s.length/4时，才是平衡的字符串，我们要将字符串S进行替换，让它变成符合这种要求的新字符串。 到这里思路就是，统计s中各个字符出现的次数，然后将多于s.length/4的个数拿出来就得到解了。 然而周赛时我就是这个思路一直不能AC，再仔细分析题目，发现，需要用子串做替换。那我们接着上面的思路，就可以用两个指针标识一段区间（滑动窗口），保证替换掉这个区间内的字符能满足要求，然后求这个窗口的最小值。 区间的判定举一个例子来说明。 例如我们有s = &quot;QWWQQEQW&quot;，此时长度len = 8，那么成为平衡字符串时每个字母必须出现的次数为num = 8/4 = 2. 用一个Mapmap来统计s中字符出现的次数，此时map为： {Q:4,W:3,E:1} 我们需要将多出来的 Q和W进行替换，也就是说我们要找一个窗口，窗口中Q和W的数量要大于等于num=2，大于也没关系我们可以将它替换为该字母本身。 于是将Map进行修改，将每个字母出现次数减去num。此时Map中value代表的含义是：窗口中需要的字符对应的个数. {Q:2,W:1,E:-1} 设置两个指针left和right初始在0位置。 画图更好理解 代码 12345678910111213141516171819202122232425262728293031323334353637public int balancedString(String s) &#123; int num = s.length() / 4; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; s.length(); i++) &#123; char ch = s.charAt(i); if (map.containsKey(ch)) &#123; map.put(ch, map.get(ch) + 1); &#125; else map.put(ch, 1); &#125; boolean balance = true; char[] chs = &#123;'Q', 'W', 'E', 'R'&#125;; for (char ch : chs) &#123; if (!map.containsKey(ch) || map.get(ch) != num) balance = false; if(map.containsKey(ch)) map.put(ch, map.get(ch) - num); &#125; if (balance) return 0; int left = 0, right = 0, result = s.length(); while(left &lt;= right &amp;&amp; right &lt; s.length())&#123; map.put(s.charAt(right),map.get(s.charAt(right)) - 1); boolean find = true; while(find)&#123; for(char ch : chs)&#123; if(map.containsKey(ch) &amp;&amp; map.get(ch) &gt; 0)&#123; find = false; break; &#125; &#125; if(find)&#123; result = Math.min(result, right - left + 1); map.put(s.charAt(left),map.get(s.charAt(left)) + 1); left++; &#125; &#125; right++; &#125; return result;&#125; 复杂度分析 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】5230.缀点成线","date":"2019-10-20T08:41:29.000Z","path":"2019/10/20/leetcode-5230/","text":"5230.缀点成线在一个 XY 坐标系中有一些点，我们用数组 coordinates 来分别记录它们的坐标，其中 coordinates[i] = [x, y] 表示横坐标为 x、纵坐标为 y 的点。 请你来判断，这些点是否在该坐标系中属于同一条直线上，是则返回 true，否则请返回 false. 示例 1： 输入：coordinates = [[1,2],[2,3],[3,4],[4,5],[5,6],[6,7]]输出：true 示例 2： 输入：coordinates = [[1,1],[2,2],[3,4],[4,5],[5,6],[7,7]]输出：false 提示： 2 &lt;= coordinates.length &lt;= 1000coordinates[i].length == 2-10^4 &lt;= coordinates[i][0], coordinates[i][1] &lt;= 10^4coordinates 中不含重复的点 题解分析这是第159场周赛的最后一题。 本次周赛做出了第1和第4题（菜的抠脚),中间那两道字符串的题没AC。 看完论文做那两个字符串题。（此处立flag） 解法1 - 斜率法读完题目立刻就有的思路就是，我们随便求两个点的斜率，然后拿其他点带进来验证，如果有不相同的情况，那么一定不在一条直线，直接return false，如果都通过了验证再return true。 光顾着时间了，咔咔写完一提交，分母的情况没考虑。 也就是说，如果我们找的两个点x坐标相同，那么此时分母为0，不可以继续做除法，那么就需要换一个点。 斜率公式： $k = \\frac{y_2-y_1}{x_2-x_1} \\tag{1}$或者$k = \\frac{y_1-y_2}{x_1-x_2} \\tag{2}$ 代码 1234567891011121314151617181920public boolean checkStraightLine(int[][] coordinates) &#123; int left = 0, right = coordinates.length - 1; double k = 0; while(left &lt; right)&#123; if(coordinates[left][0] != coordinates[right][0])&#123; k = (double)Math.abs(coordinates[left][1] - coordinates[right][1]) / Math.abs(coordinates[left][0] - coordinates[right][0]); break; &#125;else left++; &#125; if(left == right) return true;//垂直 for(int i = 0; i &lt; coordinates.length - 1;i++)&#123; double a =(double) Math.abs(coordinates[i][1] - coordinates[i+1][1]); double b = (double)Math.abs(coordinates[i][0] - coordinates[i+1][0]); if(b == 0) return false; //如果在这里出现垂直，那么一定不共线 double current = a/b; if(current != k) return false; &#125; return true;&#125; 复杂度分析： 假设coordinates长度为n 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】5233.规划兼职工作","date":"2019-10-20T08:07:15.000Z","path":"2019/10/20/leetcode-5233/","text":"5233.规划兼职工作你打算利用空闲时间来做兼职工作赚些零花钱。 这里有 n 份兼职工作，每份工作预计从 startTime[i] 开始到 endTime[i] 结束，报酬为 profit[i]。 给你一份兼职工作表，包含开始时间 startTime，结束时间 endTime 和预计报酬 profit 三个数组，请你计算并返回可以获得的最大报酬。 注意，时间上出现重叠的 2 份工作不能同时进行。 如果你选择的工作在时间 X 结束，那么你可以立刻进行在时间 X 开始的下一份工作。 示例 1： 输入：startTime = [1,2,3,3], endTime = [3,4,5,6], profit = [50,10,40,70]输出：120解释：我们选出第 1 份和第 4 份工作，时间范围是 [1-3]+[3-6]，共获得报酬 120 = 50 + 70。 示例 2： 输入：startTime = [1,2,3,4,6], endTime = [3,5,10,6,9], profit = [20,20,100,70,60]输出：150解释：我们选择第 1，4，5 份工作。共获得报酬 150 = 20 + 70 + 60。 示例 3： 输入：startTime = [1,1,1], endTime = [2,3,4], profit = [5,6,4]输出：6 提示： 1 &lt;= startTime.length == endTime.length == profit.length &lt;= 5 * 10^41 &lt;= startTime[i] &lt; endTime[i] &lt;= 10^91 &lt;= profit[i] &lt;= 10^4 题解分析这是第159场周赛的最后一题。 本次周赛做出了第1和第4题（菜的抠脚),中间那两道字符串的题没AC。 看完论文做那两个字符串题。（此处立flag） 解法1 - 动态规划分析题目，有几个条件需要明确。1.某个job结束后可以立刻开始下一个job。2.无法重叠进行工作。 思路：明确dp的定义： dp[i]:在i时刻，能赚到的钱数的最大值。 $ dp(i) = \\begin{cases} dp(i-1), &amp; \\text{i时刻没有要结束的任务}\\\\ max(dp(i),dp(start_i) + profit(i)), &amp; \\text{i时刻有结束的任务,$start_i$表示结束时刻$i$对应的开始时刻} \\end{cases}$解释一下方程，当i时刻有要结束的任务时，就在当前dp(i)和对应起始点的dp(i)+profit(i)选一个最大值，为什么是起始点呢？因为如果选择做当前的这个job，那么当前任务的起始点相当于上一个做过的job的最晚结束点。 列好方程，下一步就是按照endTime排序，找到endTime中最晚结束的时间last,我也把它记作len，用来初始化dp数组。 具体求解时，从1 ~ len填充dp数组，同时维护一个index，用来遍历job对应的数组。 这里有个小trick，需要对endTime排序，那么相应的startTime和profit也得跟着排序，我习惯把他们放在一个二维数组中，行就是原始数据个数，列就是这三个属性，然后lambda表达式排序爽歪歪。 代码 1234567891011121314151617181920public int jobScheduling(int[] startTime, int[] endTime, int[] profit) &#123; int[][] data = new int[startTime.length][3]; for (int i = 0; i &lt; startTime.length; i++) &#123; data[i][0] = endTime[i]; data[i][1] = startTime[i]; data[i][2] = profit[i]; &#125; Arrays.sort(data, (d1, d2) -&gt; d1[0] == d2[0] ? d1[1] - d2[1] : d1[0] - d2[0]); int len = data[startTime.length - 1][0]; int[] dp = new int[len + 1]; int index = 0; for (int i = 1; i &lt;= len; i++) &#123; dp[i] = dp[i - 1]; while (index &lt; data.length &amp;&amp; i == data[index][0]) &#123; dp[i] = Math.max(dp[i], dp[data[index][1]] + data[index][2]); index++; &#125; &#125; return dp[len];&#125; 复杂度分析：假设startTime长度为n，最晚结束的时间为t 时间复杂度：$O(max(n,t))$空间复杂度：$O(3n) = O(n)$"},{"title":"【LeetCode】167.两数之和Ⅱ - 输入有序数组","date":"2019-10-19T00:48:46.000Z","path":"2019/10/19/leetcode-167/","text":"167.两数之和Ⅱ - 输入有序数组给定一个已按照升序排列的有序数组，找到两个数使得它们相加之和等于目标数。 函数应该返回这两个下标值 index1 和 index2，其中 index1 必须小于 index2。 说明: 返回的下标值（index1 和 index2）不是从零开始的。 你可以假设每个输入只对应唯一的答案，而且你不可以重复使用相同的元素。 示例: 输入: numbers = [2, 7, 11, 15], target = 9输出: [1,2]解释: 2 与 7 之和等于目标数 9 。因此 index1 = 1, index2 = 2 。 题解分析解法1 - 左右双指针这道题和第一题Two Sum的不同之处在于，它是排序的数组，因此我们可以利用这个特性来求解。 设定左指针left指向数组第一个元素，右指针right指向数组最后一个元素，从两头向中间移动。算法如下：1.计算sum = numbers[left] + numbers[right];2.当sum &gt; target时，将right向左移动一位3.当sum &lt; target时，将left向右移动一位4.否则，将left和right返回。 ps.之前的3数之和、四数之和也都有先给数组排序的这个过程。做题顺序还是很重要的。 代码 12345678910111213141516public int[] twoSum(int[] numbers, int target) &#123; if(numbers == null || numbers.length == 0)return new int[]&#123;&#125;; int[] result = new int[2]; int left = 0, right = numbers.length - 1; while(left &lt; right)&#123; int sum = numbers[left] + numbers[right]; if(sum &lt; target) left++; else if(sum &gt; target) right--; else &#123; result[0] = left+1; result[1] = right+1; break; &#125; &#125; return result;&#125; 复杂度分析： 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】86.分隔链表","date":"2019-10-18T02:57:34.000Z","path":"2019/10/18/leetcode-86/","text":"86.分隔链表给定一个链表和一个特定值 x，对链表进行分隔，使得所有小于 x 的节点都在大于或等于 x 的节点之前。 你应当保留两个分区中每个节点的初始相对位置。 示例: 输入: head = 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;2, x = 3输出: 1-&gt;2-&gt;2-&gt;4-&gt;3-&gt;5 题解分析解法1 - 双指针拆解大法通过分析题意可以得出，在分隔后的链表中，有一个分界点，他前边的元素都比特定值x小，后边都大于等于特定值。 我们可以把这两部分分别拆成2个链表，之后再把他们相连。 具体的，算法如下：1.设置p_head与q_head哑节点。并初始化p和q指针指向他们。2.遍历原链表，其值小于x时，用p指针连接它，否则用q指针连接它.3.遍历结束后，p链接到q，q的末尾设置为null。 代码 1234567891011121314151617181920public ListNode partition(ListNode head, int x) &#123; ListNode p_head = new ListNode(0); ListNode q_head = new ListNode(0); ListNode p = p_head; ListNode q = q_head; while(head != null)&#123; if(head.val &lt; x)&#123; p.next = head; p = p.next; &#125;else&#123; q.next = head; q = q.next; &#125; head = head.next; &#125; p.next = q_head.next; q.next = null; return p_head.next;&#125; 复杂度分析 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】28.实现strStr() (待学习补充各种算法）","date":"2019-10-18T02:29:53.000Z","path":"2019/10/18/leetcode-28/","text":"28.实现strStr()实现 strStr() 函数。 给定一个 haystack 字符串和一个 needle 字符串，在 haystack 字符串中找出 needle 字符串出现的第一个位置 (从0开始)。如果不存在，则返回 -1。 示例 1: 输入: haystack = “hello”, needle = “ll”输出: 2 示例 2: 输入: haystack = “aaaaa”, needle = “bba”输出: -1 说明: 当 needle 是空字符串时，我们应当返回什么值呢？这是一个在面试中很好的问题。 对于本题而言，当 needle 是空字符串时我们应当返回 0 。这与C语言的 strstr() 以及 Java的 indexOf() 定义相符。 题解分析解法1 - 原生indexOf()的仿写这是String类提供的比较函数，当然我们不能直接调库，要不也太… 参考indexOf()的写法来实现该方法，具体思考参考我之前写过的博客:【LeetCode刷题日记】28.strStr 代码 1234567891011121314151617181920212223public int strStr(String haystack, String needle) &#123; int aLen = haystack.length(); int bLen = needle.length(); if(bLen == 0) return 0; char first = needle.charAt(0); int max = aLen - bLen; for(int i = 0; i &lt;= max; i++)&#123; if(haystack.charAt(i) != first)&#123; while(++i &lt;= max &amp;&amp; haystack.charAt(i) != first); &#125; if(i &lt;= max)&#123; int j = i + 1; int end = j + (bLen - 1); for(int k = 1; j &lt; end &amp;&amp; haystack.charAt(j) == needle.charAt(k);j++,k++); if(j == end)&#123; return i; &#125; &#125; &#125; return -1;&#125; 复杂度分析：假设haystack长度为n，needle长度为m 时间复杂度：当原字符串中不存在子串时，那么扫描一遍hystack算法结束，此时$O(n-m+1)$当needle出现在haystack的末尾，前面都只有neddle的部分，均匹配失败，比较的次数$O(n)$因此时间复杂度$O(n)$空间复杂度：$O(1)$ 补充2 - 如何寻找haystack中needle出现的全部位置呢？这在之前的30题中已经提到过了，是对上面方法的一点点改动就能解决。 1234567891011121314151617181920212223242526272829//求子串b在a中的位置，这里对原生strstr有改编，求b在a中出现的所有位置//题外话：这个方法可以单独拿出来和strstr一起比较。private List&lt;Integer&gt; strStr(String a, String b) &#123; int aLen = a.length(); int bLen = b.length(); if (bLen == 0) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); res.add(0); return res; &#125; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); char first = b.charAt(0); int max = aLen - bLen; for (int i = 0; i &lt;= max; i++) &#123; if (a.charAt(i) != first) &#123;//找到第一个元素出现的位置 while (++i &lt;= max &amp;&amp; a.charAt(i) != first) ; &#125; if (i &lt;= max) &#123; int j = i + 1; int end = j + (bLen - 1); for (int k = 1; j &lt; end &amp;&amp; a.charAt(j) == b.charAt(k); k++, j++) ; if (j == end) &#123; result.add(i);//这里不return了，而是添加到结果列表中。 &#125; &#125; &#125; return result;&#125; 复杂度分析： 时间复杂度：$O(n)$空间复杂度：$O(n)$"},{"title":"【LeetCode】61.旋转链表","date":"2019-10-17T02:33:18.000Z","path":"2019/10/17/leetcode-61/","text":"61.旋转链表给定一个链表，旋转链表，将链表每个节点向右移动 k 个位置，其中 k 是非负数。 示例 1: 输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL, k = 2输出: 4-&gt;5-&gt;1-&gt;2-&gt;3-&gt;NULL解释:向右旋转 1 步: 5-&gt;1-&gt;2-&gt;3-&gt;4-&gt;NULL向右旋转 2 步: 4-&gt;5-&gt;1-&gt;2-&gt;3-&gt;NULL 示例 2: 输入: 0-&gt;1-&gt;2-&gt;NULL, k = 4输出: 2-&gt;0-&gt;1-&gt;NULL解释:向右旋转 1 步: 2-&gt;0-&gt;1-&gt;NULL向右旋转 2 步: 1-&gt;2-&gt;0-&gt;NULL向右旋转 3 步: 0-&gt;1-&gt;2-&gt;NULL向右旋转 4 步: 2-&gt;0-&gt;1-&gt;NULL 题解分析解法1 - 双指针大法首先分析题目，向右旋转其实意味着链表右移，从右往左数第k个节点将成为新的头结点，要注意几个条件： 1.当k大于链表长度时，需要k对长度len取余。2.当取余结果为0时说明不需要移动。3.当k本身为0时也不需要移动。 算法流程如下： 1.求链表长度, 求完长度指针p指向链表末尾节点。2.计算需要移动的步数step，step = len - (k % len)3.s指针和f指针初始指向头部，f先走1步，然后f和s同时走step - 1步，此时f指向新的头结点，s指向新的尾节点。4.设置s.next为null5.设置p.next指向head6.返回f节点，即新的头结点 代码 12345678910111213141516171819202122232425262728293031public ListNode rotateRight(ListNode head, int k) &#123; // k 为0时，不需要旋转。 if(head == null || head.next == null || k == 0) return head; int len = 0; //长度 ListNode p = head; ListNode f = head; ListNode s = head; //求链表长度，p最后指向链表末尾节点 while(p != null)&#123; len++; p = p.next; &#125; //如果是长度的倍数则不需要旋转 if(k % len == 0) return head; // 右移步数，k % len的位置是新的头结点，因此len - (k % len)为从head到这个头结点需要移动的步数。 int step = len - (k % len); //f先走一步，是为了让slow移动到f的前一个节点，f的前一个节点为新链表末尾节点。 f = f.next; //f先走了一步因此step-1，原本step判定条件为step &gt; 0 while(step &gt; 1)&#123; f = f.next; s = s.next; step--; &#125; //s指向的位置为新的尾节点 s.next = null; //连在头节点上 p.next = head; //返回新的头节点 return f;&#125; 复杂度分析： 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】19.删除链表的倒数第N个节点","date":"2019-10-17T01:15:01.000Z","path":"2019/10/17/leetcode-19/","text":"19.删除链表的倒数第N个节点给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。 示例： 给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 n = 2. 当删除了倒数第二个节点后，链表变为 1-&gt;2-&gt;3-&gt;5. 说明： 给定的 n 保证是有效的。 进阶： 你能尝试使用一趟扫描实现吗？ 题解分析解法1 - 双指针一趟搞定我们可以设置2个指针，一个fast指针，一个slow指针。 先让fast向前走n步，然后让fast和slow同时往前走，直到fast的下一个节点为null停止，此时让slow.next = slow.next.next，就相当于删掉了倒数第n个节点。思路是这样，但却会在边界上出问题，需要单独写判断条件，那么进一步考虑。 设置一个哑节点d，让它的next指向head，同时fast和slow都指向d，于是fast要先走n+1步了，随后slow和fast同步走，当fast为null时停止，此时让slow.next = slow.next.next，就删掉了倒数第n个节点，对边界的条件同样满足，最后返回d.next即可。 代码 12345678910111213141516171819public ListNode removeNthFromEnd(ListNode head, int n) &#123; if(head == null) return null; ListNode d = new ListNode(0); d.next = head; ListNode fast = d; int step = n; ListNode slow = d; while(step &gt;= 0)&#123; fast = fast.next; step--; &#125; while(fast != null)&#123; fast = fast.next; slow = slow.next; &#125; slow.next = slow.next.next; return d.next;&#125; 复杂度分析 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】30.串联所有单词的子串","date":"2019-10-16T09:12:08.000Z","path":"2019/10/16/leetcode-30/","text":"30.串联所有单词的子串给定一个字符串 s 和一些长度相同的单词words。找出 s 中恰好可以由 words 中所有单词串联形成的子串的起始位置。 注意子串要与 words 中的单词完全匹配，中间不能有其他字符，但不需要考虑 words 中单词串联的顺序。 示例 1： 输入： s = “barfoothefoobarman”, words = [“foo”,”bar”]输出：[0,9]解释：从索引 0 和 9 开始的子串分别是 “barfoo” 和 “foobar” 。输出的顺序不重要, [9,0] 也是有效答案。 示例 2： 输入： s = “wordgoodgoodgoodbestword”, words = [“word”,”good”,”best”,”word”]输出：[] 题解分析解法1 - 原始解法（超时）要在s中找子串出现的所有位置，所以一开始我想找到words的所有组合，然后对每组words串联的单词在s中查找位置，不过超时了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// 在这里对所有的子串进行判定，index不为-1即在s中存在子串public List&lt;Integer&gt; findSubstring(String s, String[] words) &#123; if(s == null || s.length() == 0) return new ArrayList&lt;&gt;(); List&lt;String&gt; wordsList = getWordsList(words); List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); for(String str : wordsList)&#123; List&lt;Integer&gt; indexList = strStr(s,str); for(int index : indexList)&#123; if(index != -1)&#123; result.add(index); &#125; &#125; &#125; return result;&#125;//求words的排列private List&lt;String&gt; getWordsList(String[] words)&#123; List&lt;String&gt; result = new ArrayList&lt;&gt;(); if(words == null || words.length == 0) return result; Arrays.sort(words); boolean[] used = new boolean[words.length]; doPermute(words,result,new ArrayList(),used); return result;&#125;// words中存在重复元素，求不重复的排列private void doPermute(String[] words, List&lt;String&gt; result,List&lt;String&gt; list,boolean[] used)&#123; if(list.size() == words.length)&#123; result.add(transfer(list)); &#125;else&#123; for(int i = 0; i &lt; words.length; i++)&#123; if(!used[i])&#123; if(i &gt; 0 &amp;&amp; words[i].equals(words[i - 1]) &amp;&amp; !used[i - 1])continue; list.add(words[i]); used[i] = true; doPermute(words,result,list,used); list.remove(list.size() - 1); used[i] = false; &#125; &#125; &#125;&#125;//将排列好的list转换成字符串private String transfer(List&lt;String&gt; list)&#123; StringBuilder sb = new StringBuilder(); for(String s : list)&#123; sb.append(s); &#125; return sb.toString();&#125;//求子串b在a中的位置，这里对原生strstr有改编，求b在a中出现的所有位置//题外话：这个方法可以单独拿出来和strstr一起比较。private List&lt;Integer&gt; strStr(String a, String b) &#123; int aLen = a.length(); int bLen = b.length(); if (bLen == 0) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); res.add(0); return res; &#125; List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); char first = b.charAt(0); int max = aLen - bLen; for (int i = 0; i &lt;= max; i++) &#123; if (a.charAt(i) != first) &#123;//找到第一个元素出现的位置 while (++i &lt;= max &amp;&amp; a.charAt(i) != first) ; &#125; if (i &lt;= max) &#123; int j = i + 1; int end = j + (bLen - 1); for (int k = 1; j &lt; end &amp;&amp; a.charAt(j) == b.charAt(k); k++, j++) ; if (j == end) &#123; result.add(i); &#125; &#125; &#125; return result;&#125; 解法2 - 利用两个Map我们可以将words中的单词对应在一个Map中，单词作为key，出现次数作为value。 将words中所有单词组成的串的长度记为all_len,每个单词长度为word_len，对s从0开始取all_len长度的子串，对子串按照word_len进行分割，存在一个Map中，最后比较两个Map是否相同，如果相同则将i加入结果列表中。 代码1234567891011121314151617181920212223public List&lt;Integer&gt; findSubstring(String s, String[] words)&#123; if(s == null || s.length() == 0 || words == null || words.length == 0) return new ArrayList&lt;&gt;(); List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); int word_len = words[0].length(); int word_num = words.length; int all_len = word_len * word_num; Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); for(String word : words)&#123; map.put(word, map.getOrDefault(word,0) + 1); &#125; for(int i = 0; i &lt;= s.length() - all_len; i++)&#123; String temp = s.substring(i, i + all_len); Map&lt;String, Integer&gt; temp_map = new HashMap&lt;&gt;(); for(int j = 0; j &lt; all_len; j += word_len)&#123; String t_w = temp.substring(j,j+word_len); temp_map.put(t_w, temp_map.getOrDefault(t_w,0) + 1); &#125; if(map.equals(temp_map)) result.add(i); &#125; return result; &#125; 复杂度分析： 假设s长度为n，words大小为m 时间复杂度：$O(n^2)$空间复杂度：$O(m)$"},{"title":"【LeetCode】18.四数之和","date":"2019-10-16T08:49:25.000Z","path":"2019/10/16/leetcode-18/","text":"18.四数之和给定一个包含 n 个整数的数组 nums 和一个目标值 target，判断 nums 中是否存在四个元素 a，b，c 和 d ，使得 a + b + c + d 的值与 target 相等？找出所有满足条件且不重复的四元组。 注意： 答案中不可以包含重复的四元组。 示例： 给定数组 nums = [1, 0, -1, 0, -2, 2]，和 target = 0。 满足要求的四元组集合为：[ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2]] 题解分析解法 - 延伸的三数之和（双指针）在三数之和的基础上再套一个for循环就可以求解了。 代码 123456789101112131415161718192021222324252627282930313233343536public List&lt;List&lt;Integer&gt;&gt; fourSum(int[] nums, int target) &#123; if(nums == null || nums.length &lt; 4) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); Arrays.sort(nums); int len = nums.length; for(int i = 0; i &lt; len - 3; i++)&#123; //1.最小的4个相加大于target意味后面找不到解了 if(nums[i] + nums[i + 1] + nums[i + 2] + nums[i + 3] &gt; target) break; //最小的加3个最大的都小于target，那么这个最小的不考虑了。 if(nums[i] + nums[len - 1] + nums[len - 2] + nums[len - 3] &lt; target) continue; //去重 if(i &gt; 0 &amp;&amp; nums[i] == nums[i - 1])continue; for(int j = i + 1; j &lt; len - 2; j++)&#123; if(nums[i] + nums[j] + nums[j+1] + nums[j + 2] &gt; target) break; if(nums[i] + nums[j] + nums[len - 1] + nums[len - 2] &lt; target) continue; if(j &gt; i + 1 &amp;&amp; nums[j] == nums[j - 1]) continue; int left = j + 1, right = len - 1; while(left &lt; right)&#123; int sum = nums[i] + nums[j] + nums[left] + nums[right]; if(sum == target)&#123; result.add(Arrays.asList(nums[i], nums[j], nums[left], nums[right])); while(left &lt; right &amp;&amp; nums[left] == nums[left + 1])left++; while(left &lt; right &amp;&amp; nums[right] == nums[right - 1])right--; left++; right--; &#125;else if(sum &lt; target)&#123; left++; &#125;else&#123; right--; &#125; &#125; &#125; &#125; return result;&#125; 复杂度分析： 时间复杂度：$O(n^3)$空间复杂度：$O(1)$"},{"title":"【问题集锦】安装Keras问题总结(持续更新)","date":"2019-10-15T13:35:44.000Z","path":"2019/10/15/keras-problem-set/","text":"PyCharm 永久破解解决1.下载破解补丁 链接: https://pan.baidu.com/s/1QIt0v9XRYlXDbCuLHZ9XiQ 提取码: 871k 复制这段内容后打开百度网盘手机App，操作更方便哦 2.把补丁放在Pycharm的bin目录下 3.在激活界面，选Activate code，输入： MTW881U3Z5-eyJsaWNlbnNlSWQiOiJNVFc4ODFVM1o1IiwibGljZW5zZWVOYW1lIjoiTnNzIEltIiwiYXNzaWduZWVOYW1lIjoiIiwiYXNzaWduZWVFbWFpbCI6IiIsImxpY2Vuc2VSZXN0cmljdGlvbiI6IkZvciBlZHVjYXRpb25hbCB1c2Ugb25seSIsImNoZWNrQ29uY3VycmVudFVzZSI6ZmFsc2UsInByb2R1Y3RzIjpbeyJjb2RlIjoiSUkiLCJwYWlkVXBUbyI6IjIwMTktMTEtMDYifSx7ImNvZGUiOiJBQyIsInBhaWRVcFRvIjoiMjAxOS0xMS0wNiJ9LHsiY29kZSI6IkRQTiIsInBhaWRVcFRvIjoiMjAxOS0xMS0wNiJ9LHsiY29kZSI6IlBTIiwicGFpZFVwVG8iOiIyMDE5LTExLTA2In0seyJjb2RlIjoiR08iLCJwYWlkVXBUbyI6IjIwMTktMTEtMDYifSx7ImNvZGUiOiJETSIsInBhaWRVcFRvIjoiMjAxOS0xMS0wNiJ9LHsiY29kZSI6IkNMIiwicGFpZFVwVG8iOiIyMDE5LTExLTA2In0seyJjb2RlIjoiUlMwIiwicGFpZFVwVG8iOiIyMDE5LTExLTA2In0seyJjb2RlIjoiUkMiLCJwYWlkVXBUbyI6IjIwMTktMTEtMDYifSx7ImNvZGUiOiJSRCIsInBhaWRVcFRvIjoiMjAxOS0xMS0wNiJ9LHsiY29kZSI6IlBDIiwicGFpZFVwVG8iOiIyMDE5LTExLTA2In0seyJjb2RlIjoiUk0iLCJwYWlkVXBUbyI6IjIwMTktMTEtMDYifSx7ImNvZGUiOiJXUyIsInBhaWRVcFRvIjoiMjAxOS0xMS0wNiJ9LHsiY29kZSI6IkRCIiwicGFpZFVwVG8iOiIyMDE5LTExLTA2In0seyJjb2RlIjoiREMiLCJwYWlkVXBUbyI6IjIwMTktMTEtMDYifSx7ImNvZGUiOiJSU1UiLCJwYWlkVXBUbyI6IjIwMTktMTEtMDYifV0sImhhc2giOiIxMDgyODE0Ni8wIiwiZ3JhY2VQZXJpb2REYXlzIjowLCJhdXRvUHJvbG9uZ2F0ZWQiOmZhbHNlLCJpc0F1dG9Qcm9sb25nYXRlZCI6ZmFsc2V9-aKyalfjUfiV5UXfhaMGgOqrMzTYy2rnsmobL47k8tTpR/jvG6HeL3FxxleetI+W+Anw3ZSe8QAMsSxqVS4podwlQgIe7f+3w7zyAT1j8HMVlfl2h96KzygdGpDSbwTbwOkJ6/5TQOPgAP86mkaSiM97KgvkZV/2nXQHRz1yhm+MT+OsioTwxDhd/22sSGq6KuIztZ03UvSciEmyrPdl2ueJw1WuT9YmFjdtTm9G7LuXvCM6eav+BgCRm+wwtUeDfoQqigbp0t6FQgkdQrcjoWvLSB0IUgp/f4qGf254fA7lXskT2VCFdDvi0jgxLyMVct1cKnPdM6fkHnbdSXKYDWw==-MIIElTCCAn2gAwIBAgIBCTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE4MTEwMTEyMjk0NloXDTIwMTEwMjEyMjk0NlowaDELMAkGA1UEBhMCQ1oxDjAMBgNVBAgMBU51c2xlMQ8wDQYDVQQHDAZQcmFndWUxGTAXBgNVBAoMEEpldEJyYWlucyBzLnIuby4xHTAbBgNVBAMMFHByb2QzeS1mcm9tLTIwMTgxMTAxMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQAF8uc+YJOHHwOFcPzmbjcxNDuGoOUIP+2h1R75Lecswb7ru2LWWSUMtXVKQzChLNPn/72W0k+oI056tgiwuG7M49LXp4zQVlQnFmWU1wwGvVhq5R63Rpjx1zjGUhcXgayu7+9zMUW596Lbomsg8qVve6euqsrFicYkIIuUu4zYPndJwfe0YkS5nY72SHnNdbPhEnN8wcB2Kz+OIG0lih3yz5EqFhld03bGp222ZQCIghCTVL6QBNadGsiN/lWLl4JdR3lJkZzlpFdiHijoVRdWeSWqM4y0t23c92HXKrgppoSV18XMxrWVdoSM3nuMHwxGhFyde05OdDtLpCv+jlWf5REAHHA201pAU6bJSZINyHDUTB+Beo28rRXSwSh3OUIvYwKNVeoBY+KwOJ7WnuTCUq1meE6GkKc4D/cXmgpOyW/1SmBz3XjVIi/zprZ0zf3qH5mkphtg6ksjKgKjmx1cXfZAAX6wcDBNaCL+Ortep1Dh8xDUbqbBVNBL4jbiL3i3xsfNiyJgaZ5sX7i8tmStEpLbPwvHcByuf59qJhV/bZOl8KqJBETCDJcY6O2aqhTUy+9x93ThKs1GKrRPePrWPluud7ttlgtRveit/pcBrnQcXOl1rHq7ByB8CFAxNotRUYL9IF5n3wJOgkPojMy6jetQA5Ogc8Sm7RG6vg1yow== 无法定位程序输入点OPENSSL_sk_new_reserve于动态链接库解决1.打开Anaconda安装位置下的DLLs，如D:\\Anaconda3\\DLLs 2.查看libssl-1_1-x64.ddl的日期 3.进入\\Anaconda3\\Library\\bin目录，找到同名文件比对日期，如果不同则用DLLs的替换bin下的。（记得备份原文件) Anaconda 切换环境查看环境conda info -e 切换环境conda activate tensorflow 撤销切换conda deactivate PyCharm 一直出现 connecting to console问题找不到ssl 是因为本机没有安装openssl导致的。 解决下载openssl，安装后重新打开PyCharm即可解决。"},{"title":"【LeetCode】16.最接近的三数之和","date":"2019-10-15T06:26:36.000Z","path":"2019/10/15/leetcode-16/","text":"16.最接近的三数之和给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。 例如，给定数组 nums = [-1，2，1，-4], 和 target = 1. 与 target 最接近的三个数的和为 2. (-1 + 2 + 1 = 2). 题解分析解法1 - 15题的双指针移植按照15题的方法，我们可以直接用双指针，每取一个元素，就令left为它下一个元素的索引，right为数组末尾，计算此时的3数之和，维护一个closed变量保存最接近的值，通过缩短left与right的区间，不断地迭代更新这个值。 代码 12345678910111213141516171819public int threeSumClosest(int[] nums, int target) &#123; if(nums.length&lt;=2)return 0; Arrays.sort(nums); int closest = nums[0] + nums[1] + nums[2]; for(int i = 0; i&lt;nums.length-2;i++)&#123; int l = i + 1; int r = nums.length - 1; while(l &lt; r)&#123; int threeSum = nums[i] + nums[l] + nums[r]; if(Math.abs(threeSum - target) &lt; Math.abs(closest - target))&#123; closest = threeSum; &#125; if(threeSum &gt; target) r--; else if(threeSum &lt; target) l++; else return target; &#125; &#125; return closest;&#125; 复杂度分析： 时间复杂度：$O(n^2)$空间复杂度：$O(1)$ 解法2 - 优化参考15题的优化思路，当我们确定i，left及right的位置后，需要不断向中间逼近，那么什么时候不需要这么做呢？ 考虑，当取最小值min，即nums[i] + nums[left] + nums[left + 1]时，如果min大于target，那就说明即便是往后逼近，也不会取到更小的值了，此时可以直接按照条件更新closed，结束本次循环。 同理对于最大值也是一样。 关于元素重复的问题，也和15题类似，如果nums[left] == nums[left+1]，那么left可以直接后移，直到一个不重复的元素；对于right也是一样。 代码 12345678910111213141516171819202122232425262728293031323334353637383940public int threeSumClosest(int[] nums, int target) &#123; if(nums == null || nums.length &lt; 3) return 0; Arrays.sort(nums); int closed = nums[0] + nums[1] + nums[2]; for(int i = 0; i &lt; nums.length - 2; i++)&#123; int left = i + 1, right = nums.length - 1; while(left &lt; right)&#123; int min = nums[i] + nums[left] + nums[left + 1]; if(min &gt; target)&#123; if(Math.abs(min - target) &lt; Math.abs(closed - target))&#123; closed = min; &#125; break; &#125; int max = nums[right] + nums[right - 1] + nums[right - 2]; if(max &lt; target)&#123; if(Math.abs(max - target) &lt; Math.abs(closed - target))&#123; closed = max; &#125; break; &#125; int sum = nums[i] + nums[left] + nums[right]; if(Math.abs(sum - target) &lt; Math.abs(closed - target))&#123; closed = sum; &#125; if(sum &lt; target)&#123; while(left &lt; right &amp;&amp; nums[left] == nums[left+1])left++; left++; &#125; else if(sum &gt; target)&#123; while(left &lt; right &amp;&amp; nums[right] == nums[right -1])right--; right--; &#125; else return target; &#125; &#125; return closed;&#125; 复杂度分析： 时间复杂度：$O(n^2)$空间复杂度：$O(1)$"},{"title":"【LeetCode】15. 三数之和","date":"2019-10-15T00:56:53.000Z","path":"2019/10/15/leetcode-15/","text":"15.三数之和给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 注意：答案中不可以包含重复的三元组。 例如, 给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为：[ [-1, 0, 1], [-1, -1, 2]] 题解分析解法1 - 延伸的两数之和(Map+Set大法)三元组不可以重复，那我们可以先不考虑是不是重复，而是找到以后都扔到Set里边去。找的时候可以先取第一个数a，然后取第二个数b，计算需要的第三个数的值c，然后在数组里找有没有这个值。可以用Map来存放被访问过的值，这样就不用重复地去查询数组。 代码 1234567891011121314151617181920212223242526public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums)&#123; int count = 0;//计数，与题目无关 if(nums == null || nums.length &lt; 3) return new ArrayList&lt;&gt;(); Set&lt;List&lt;Integer&gt;&gt; set = new HashSet&lt;&gt;(); Arrays.sort(nums);//排序 for(int i = 0; i &lt; nums.length; i++)&#123; int first = nums[i]; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for(int j = i + 1; j &lt; nums.length; j++)&#123; count++; int needed = 0 - first - nums[j];//找到俩，需要找第三个 if(map.containsKey(needed))&#123;//找到第三个 List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(first); list.add(nums[j]); list.add(needed); set.add(list); &#125; if(!map.containsKey(nums[j]))//把第二个加入备选 map.put(nums[j],j); &#125; &#125; List&lt;List&lt;Integer&gt;&gt; answer = new ArrayList&lt;&gt;(set); System.out.println(\"count :\" + count); return answer;&#125; 执行情况：123456输入：&#123;-1,0,1,2,-1,-4&#125;输出：count :15-1 2 -1 -1 1 0 复杂度分析： 时间复杂度：内层循环执行$\\frac {n(n-1)} {2}$次，复杂度为$O(n^2)$空间复杂度：使用了Set和Map。 能否优化呢？我们可以看到在这里循环进行的次数是15次。也就是说每取一个first元素，都要遍历他后面的所有元素。 解法2 - 对解法1的优化 1.实际上我们通过升序排序后，就能发现，如果我们前3个数字之和就大于0的话，那再往后取也没意义了，也不会再有解了，直接结束循环。 2.如果我们取当前最小的first，和最大以及次大的元素，他们的和如果小于0的话，说明此时的first也不合适，太小了，直接跳过它去找下一个比他大一点的first。 3.一开始我们在用Set去重，但我们发现排序之后的数组为{-4，-1&#39;，-1&#39;&#39;，0，1，2}，当我们-1&#39;做first的时候，找到的3元组为{-1&#39;，0，1&#39;&#39;}，{-1&#39;，-1&#39;&#39;，2}，下一个first取的-1&#39;&#39;时，找到的3元组为{-1&#39;&#39;，0，1}，也就是说，当相邻的两个元素相同时，我们先取的那个元素（-1&#39;）得到的解，与后一个元素能得到的解相同，于是我们在遇到-1时，判断它上一个元素是否和他相同，相同就跳过-1，这样我们不会得到重复的解。 代码 12345678910111213141516171819202122232425262728293031323334 public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums)&#123; int count = 0; if(nums == null || nums.length &lt; 3) return new ArrayList&lt;&gt;();// Set&lt;List&lt;Integer&gt;&gt; set = new HashSet&lt;&gt;();//优化后不再需要Set了 List&lt;List&lt;Integer&gt;&gt; three = new ArrayList&lt;&gt;(); Arrays.sort(nums);//排序 for(int i = 0; i &lt; nums.length - 2; i++)&#123; int first = nums[i]; //优化1：最小的3个相加&gt;0 就不用考虑后面的了，没有解了。 if(first + nums[i + 1]+ nums[i + 2] &gt; 0) break; //最小的加上俩最大的，都小于0，这个小的不考虑了。 if(first + nums[nums.length - 1] + nums[nums.length - 2] &lt; 0)continue; //这个数字与前一个重复，它在上一轮被考虑过了，这次就不考虑它。 if(i &gt; 0 &amp;&amp; first == nums[i - 1])continue; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for(int j = i + 1; j &lt; nums.length; j++)&#123; count++; int needed = 0 - first - nums[j];//找到俩，需要找第三个 if(map.containsKey(needed))&#123;//找到第三个 List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(first); list.add(nums[j]); list.add(needed); three.add(list); &#125; if(!map.containsKey(nums[j]))//把第二个加入备选 map.put(nums[j],j); &#125; &#125; System.out.println(\"count :\" + count); return three; &#125; 执行情况：1234输出：count :4-1 1 0 -1 2 -1 可能你发现了，上面这段代码其实还有点问题，就是当我们遇到{0，0，0，0}这样的输入时，还是会得到两组{0，0，0}，也就是说，当我们选第一个0和第二个0，我们可以和第3个0得到一个三元组，然后我们会去查看第4个0，发现它依旧能和第一个0以及第二个0进行组合，所以说，还是会得到重复的元组，所以上面的第三部优化没有真正做到去重（可以用Set大法：），对于出现多个重复的元素，我们应该怎么去重呢？ 解法3 - 双指针我们可以用双端指针来做这件事，对于{0，0&#39;，0&#39;&#39;，0&#39;&#39;&#39;，0&#39;&#39;&#39;&#39;，0&#39;&#39;&#39;&#39;&#39;}这样的情况，首先我们0做first，然后在后面的0里变找剩下两个元素，设置两个指针low指向0&#39;，high指向0&#39;&#39;&#39;&#39;，我们比较他们3个的和，如果=high`或者遇到不相同的元素，结束本次操作。 代码 123456789101112131415161718192021222324252627282930public List&lt;List&lt;Integer&gt;&gt; threeSum2(int[] nums)&#123; int count = 0; if(nums == null || nums.length &lt; 3) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; three = new ArrayList&lt;&gt;(); Arrays.sort(nums);//排序 for(int i = 0; i &lt; nums.length - 2; i++)&#123; //优化1：最小的3个相加&gt;0 就不用考虑后面的了，没有解了。 if(nums[i] + nums[i + 1]+ nums[i + 2] &gt; 0) break; //最小的加上俩最大的，都小于0，这个小的不考虑了。 if(nums[i] + nums[nums.length - 1] + nums[nums.length - 2] &lt; 0)continue; //这个数字与前一个重复，它在上一轮被考虑过了，这次就不考虑它。 if(i &gt; 0 &amp;&amp; nums[i] == nums[i - 1])continue; int low = i + 1, high = nums.length - 1; while(low &lt; high)&#123; count++; int sum = nums[i] + nums[low] + nums[high]; if(sum == 0)&#123; three.add(Arrays.asList(nums[i],nums[low],nums[high])); while(low &lt; high &amp;&amp; nums[low] == nums[low + 1]) low ++; while(low &lt; high &amp;&amp; nums[high] == nums[high - 1]) high --; low ++; high --; &#125;else if(sum &lt; 0) low ++; else high --; &#125; &#125; System.out.println(\"count:\" + count); return three; &#125; 执行结果：12345输入：&#123;0,0,0,0,1,1,2,2&#125;输出：count:50 0 0 复杂度分析： 时间复杂度：$O(n^2)$空间复杂度：$O(1)$"},{"title":"【DL】学习笔记(持续更新)","date":"2019-10-14T13:49:07.000Z","path":"2019/10/14/DL-note-1/","text":"放上几个学习的链接：https://www.jianshu.com/p/fe114409daafhttps://www.jianshu.com/p/c0215d26d20a莫烦的超棒学习资源 Logistic回归总结： Logistic Regression模型：$y’ = \\sigma(W^tx+b)$，记住使用的激活函数是sigmoid函数。 损失函数：$L(y’,y) = -[y·\\log(y’)+(1-y)·\\log(1-y’)]$衡量预测值y’与真实值y的差距，越小越好。 代价函数：损失均值，$J(W,b) = \\frac {1} {m}·\\sum_{i=1}^{m} {L(y’(i),y(i)})$，是$W$和$b$的函数，学习的过程就是寻找$W$和$b$使得$J(W,b)$最小化的过程。求最小值的方法是用梯度下降法。 模型训练步骤1)初始化W和b2)指定学习率和迭代次数3)每次迭代，根据当前W和b计算对应的梯度（J对W，b的偏导数），然后更新W和b4)迭代结束，学得W和b，带入模型进行预测，分别测试在训练集和测试集上的准确率，从而评价模型。 参考：https://www.jianshu.com/p/4cf34bf158a1 神经网络详解什么是神经网络我们这里讲解的神经网络，就是在Logistic regression的基础上增加了一个或几个隐层（hidden layer），下面展示的是一个最最最简单的神经网络，只有两层： 这里，我们先规定一下记号（Notation）： z是x和w、b线性运算的结果，z=wx+b； a是z的激活值； 下标的1,2,3,4代表该层的第i个神经元（unit）； 上标的[1],[2]等代表当前是第几层。 y^代表模型的输出，y才是真实值，也就是标签 另外，有一点经常搞混： 上图中的x1，x2，x3，x4不是代表4个样本！而是一个样本的四个特征（4个维度的值）！你如果有m个样本，代表要把上图的过程重复m次： 神经网络的“两个传播”： 前向传播（Forward Propagation） 前向传播就是从input，经过一层层的layer，不断计算每一层的z和a，最后得到输出y^ 的过程，计算出了y^，就可以根据它和真实值y的差别来计算损失（loss）。 反向传播（Backward Propagation） 反向传播就是根据损失函数L(y^,y)来反方向地计算每一层的z、a、w、b的偏导数（梯度），从而更新参数。 每经过一次前向传播和反向传播之后，参数就更新一次，然后用新的参数再次循环上面的过程。这就是神经网络训练的整个过程。 前向传播如果用for循环一个样本一个样本的计算，显然太慢，看过我的前几个笔记的朋友应该知道，我们是使用Vectorization，把m个样本压缩成一个向量X来计算，同样的把z、a都进行向量化处理得到Z、A，这样就可以对m的样本同时进行表示和计算了。 这样，我们用公式在表示一下我们的两层神经网络的前向传播过程：Layer 1: $Z^[1] = W^[1]·X + b^[1]$$A^[1] = \\sigma(Z^[1])$ Layer 2: $Z^[2] = W^[2]·A^[1] + b^[2]$$A^[2] = σ(Z^[2])$ 而我们知道，$X$其实就是$A^[0]$，所以不难看出:每一层的计算都是一样的： Layer i: $Z^[i] = W^[i]·A^[i-1] + b^[i]$$A^[i] = \\sigma(Z^[i])$（注：σ是sigmoid函数）因此，其实不管我们神经网络有几层，都是将上面过程的重复。 对于损失函数，就跟Logistic regression中的一样，使用“交叉熵（cross-entropy）”，公式就是 二分类问题： $L(\\hat{y},y) = -[y·log(\\hat{y} )+(1-y)·log(1-\\hat{y} )]$ 多分类问题： $L=-\\sum y(j)·\\hat{y}(j)$ 这个是每个样本的loss，我们一般还要计算整个样本集的loss，也称为cost，用J表示，J就是L的平均：$J(W,b) = \\frac{1}{m}·\\sum_{i=1}^{m}{L(\\hat{y}(i),y(i)})$ 上面的求Z、A、L、J的过程就是正向传播。 反向传播反向传播说白了根据根据J的公式对W和b求偏导，也就是求梯度。因为我们需要用梯度下降法来对参数进行更新，而更新就需要梯度。但是，根据求偏导的链式法则我们知道，第l层的参数的梯度，需要通过l+1层的梯度来求得，因此我们求导的过程是“反向”的，这也就是为什么叫“反向传播”。 各种深度学习框架TensorFlow、Keras，它们都是只需要我们自己构建正向传播过程，反向传播的过程是自动完成的。 进行了反向传播之后，我们就可以根据每一层的参数的梯度来更新参数了，更新了之后，重复正向、反向传播的过程，就可以不断训练学习更好的参数了。 深层神经网络（Deep Neural Network）前面的讲解都是拿一个两层的很浅的神经网络为例的。深层神经网络也没什么神秘，就是多了几个/几十个/上百个hidden layers罢了。可以用一个简单的示意图表示： 注意，在深层神经网络中，我们在中间层使用了“ReLU”激活函数，而不是sigmoid函数了，只有在最后的输出层才使用了sigmoid函数，这是因为ReLU函数在求梯度的时候更快，还可以一定程度上防止梯度消失现象，因此在深层的网络中常常采用。关于激活函数的问题，可以参阅：神经网络中的激活函数及其对比 关于深层神经网络，我们有必要再详细的观察一下它的结构，尤其是每一层的各个变量的维度，毕竟我们在搭建模型的时候，维度至关重要。 超快速理解深度学习的概念先理解，再看公式，其实公式就是在表达这个意思的数学语言而已。 优化器 Optimizer - 加速神经网络训练过程 越复杂的网络，越多数据，训练花费的时间也就越多，因为计算量太大了。 SGD 随机梯度下降：每次使用一个样本对参数进行更新。BGD 批量梯度下降：每次使用所有样本进行梯度的更新。MBGD 小批量梯度下降：上两种的折中，每次使用 batch_size个样本对参数进行更新。 具体公式推导请参考：BGD,SGD,MBGD的理解深度学习必备：随机梯度下降（SGD）优化算法及可视化 如果觉得SGD还不够快，怎么办？ 其实还有很多方法。 大多其他方法是在更新神经网络参数的那一步上动动手脚。 传统参数$W$的更新：将原始w累加 - 学习率 * 校正值$W += - Leraning rate \\times \\delta x$这种方法可能会让学习曲折无比： 将走路的人放在一个下坡上，这样他走路的时候就不自觉地向下走，弯路也会变少，这是Momentum的更新方法： AdaGrad，在学习率上动动手脚，使得每个参数的更新都有与众不同的学习效率，与Momentum不同，给他一个不好走路的鞋子，当他斜着走路的时候就会脚疼，逼着他直走。 结合Momentum和AdaGrad于是有了RMSProp 但是会发现，关于学习率的部分并没有体现到，于是有了Adam方法. 大多数时候使用Adam都能又快又好。 为什么需要激活函数 Activation Functions激活函数就是用来解决不能用线性方程解决的问题。（把线性方程掰弯） 激活函数（非线性方程）套在$Wx$上，就可以达到效果。 激活函数必须是可微分的，因为在反向传播的时候只有可微分的激励函数才能把误差传递回去。 激活函数有： relu、sigmoid、tanh 层数较少时可以任选CNN推荐reluRNN推荐relu或tanh 更详细参考：深度学习中常用的激励函数 为什么要特征标准化 Feature Normalization？多个特征往往有不同的量纲和数量级，当他们之间差别过大时，如果直接用原始数据进行分析，那么数值较高的特征往往会起更大的作用，削弱数值较低特征的作用，因此需要对特征进行标准化处理。 常用的标准化方法有两种： 1.minmax normalization：将特征缩放到[0,1]区间 2.std normalization：将数据缩放到（均值mean = 0，标准差std = 1）的区间 更多参考：数据特征 标准化和归一化你了解多少？三种常用数据标准化方法 为什么要 批标准化 Batch Normalization?神经网络的输入层会发生什么问题？ 当数据取值范围差距过大时，如下图，经过激活函数后$tanh(Wx2)\\approx 0.96 $，很接近1了，因此神经网络在初期就对那些比较大的x的特征范围不敏感了。 这个问题可以通过上面的标准化来解决，但这个不敏感问题不仅仅发生在神经网络的输入层，在隐藏层中也常常会发生。 这就是Batch Normalization来处理的。 把数据分成小批小批的进行SGD，在每批数据进行前向传递的时候，对每一层都进行Normalization 的处理。 Batch Normalization也可以被看作一个层面。它被添加在全连接层和激励函数之间。使激活前的数据分布在有效的范围内。 Batch Normalization还进行了反向Normalize。将标准化后的数据进行扩展和平移。是为了让神经网络自己学会使用和修改扩展参数$\\gamma$ 和平移参数$\\beta$,这样神经网络就能慢慢琢磨出来前面的Normalization操作是否起到优化作用，如果没起到优化作用，就使用$\\gamma$和$\\beta$来抵消一些Normalization的操作 Batch Normalization的总结： 让每一层的值在有效的范围内传递下去。 什么是卷积神经网络 Convolutional Neural Network基本概念参考理解CNN卷积层与池化层计算一片超棒的CNN学习资料 比较流行的搭建结构（在图片分类）是： Classifier 分类器进行分类预测Fully Connected 全连接层Fully Connected 全连接层Max Pooling 池化Convolution 卷积层Max Pooling 池化Convolution 卷积层IMAGE 输入的图片 —预留例子的补充— 什么是循环神经网络 Recurrent Neural NetworkRNN是一种特殊的神经网络结构, 它是根据”人的认知是基于过往的经验和记忆”这一观点提出的. 它与DNN,CNN不同的是: 它不仅考虑前一时刻的输入,而且赋予了网络对前面的内容的一种‘记忆’功能. RNN之所以称为循环神经网络，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。 应用领域 自然语言处理(NLP): 主要有视频处理, 文本生成, 语言模型, 图像处理 机器翻译, 机器写小说 语音识别 图像描述生成 文本相似度计算 音乐推荐、网易考拉商品推荐、Youtube视频推荐等新的应用领域. 更具体的模型内容参考：深度学习之RNN(循环神经网络)这篇很ok，一文看懂循环神经网络-RNN（独特价值+优化算法+实际应用）如何深度理解RNN？——看图就好！ 如何让NN分析数据间的关联呢？分析Data 0的时候把结果存入记忆，分析Data 1的时候会产生新记忆，但新记忆和老记忆没有关系，因此就把老记忆调用过来一起分析。 RNN的结构很自由 用于分类问题：例如语句的情感分析，可以用只有最后一个时间点输出判断结果。 用于图片描述：只需要一个输入，输出对图片描述的一段话 用于翻译：给出英文翻译成中文 什么是 LSTM RNN？LSTM：Long Short-Term Memory 长短期记忆，是当下流行的RNN形式之一。 RNN会有梯度消失(gradient vanishing)和梯度爆炸(gradient exploding)的问题。关于梯度消失，更多参考： RNN梯度消失和爆炸的原因深度学习中RNN梯度消失LSTM如何解决梯度消失问题 LSTM是为了解决这个问题产生的，它比RNN多了3个控制器。输入,输出,忘记 LSTM的超棒教程：BiLSTM介绍及代码实现 什么是自编码 Autoencoder ？自编码器是一种能够通过无监督学习，学到输入数据高效表示的人工神经网络。输入数据的这一高效表示称为编码（codings），其维度一般远小于输入数据，使得自编码器可用于降维。更重要的是，自编码器可作为强大的特征检测器（feature detectors），应用于深度神经网络的预训练。此外，自编码器还可以随机生成与训练数据类似的数据，这被称作生成模型（generative model）。比如，可以用人脸图片训练一个自编码器，它可以生成新的图片。 更多参考：第十五章——自编码器（Autoencoders） 什么是超参数 参数(parameters)/模型参数 由模型通过学习得到的变量，比如权重和偏置 超参数(hyperparameters)/算法参数 根据经验进行设定，影响到权重和偏置的大小，比如迭代次数、隐藏层的层数、每层神经元的个数、学习速率等. 什么是消融实验（Ablation experiment)该想法源自于神经科学领域的研究，该领域的主要目标是理解我们的大脑是如何工作的。 许多关于大脑功能的见解看法都是通过消融研究获得的，本质上来说，消融即选择性地切除或破坏大脑特定区域的组织，以可控的方式进行消融，检测大脑该部分对诸如言语生成、运动等日常工作的影响。 在人工神经网络上应用消融的方法十分简单的，首先，我们训练神经网络来完成特定的任务，比如说识别手写数字。第二步，我们切除网络的某一部分，然后评估由这种破坏导致的性能变化。第三步，我们确定网络性能的改变和被破坏的位置之间是否有联系。通过这种方法，我们发现网络的某些特定能力，比如控制机器人执行前进动作，是通过局部网络控制的。 更多：对人工神经网络“开刀”，利用神经科学消融法检测人工神经网络 什么是encoder-decoder模型框架？准确的说，Encoder-Decoder并不是一个具体的模型，而是一类框架。Encoder和Decoder部分可以是任意的文字，语音，图像，视频数据，模型可以采用CNN，RNN，BiRNN、LSTM、GRU等等。所以基于Encoder-Decoder，我们可以设计出各种各样的应用算法。 什么是Seq2Seq？更多来自：轰炸理解深度学习里面的encoder-decoder模型 所谓的Sequence2Sequence任务主要是泛指一些Sequence到Sequence的映射问题，Sequence在这里可以理解为一个字符串序列，当我们在给定一个字符串序列后，希望得到与之对应的另一个字符串序列（如 翻译后的、如语义上对应的）时，这个任务就可以称为Sequence2Sequence了。 在现在的深度学习领域当中，通常的做法是将输入的源Sequence编码到一个中间的context当中，这个context是一个特定长度的编码（可以理解为一个向量），然后再通过这个context还原成一个输出的目标Sequence。 如果用人的思维来看，就是我们先看到源Sequence，将其读一遍，然后在我们大脑当中就记住了这个源Sequence，并且存在大脑的某一个位置上，形成我们自己的记忆（对应Context），然后我们再经过思考，将这个大脑里的东西转变成输出，然后写下来。 那么我们大脑读入的过程叫做Encoder，即将输入的东西变成我们自己的记忆，放在大脑当中，而这个记忆可以叫做Context，然后我们再根据这个Context，转化成答案写下来，这个写的过程叫做Decoder。其实就是编码-存储-解码的过程。 而对应的，大脑怎么读入（Encoder怎么工作）有一个特定的方式，怎么记忆（Context）有一种特定的形式，怎么转变成答案（Decoder怎么工作）又有一种特定的工作方式。 好了，现在我们大体了解了一个工作的流程Encoder-Decoder后，我们来介绍一个深度学习当中，最经典的Encoder-Decoder实现方式，即用RNN来实现。 基本的Encoder-Decoder模型非常经典，但是也有局限性。最大的局限性就在于编码和解码之间的唯一联系就是一个固定长度的语义向量c。也就是说，编码器要将整个序列的信息压缩进一个固定长度的向量中去。但是这样做有两个弊端，一是语义向量无法完全表示整个序列的信息，还有就是先输入的内容携带的信息会被后输入的信息稀释掉，或者说，被覆盖了。输入序列越长，这个现象就越严重。这就使得在解码的时候一开始就没有获得输入序列足够的信息， 那么解码的准确度自然也就要打个折扣了. 为解决这个问题，提出了Attention模型。 什么是Attention模型？简单的说，这种模型在产生输出的时候，还会产生一个“注意力范围”表示接下来输出的时候要重点关注输入序列中的哪些部分，然后根据关注的区域来产生下一个输出，如此往复。模型的大概示意图如下所示: 相比于之前的encoder-decoder模型，attention模型最大的区别就在于它不在要求编码器将所有输入信息都编码进一个固定长度的向量之中。相反，此时编码器需要将输入编码成一个向量的序列，而在解码的时候，每一步都会选择性的从向量序列中挑选一个子集进行进一步处理。这样，在产生每一个输出的时候，都能够做到充分利用输入序列携带的信息。而且这种方法在翻译任务中取得了非常不错的成果。 更多参考：Encoder-Decoder模型和Attention模型台大李宏毅深度学习——seq2seqAttention注意力机制介绍 什么是dropout想要提高CNN的表达或分类能力，最直接的方法就是采用更深的网络和更多的神经元，即deeper and wider。但是，复杂的网络也意味着更加容易过拟合。于是就有了Dropout，大部分实验表明其具有一定的防止过拟合的能力。 最早的Dropout可以看Hinton的这篇文章《Improving neural networks by preventing co-adaptation of feature Detectors》 需要注意的是：论文中Dropout被使用在全连接层之后，而目前的caffe框架中，其可以使用在各种层之后。 如上图左，为没有Dropout的普通2层全连接结构，记为 r=a(Wv)，其中a为激活函数。 如上图右，为在第2层全连接后添加Dropout层的示意图。即在 模 型 训 练 时 随机让网络的某些节点不工作（输出置0），其它过程不变。 对于Dropout这样的操作为何可以防止训练过拟合，原作者也没有给出数学证明，只是有一些直观的理解或者说猜想。下面说几个我认为比较靠谱的解释： (1) 由于随机的让一些节点不工作了，因此可以避免某些特征只在固定组合下才生效，有意识地让网络去学习一些普遍的共性（而不是某些训练样本的一些特性） (2) Bagging方法通过对训练数据有放回的采样来训练多个模型。而Dropout的随机意味着每次训练时只训练了一部分，而且其中大部分参数还是共享的，因此和Bagging有点相似。因此，Dropout可以看做训练了多个模型，实际使用时采用了模型平均作为输出（具体可以看一下论文，论文讲的不是很明了，我理解的也够呛） 训练的时候，我们通常设定一个dropout ratio = p,即每一个输出节点以概率 p 置0(不工作)。假设每一个输出都是相互独立的，每个输出都服从二项伯努利分布B(1-p)，则大约认为训练时 只使用了 (1-p)比例的输出。 测试的时候，最直接的方法就是保留Dropout层的同时，将一张图片重复测试M次，取M次结果的平均作为最终结果。假如有N个节点，则可能的情况为R=2^N,如果M远小于R，则显然平均效果不好；如果M≈N，那么计算量就太大了。因此作者做了一个近似：可以直接去掉Dropout层，将所有输出 都使用 起来，为此需要将尺度对齐，即比例缩小输出 r=r*(1-p)。即如下公式：这里写图片描述特别的， 为了使用方便，我们不在测试时再缩小输出，而在训练时直接将输出放大1/(1-p)倍。 结论： Dropout得到了广泛的使用，但具体用到哪里、训练一开始就用还是后面才用、dropout_ratio取多大，还要自己多多尝试。有时添加Dropout反而会降低性能。 更多参考：理解droupout 什么是 beam search?集束搜索(beam search)： 集束搜索可以认为是维特比算法的贪心形式，在维特比所有中由于利用动态规划导致当字典较大时效率低，而集束搜索使用beam size参数来限制在每一步保留下来的可能性词的数量。集束搜索是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。 假设字典为[a,b,c]，beam size选择2，则如下图有： 1：在生成第1个词的时候，选择概率最大的2个词，那么当前序列就是a或b 2：生成第2个词的时候，我们将当前序列a或b，分别与字典中的所有词进行组合，得到新的6个序列aa ab ac ba bb bc,然后从其中选择2个概率最高的，作为当前序列，即ab或bb 3：不断重复这个过程，直到遇到结束符为止。最终输出2个概率最高的序列。 显然集束搜索属于贪心算法，不能保证一定能够找到全局最优解，因为考虑到搜索空间太大，而采用一个相对的较优解。而维特比算法在字典大小较小时能够快速找到全局最优解。 而贪心搜索由于每次考虑当下词的概率，而通常英文中有些常用结构，如“is going”，出现概率较大，会导致模型最终生成的句子过于冗余。如“is visiting”和“is going to be visiting”。贪心搜索可以认为beam size为1时的集束搜索特例。 更多来自：NLP 自然语言处理 集束搜索beam search和贪心搜索greedy search 什么是end2end传统的图像识别问题往往通过分治法将其分分解为预处理、特征提取和选择、分类器设计等若干步骤。分治法的动机是将图像识别的母问题分解为简单、可控且清晰的若干小的子问题。不过分步解决子问题时，尽管可以在子问题上得到最优解，但子问题上的最优解并不意味着就能得到全局问题的最后解。 深度学习提供了一种“端到端”的学习范式，整个学习的流程并不进行人为的子问题划分，而是完全交给深度学习模型直接学习从原始数据到期望输出的映射。 如图所示，对深度模型而言，其输入数据是未经任何人为加工的原始样本形式，后续则是堆叠在输入层上的众多操作层。这些操作层整体可以看作一个复杂的函数Fcnn，最终的损失函数由数据损失（data loss）和模型参数的正则化损失（regularization loss）共同组成，模型深度的训练则是在最终损失驱动下对模型进行参数更新并将误差反向传播至网络各层。模型的训练可以简单抽象为从原始数据向最终目标的直接拟合，而中间的这些部件起到了将原始数据映射为特征随后在映射为样本标记的作用。 总结一下：端到端的学习其实就是不做其他额外处理，从原始数据输入到任务结果输出，整个训练和预测过程，都是在模型里完成的。 end2end的好处： ​通过缩减人工预处理和后续处理，尽可能使模型从原始输入到最终输出，给模型更多的可以根据数据自动调节的空间，增加模型的整体契合度。end2end强调的是全局最优，中间部分局部最优并不能代表整体最优 什么是Scheduled Sampling基础模型只会使用真实lable数据作为输入， 现在，train-decoder不再一直都是真实的lable数据作为下一个时刻的输入。train-decoder时以一个概率P选择模型自身的输出作为下一个预测的输入,以1-p选择真实标记作为下一个预测的输入。Secheduled sampling(计划采样)，即采样率P在训练的过程中是变化的。一开始训练不充分，先让P小一些，尽量使用真实的label作为输入，随着训练的进行，将P增大，多采用自身的输出作为下一个预测的输入。随着训练的进行，P越来越大大，train-decoder模型最终变来和inference-decoder预测模型一样，消除了train-decoder与inference-decoder之间的差异 总之：通过这个scheduled-samping方案，抹平了训练decoder和预测decoder之间的差异！让预测结果和训练时的结果一样。 更多参考：seq2seq聊天模型（二）——Scheduled Sampling 什么是反向传播算法（BP）？具体参考：读懂反向传播算法（bp算法）"},{"title":"【LeetCode】11. 盛最多水的容器","date":"2019-10-14T03:01:45.000Z","path":"2019/10/14/leetcode-11/","text":"11. 盛最多水的容器给定 n 个非负整数 $a_1，a_2，…，a_n$，每个数代表坐标中的一个点 (i, $a_i$) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, $a_i$) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 说明：你不能倾斜容器，且 n 的值至少为 2。 图片来自：LeetCode图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例: 输入: [1,8,6,2,5,4,8,3,7]输出: 49 题解分析解法 - 双指针考虑在什么条件下能乘最多的水。 底 * 高最大的情况下能装最多水，并且高取决于区间两边较短的那个元素。 因此可以设置头尾两个指针，计算此时容量，并根据需要移动指针，具体的：1.维护一个最大的area。2.如果左端比右端矮，那么左端后移；否则，右端前移。3.直到左右相遇时结束。 代码 12345678910111213public int maxArea(int[] height) &#123; int area = 0; int left = 0, right = height.length - 1; while(left &lt; right)&#123; area = Math.max(area, (right - left) * Math.min(height[left],height[right])); if(height[left] &lt; height[right])&#123; left++; &#125;else&#123; right--; &#125; &#125; return area;&#125; 复杂度分析 时间复杂度：$O(n)$空间复杂度：$O(1)$"},{"title":"【LeetCode】3.无重复字符的最长子串","date":"2019-10-14T01:41:19.000Z","path":"2019/10/14/leetcode-3/","text":"3.无重复字符的最长子串给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 示例 1: 输入: “abcabcbb”输出: 3解释: 因为无重复字符的最长子串是 “abc”，所以其长度为 3。 示例 2: 输入: “bbbbb”输出: 1解释: 因为无重复字符的最长子串是 “b”，所以其长度为 1。 示例 3: 输入: “pwwkew”输出: 3解释: 因为无重复字符的最长子串是 “wke”，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，”pwke” 是一个子序列，不是子串。 题解分析解法1 - 暴力法我们可以求出所有子串并且检查是否包含重复元素，如果包含就丢弃，不包含就以此更新最长子串长度，最终会得到结果。 代码：不写了，不是个好方法。 复杂度分析： 时间复杂度：$O(n^3)$ 求所有子串的复杂度为$O(n^2),对于每个子串，检查是否包含重复元素的复杂度最坏为$O(n)$，即子串为原字符串。因此复杂度为$O(n^3)$空间复杂度：$O(min(n, m))$，我们需要 $O(k)$ 的空间来检查子字符串中是否有重复字符，其中 k 表示 Set 的大小。而 Set 的大小取决于字符串 n 的大小以及字符集/字母 m 的大小。 解法2 - 滑动窗口什么是滑动窗口？ 画个图先直观的形成一个印象： 有了印象之后，滑动窗口经常用来解决子串的问题，从图上也能看出来，就是设置一个区间，然后在这个字符串or数组上滑动。 有了是什么之后，那我们就看看为什么。 为什么要用滑动窗口? 假如我们要求这个字符串里不重复的最长子串长度的话，很自然的想法是里外循环： 我们从头开始找，找到end=3的时候我们发现有重复了，说明从start = 0 开始的最长不重复子串长度是3，然后我们就从start = 1开始再找，当end = 4 时我们发现重复，于是我们继续将start往后推，这样的话是能解决问题的，不过时间复杂度是$O(n^2)$。 如果用滑动窗口的话，我们是不是需要判断窗口什么时候移，怎么移？ 大概思路就是这样，那怎么直到start每次移动多少呢？ 用一个Map&lt;字符，下次的位置&gt;来记录，比如说一开始end = 0的时候我们遍历到字符’s’，那么我们将它作为key加入map，value则是它的下一个位置，表示说当你遇到‘s’字符时，你滑动到‘s’的下一个位置开始将不会重复.. 代码 12345678910111213public int lengthOfLongestSubstring(String s) &#123; if(s == null || s.length() == 0) return 0; int result = 0; Map&lt;Character, Integer&gt; map = new HashMap(); for(int start = 0, end = 0; end &lt; s.length(); end++)&#123; if(map.containsKey(s.charAt(end)))&#123; start = Math.max(map.get(s.charAt(end)),start); &#125; map.put(s.charAt(end),end + 1); result = Math.max(result, end - start + 1); &#125; return result;&#125; 复杂度分析 时间复杂度：$O(n)$空间复杂度：$O(min(m,n))$"},{"title":"【LeetCode】80.删除排序数组中的重复项2","date":"2019-10-12T07:18:15.000Z","path":"2019/10/12/leetcode-80/","text":"80.删除排序数组中的重复项2给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素最多出现两次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 示例 1: 给定 nums = [1,1,1,2,2,3], 函数应返回新长度 length = 5, 并且原数组的前五个元素被修改为 1, 1, 2, 2, 3 。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,0,1,1,1,1,2,3,3], 函数应返回新长度 length = 7, 并且原数组的前五个元素被修改为 0, 0, 1, 1, 2, 3, 3 。 你不需要考虑数组中超出新长度后面的元素。 题解分析解法1 - 双指针这道题是26题的变形。 题目需要原地删除，因此需要用双指针来做。 与26题不同的点在于，数组中的元素最多出现2次，如何达到这个要求呢？ 26题的方法是，用p指针和q指针的元素进行比较，不相同就把q指针的元素复制到p+1的位置，相同则q指针后移，此时考虑的是数组中没有重复元素，即元素最多出现1次。 出现2次的情况就需要和用q指针和p指针以及p指针前一个位置的元素进行比较，p和p-1位置的元素要么相同，要么不同。当p和p-1位置的元素相同时，若q与p-1位置的元素相同，说明q位置是p位置元素重复的第三个元素，那么它不能加入新数组；如果p位置和p-1位置不同，说明p位置也只有某个元素的1个副本，q位置的元素此时可以复制到p+1的位置。 所以我们只需要比较q位置的元素是否等于p-1位置的元素，来决定是否要复制元素。 数组的前两个元素不需要做处理，不管这两个元素相同或不同都不需要去修改这两个元素。因此我们初始令p = 1，q = 2。 代码： 12345678910111213public int removeDuplicates(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; int p = 1; int q = 2; while(q &lt; nums.length)&#123; if(nums[q] != nums[p - 1] )&#123; nums[p + 1] = nums[q]; p++; &#125; q++; &#125; return p + 1;&#125; 复杂度分析 假设，数组长度为n 时间复杂度:O(n)空间复杂度:O(1) 思考 - 当要求数组中元素最多重复k次，怎么做？可以根据上面的方法，来思考一番。 当最多重复2次的时候，我们将q与p-1比较，当最多重复k次的时候，我们是不是可以将q与p-k+1比较？ 画个图理解一下： 通过图解，我们可以把滑动窗口的思想融入进来，维护一个k大小的窗口，q指向的元素每次和窗口最左端的元素比较，如果不相等，那么窗口可以向前滑动；如果相等，则q前移，窗口不动。 代码：1234567891011121314public int removeDuplicates(int[] nums, int k) &#123; if(nums == null ) return 0; if(nums.length &lt; k) return k; int p = k - 1; int q = k; while(q &lt; nums.length)&#123; if(nums[q] != nums[p - k + 1] )&#123; nums[p + 1] = nums[q]; p++; &#125; q++; &#125; return p + 1;&#125; 复杂度分析 假设，数组长度为n 时间复杂度:O(n)空间复杂度:O(1)"},{"title":"【LeetCode】26.删除排序数组中的重复项","date":"2019-10-12T01:48:13.000Z","path":"2019/10/12/leetcode-26/","text":"26.删除排序数组中的重复项给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 示例 1: 给定数组 nums = [1,1,2], 函数应该返回新的长度 2, 并且原数组 nums 的前两个元素被修改为 1, 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,0,1,1,1,2,2,3,3,4], 函数应该返回新的长度 5, 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4。 你不需要考虑数组中超出新长度后面的元素。 题解分析解法1 - 双指针复制法首先注意数组是有序的，那么重复的元素一定会相邻。 要求删除重复元素，实际上就是将不重复的元素移到数组的左侧。 考虑用2个指针，一个在前记作p，一个在后记作q，算法流程如下： 1.比较p和q位置的元素是否相等。 如果相等，q后移1位 如果不相等，将q位置的元素复制到p+1位置上，p后移一位,q后移1位 重复上述过程，直到q 等于数组长度。 返回 p + 1，即为新数组长度。 图示： 图示看完应该就很明白了。 代码12345678910111213 public int removeDuplicates(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; int p = 0; int q = 1; while(q &lt; nums.length)&#123; if(nums[p] != nums[q])&#123; nums[p + 1] = nums[q]; p++; &#125; q++; &#125; return p + 1;&#125; 复杂度分析 时间复杂度:O(n)空间复杂度:O(1) 解法1 - 双指针复制法的一丢丢优化上一个方法已经解决了问题，但我们有没有优化的空间呢？ 实际上还有一点点优化空间。 考虑如下数组： 此时数组中没有重复元素，按照上面的方法，每次比较时nums[p]都不等于nums[q]，因此就会将q指向的元素原地复制一遍，这个操作其实是不必要的。 因此我们可以添加一个小判断，当q - p &gt; 1时，才进行复制。 代码： 123456789101112131415public int removeDuplicates(int[] nums) &#123; if(nums == null || nums.length == 0) return 0; int p = 0; int q = 1; while(q &lt; nums.length)&#123; if(nums[p] != nums[q])&#123; if(q - p &gt; 1)&#123; nums[p + 1] = nums[q]; &#125; p++; &#125; q++; &#125; return p + 1;&#125; 复杂度分析： 时间复杂度：O(n)空间复杂度：O(1)"},{"title":"【论文笔记】Relation Classification via Convolutional Deep Neural Network","date":"2019-10-11T14:24:24.000Z","path":"2019/10/11/Zeng-2014-note/","text":"CNN：https://blog.csdn.net/kane7csdn/article/details/83617086https://blog.csdn.net/liangchunjiang/article/details/79030681论文模型调试：https://blog.csdn.net/u014586129/article/details/90142875https://www.jianshu.com/p/28fb2aed876f https://blog.csdn.net/manmanxiaowugun/article/details/81157105https://www.sohu.com/a/165856071_465975https://blog.csdn.net/u013414502/article/details/82110202#1%E5%A6%82%E4%BD%95%E6%8F%92%E5%85%A5%E5%85%AC%E5%BC%8Fhttps://www.cnblogs.com/Luv-GEM/p/11598294.html 高效论文阅读顺序： 1.题目，关键词，摘要2.结论3.图表4.引言5.核心部分：研究结果和讨论6.实验 偶然看到的做笔记大法 主旨： Abstract 作者想解决什么问题？question 作者通过什么理论/模型来解决这个问题？method 作者给出的答案是什么？answer Introduction 作者为什么研究这个课题？ 目前这个课题的研究进行到了哪一阶段？ 作者使用理论基于哪些假设？（现在也许有点明白了，不明白再去查） Conclusion 这篇文章存在哪些缺陷？ 作者关于这个课题的构思有哪几点？ 研究方法： 数据来源+重要指标+模型步骤（看不懂理论推导没关系，那玩意儿一般很少人能看懂）+每个步骤得出的结论。 我们可以将【研究方法】这一部分进一步整理成： 研究的数据从哪里来？ 研究中用到的重要指标有哪些？ 模型分哪几步？每一步分别得出了什么结论？ 摘要用于关系分类的最先进的方法主要是基于统计机器学习的，性能很大程度上取决于提取特征的质量。提取的特征通常源自于预先存在的NLP系统的输出，这导致现有NLP工具的错误在特征提取任务中被不断传播且放大。 本文利用卷积神经网络提取词汇和句子级别的特征 将所有单词标记(word tokens)作为输入，无需复杂预处理。 首先，通过查找词嵌入(word embeddings)将单词标记转换成向量。然后，根据给定的名词抽取出词汇级别的特征。同时使用卷积方法学习句子级别的特征。将这两个级别的特征串联形成最终提取的特征向量。最后，将这些特征输入softmax分类器来预测两个标记名词间的关系。实验结果表明该方法明显优于最先进的方法。 注：关于词嵌入，学习Word Embedding 结论利用CNN提取词汇和句子级别的特征用来关系分类。在网络中，成功提出位置特征(position features)来指定名词对的关系。当添加PF后，系统有了显著的改进。自动学习特征可以产生出色的结果并且取代基于NLP工具提取的特征。 引言为了识别名词对之间的关系，巧妙地从不同句法和语义结构中结合词汇和句子级别的线索是十分必要的。 例如，在这个句子中： “The [fire]e1 inside WTC was caused by exploding [fuel]e2” 我们通常利用标记的名词和句子的意思来标识fuel和fire之间的cause-effect关系。 本文中，使用CNN来抽取关系分类中词汇和句子级别的特征。方法将所有单词标记作为输入而无须复杂处理。（例如词性标记和语法分析）。首先，通过查找词嵌入(word embeddings)将单词标记转换成向量。然后，根据给定的名词抽取出词汇级别的特征。同时使用卷积方法学习句子级别的特征。将这两个级别的特征串联形成最终提取的特征向量。最后，将这些特征输入softmax分类器来预测两个标记名词间的关系。实验结果表明该方法明显优于最先进的方法。 Collobert等人先前研究了使用CNN提取NLP特征的想法，所有任务都被认为是顺序标记问题，其中输入的句子的每个单词都被赋予标记。 我们的任务可以被认为是一个多分类问题，产生不同目标函数。此外，关系分类被定义为将关系标签分配给单词对。因此有必要区分我们期望分配关系标签的单词对。为了这个目的，利用position features（PF）来编码目标名词对的相对距离。这是使用CNN进行关系分类的第一个例子 相关工作使用深度学习来学习特征，在NLP中，这些方法主要是基于学习每个单词的分布式表示，也叫做词嵌入(word embeddings). RNN也可用作关系分类，从连接两个名词的句法树路径中学习向量，用来确定他们间的语义关系。 研究方法神经网络体系结构 图1描述了用于关系分类的神经网络体系结构，网络采用句子作为输入，并且发现特征提取的多个层次，较高的层次代表了输入的更抽象的方面。 它主要包括3个组成部分：1.单词表征（Word Representation）2.特征提取3.输出 系统无需复杂的句法或语义预处理，输入是带有两个标记名词的句子。然后通过查找词嵌入将单词标记转换为向量。接下来，分别提取词汇和句子级的特征，然后直接连接形成最终的特征向量。最后，为了计算每个关系的置信度，特征向量被输入softmax分类器。分类器的输出是一个向量，其维数等于预定义关系类型的数量。每个维度的值是对应关系的置信度得分。 置信度？softmax？softmax置信度 单词表征（Word Representation）在单词表征部分中，通过查找词嵌入将每个输入单词标记变换为向量。Collobert等人汇报，从大量未标记数据中学习的词嵌入要比随机初始化词嵌入更令人满意。在关系分类中，我们首先应当使用大量未标注数据集中精力学习有判别能力的词嵌入，它拥有更多句法和语义信息。不信的是，训练词嵌入总是花费很长时间。然而，有很多训练好的词嵌入是可以免费使用的。我们的实验直接使用Turian等人提供的词嵌入。 词汇级特征词汇级特征是决定关系的重要线索。传统的词汇级别特征主要包括名词本身，名词对以及实体间词序列的类型，其质量很大程度上取决于现有NLP工具产生的结果。 取而代之，本文使用词嵌入作为基本特征的来源。选择标记名词的词嵌入及其上下文标记，所有这些特征串联到词汇级特征向量 1。 表1展示了选择的word embeddings，与句中标记名词相关。 句子级特征如上所述，所有标记都被表示成词向量，已被证明与人类对词相似性的判断有很好的相关性。尽管取得了成功，单个词向量模型是严重受限的，因为它们不能捕捉长距离特征和语义合成性（我的思考：LSTM？），这是自然语言的重要品质，它使人能够理解更长表达的含义。 在本节中，我们提出一个max-pooled CNN以提供句子级别的表示并且自动抽取句子级别的特征. 图2展示了句子级别的特征抽取框架。在Window Processing部分，每个标记进一步被表示为词特征(WF)和位置特征(PF)。然后，向量通过卷积部分。最后，我们通过非线性变换得到了句子级别的特征。 Word Features 词特征分布假设理论（Harris，1954）指出，在相同语境中出现的词语往往具有相似的含义。为了捕获这个特征，WF结合了词的向量表示以及其在上下文中的向量表示。假设我们有下列单词序列。 $S:[People]_0 \\; have_1 \\; been_2 \\; moving_3 \\; back_4 \\; into_5 \\; [downtown]_6$ 被标记的名词与标签$y$关联，$y$定义了被标记名词对包含的关系类型。每个单词也与word embeddings的索引相关联。句子$S$中的单词标记被表示为向量列表$(x_0,x_1,\\cdots,x_6)$,其中$x_i$对应于句子中第i个单词的word embedding。 要使用$w$的上下文大小，我们将$w$大小的向量窗口组合成一个更丰富的特征。例如，当$w = 3$,句子$S$中第三个单词&quot;moving&quot;的WF被表达为$[x_2,x_3,x_4]$,相似地，考虑到整个句子，WF可以作如下表示： $ \\{ [x_s,x_0,x_1],[x_0,x_1,x_2],\\cdots,[x_5,x_6,x_e]\\}$ $x_s$和$x_e$是分别对应于句子开头和结尾的特殊word embedding。 Position Features 位置特征关系分类是复杂的任务。传统上，结构特征（例如，名词间的最短依赖路径）用于解决该问题。显然，只通过WF无法捕获这类结构信息。有必要指明哪个输入标记(tokens)是句子的目标名词。为此，建议将PF用于关系分类。本位中，PF是当前单词与$w_1$和$w_2$的相对距离。 例如，句子$S$中&quot;moving&quot;与&quot;people&quot;和&quot;downtown&quot;的相对距离分别为为3和-3。 在我们的方法中，相对距离也被映射到维向量$d_e$(超参数），这个向量是随机初始化的。然后，我们获得当前单词与$w_1$和$w_2$的相对距离相关的距离向量$d_1$和$d_2$，以及$PF = [d_1,d_2]$. 结合WF和PF，单词被表示为$[WF,PF]^T$，随后被输入算法的卷积部分。 卷积我们将看到Word Representaion方法可以通过窗口中的向量组合来捕获上下文信息。但是，它只会在句子中的每个单词周围产生局部特征。在关系分类中，用目标名词标记的用于输入的句子，仅对应于关系类型而不是预测每个单词的标签。因此，可能有必要使用所有局部特征并预测全局关系。 使用神经网络时，卷积方法是用来合并所有特征的自然方法。与Collobert相似，我们首先使用线性变换来处理Window Processing部分的输出。$Z =W_1X \\tag{1}$$X \\in \\Bbb R^{n_0 \\times t}$ 是 Window Processing任务的输出。其中$n_0 = w \\times n$,$n$(超参数）是特征向量的维数，$t$是输入句子的标记数量。$W_1 \\in \\Bbb R^{n_1 \\times n_0}$是线性变换矩阵,$n_1$(超参数)是隐藏层1的大小，我们可以看见，这些特征共享相同的权重，大大减少了要学习的自由参数的数量。应用线性变换后，输出$Z in \\Bbb R^{n_1 \\times t}$ 依赖于$t$。为了确定特征向量每一个维度上最有用的特征，我们在$Z$上随时间执行最大值操作。$m_i = maxZ(i,\\cdot) \\qquad \\text{$0 \\leq i \\leq n_1$} \\tag{2}$其中，$Z(i,\\cdot)$表示矩阵$Z$的第i行。最后，我们得到特征向量$m =\\{m_1, m_2, \\cdots, m_{n_1} \\} $,其维度不再与句子长度相关。 句子级别特征向量为了学习更复杂的特征，我们设计了一个非线性层，且选择双曲线tanh作为激活函数。tanh中的一个有用的属性是它的导数可以用函数值本身来表示：$\\frac{d}{dx} tanhx = 1 - tanh^2x \\tag{3}$它的优点是在反向传播训练过程中使得梯度的计算变得容易。形式上，非线性变换可以被写作：$g = tanh(W_2m) \\tag{4}$ $W_2 \\in \\Bbb R ^ {n_2 \\times n_1}$是线性变换矩阵，其中$n_2$(超参数)是隐藏层2的大小。相比于$m \\in \\Bbb R^{n_1 \\times 1}$, $g \\in \\Bbb R^{n_2 \\times 1}$可以被认为是更高级别的特征（句子级别）。 输出自动学到的词汇和句子级特征被串联成一个单独的向量$f = [l,g]$。为了计算每个关系的置信度,特征$f\\in \\Bbb R^{n_3 \\times 1}$($n_3$等于$n_2$加上词汇级别特征的维数)，被输入到softmax分类器。$o = W_3f \\tag{5}$ $W_3 \\in \\Bbb R^{n_4 \\times n_5}是变换矩阵，$o \\in \\Bbb R^{n_4 \\times 1}$是网络最后的输出，其中$n_4$等于关系分类系统中可能的关系类型的数量。然后可以将每个输出解释为对应关系的置信分数。通过应用softmax操作，这个分数能够被解释为条件概率。（参照下一节） 反向传播训练这里提出的基于DNN的关系分类方法可以被表示为5元组$\\theta = (X,N,W_1,W_2,W_3)$(N代表WordNet上位词的词嵌入）。在本文中，认为每个输入的句子是独立的。给出输入样例$s$,带有参数$\\theta$的网络输出向量$o$,其中第$i$个部分$o_i$包含了关系$i$的得分。为了获得条件概率$p(i\\mid x,\\theta)$,我们将对所有关系类型使用softmax操作：$p(i\\mid x,\\theta)=\\frac{e^{o_i}}{\\sum_{k=1}^{n_4}{e^{o^k}}} \\tag{6}$ 给出所有（假设T）训练示例$(x^{(i)};y^{(i)})$,可以写出参数的对数似然：$J(\\theta) = \\sum_{i=1}^{T}{\\log p(y^{(i)}\\mid x^{(i)},\\theta)} \\tag{7}$ 为了计算参数$\\theta$,我们使用简单优化方法SGD来最大化对数似然$J(\\theta)$.$N,W_1,W_2,W_3$是随机初始化的，$X$使用Word Embeddings初始化。由于这些参数在神经网络的不同层，我们实现后向传播算法：通过网络使用差异化链规则，迭代的选择样例$(x,y)$并且应用以下更新规则，直到word embedding层reached$\\theta \\leftarrow \\theta + \\lambda \\frac{\\partial \\log p(y\\mid x,\\theta)}{\\partial \\theta} \\tag{8}$ 数据集与评估指标使用SemEval-2010 Task 8 数据集。该数据集包含10,717个标注的实例，包括8000训练实例和2717测试实例。有9种关系（2个方向）以及没有方向的其他类型。下面是包含关系的例子：Cause-Effect(因果), Component-Whole(组成和整体), Entity-Origin(实体和来源)。 在官方评估框架中，考虑了方向性。如果关系中单词顺序是正确的，则这一对记为正确。例如，下列$S1$和$S2$具有相同的关系Component-Whole.$S_1 : The [half]_{e_1} of the [axe]_{e_2} is make \\cdots \\Rightarrow Component-Whole(e_1,e_2)$ $S_2： This [machine]_{e_1} has two [units]_{e_2}\\cdots \\Rightarrow Component-Whole(e_2,e_1)$ 然而，这两个实例不能被分成同一类，因为$Component-Whole(e_1,e_2)$和$Component-Whole(e_2,e_1)$是不同的关系。此外，参与系统的官方排名基于9个关系（不包括Other）的宏观平均F1值.为了将我们的结果与之前的研究结果进行比较，我们采用宏观平均F1值，并在下面的实验中考虑方向性。 实验该部分设计了3组实验。 第一部分是通过交叉验证来测试一些变体，以便了解超参数的选择是如何影响性能的。 第二部分，比较通过CNN学习的特征和一些传统特征的性能。 第三部分的目标则是评估每个抽取的特征的有效性。 参数设置本部分，我们通过实验研究提出方法中的3个参数的影响：卷积部分的窗口大小$w$,隐藏层1的数量，隐藏层2的数量。由于没有官方发布的数据集，我们通过5次交叉实验尝试不同的架构来调整超参数。 如图3，分别改变超参数$w$,$n_1$和$n_2$的数量并计算F1。当窗口大小超过3之后就不会提高性能了。由于训练数据集大小有限，网络容易过拟合，尤其是使用大的隐藏层，图3中我们也能看出，当增加隐藏层1和隐藏层2的数量时，参数对结果的影响有限。由于距离维度对结果影响不大（图3没有说明），我们启发式地选择$d_e = 5$.最后单词维度和学习率和Collobert一样。表2报告了使用的所有超参数。 比较实验的结果为了获得自动学习的特征的最终性能，我们选择了7个方法与我们的方法比较，如表3. 前5个是在Hendrickx(2010)论文中描述的，都使用传统特征，用SVM或MaxEnt做分类器。这些系统设计了一系列特征并且利用各种资源（Word net，ProBank，FrameNet等）。 RNN表示用做分类的循环神经网络（Socher 2012），该方法在句法树路径上学习连接两个名词向量以确定它们的语义关系。 MVRNN模型为最小成分构建单个语义，不能期望单个固定变换能够捕获所有自然语言运算符的意义组合，因此，MVRNN为每个单词分配矩阵并修改其他单词的含义，而不是仅考虑循环过程的词嵌入。 基于结果，做出以下观察： 使用传统方法时，丰富的特征集能带来更好的性能。这种改进可以通过从训练到测试数据的语义概括的需要来解释。传统特征的质量依赖于人工选择以及之前的NLP知识。人工选择最好的特征集几乎是不可能的。 RNN 和 MVRNN包含了特征学习的过程。它们依赖于循环过程中使用的语法树。句法分析中的错误会抑制这些方法学习高质量特征的能力。即使将POS，NER以及WordNet加入训练数据集，RNN仍然不能达到使用传统特征的最好方法那么高的性能。MVRNN能够有效捕捉组合意义并且达到更高的效果。 我们的方法在这些方法中达到了最好的效果。我们还执行了t检验($p \\leq 0.05)$,这表明我们的方法明显优于所比较的方法。 学到特征的效果词汇级别的特征主要包含5组特征（L1到L5）。我们在这5组特征上做了消融测试，以确定哪种特征贡献最大。 从表4中可以观察，我们学到的词汇级特征对关系分类是有效的。当添加新特征时，F1值显著提高。 我们也在句子级特征上做了实验。当添加PF时系统提高了9.2%. 当词汇级和句子级特征结合时，我们得到了最好的结果。"},{"title":"【论文笔记】Multi-instance Multi-label Learning for Relation Extraction","date":"2019-10-10T08:42:51.000Z","path":"2019/10/10/MIMR-2012-note/","text":"摘要提出了一种用于关系抽取的多实例-多标签学习方法，该方法使用具有隐变量的图模型共同对文本中一对实体的所有实例及其所有标签进行建模。该模型在两个困难领域具有竞争优势。 引言远程监督在关系抽取中，主要解决的是训练数据标注的问题，可以通过将文本与知识库对齐来自动生成训练数据。 在本文中，我们关注关系抽取中的远程监督，这是信息抽取的一个子问题，用于解决两个命名实体间关系的抽取。上面的图1展示了带有两个标签的关系抽取域的简单示例。 远程监督带来两个建模挑战：（1）通过这种方式获取的一些训练实例是无效的，例如图1，最后一句对于元组的任何已知标签都不是正确的。此类误报的百分比可能很高，Riedel et al.(2010)报告了Freebase中的关系与纽约时报文章语料库对齐后的误报率高达31%。（2）一组相同的实体可能会有多个标签，并且不清楚哪个标签是由给定元组的任何文本描述实例化的。例如，图1中元组(Barack Obama, United States)有两个有效标签：BornIn和EmployedBy，每个（潜在的）被实例化成不同的句子。在Riedel语料库中，训练分区中7.5%的实体元组具有多个标签。 多实例多标签学习概述，传统监督学习中，一个对象只有一个实例和一个标签，对于关系抽取，对象是一个包含两个命名实体的元组，文本中对于这个元组的每个描述都会生成一个不同的实例。 图2中总结了这个多实例多标签（MIML)学习问题.在本文中，我们提出一种新颖的图模型，称为MIML-RE，该模型针对MIML学习进行关系抽取。我们的工作做出以下贡献: (1)MIML-RE是第一个将多个实例（通过对分配给实例的潜在标签建模）和多标签（通过提供一种简单的方法来捕获标签之间的依赖关系）联合建模的RE方法。例如，我们的模型能学习某些标签倾向与联合生成，而其他标签则不能联合联合分配给同一元组。(2)我们证明MIML-RE在两个困难领域中有竞争性。"},{"title":"【LeetCode】27.移动元素","date":"2019-10-10T02:25:11.000Z","path":"2019/10/10/leetcode-27/","text":"27.移动元素给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。 注意这五个元素可为任意顺序。 你不需要考虑数组中超出新长度后面的元素。 题解分析解法1 - 双指针交换元素第一个想法是，用两个指针分别指向等于val的元素和其他元素，然后交换这两个位置的元素。 有一点类似快排的partition。123456789101112131415161718192021222324252627282930313233343536 public int removeElement(int[] nums, int val) &#123; if(nums == null || nums.length == 0) return 0; int p = 0;//val的指针 int q = 0;//非val的指针 int i = 0; //寻找数组中第一个等于val元素的位置，用p记录。 while(i &lt; nums.length)&#123; if(nums[i] == val)&#123; p = i; break; &#125;else i++; &#125; // 当i 等于数组长度时，说明没找到第一个等于val的元素，也就意味着数组中没有等于val的元素 // 那么不需要remove，直接return i，即数组长度即可。 if(i == nums.length)&#123; return i; &#125; //q从p的下一个位置开始进行遍历。 q = p + 1; //比较p和q位置的元素，将不等于val的q位置元素与p位置元素交换。 while(p &lt; nums.length || q &lt; nums.length)&#123; while(q &lt; nums.length &amp;&amp; nums[q] == val) q++; if(q == nums.length) break; swap(nums,p,q); while(nums[p] != val) p++; &#125; return p;&#125;private void swap(int[] nums, int i, int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp;&#125; 复杂度分析： 时间复杂度：O(n)空间复杂度：O(1) 解法2 - 双指针复制通过上面的方法我们进行了元素的交换，实际上我们不需要等于val的元素，也就意味我们并不需要交换，只需要将非val元素复制到val元素的位置上即可。 12345678910111213 // 解法2：双指针，当j指向的元素等于val时，就递增j（j++） // 当j指向的元素不等于val时，将j位置的元素复制到i位置，i++，j++public int removeElementV2(int[] nums, int val)&#123; if(nums == null || nums.length == 0) return 0; int i = 0; for(int j = 0; j &lt; nums.length; j++)&#123; if(nums[j] != val)&#123; nums[i] = nums[j]; i++; &#125; &#125; return i; &#125; 复杂度分析： 时间复杂度：O(n)假设数组总共有 n个元素，i和 j 至少遍历 2n 步。空间复杂度：O(1) 解法3 - 双指针复制，少量复制上面的方法我们发现，当数组为[1,2,3,4]，且val=4时，会将1，2，3都原地复制，这是没必要的。因此我们可以让双指针位于数组的首部和尾部，当前指针指向的元素等于val时，就将后指针指向的元素复制到前指针的位置，后指针前移。 如果后指针指向的元素也等于val怎么办？实际上不用担心，交换之后我们还是会对前指针的元素进行判断的。即，复制之后我们不会后移前指针，只有当前指针指向的元素不等于val才会后移。 123456789101112131415//解法3：双指针，头尾交换public int removeElementV3(int[] nums,int val)&#123; if(nums == null || nums.length == 0) return 0; int i = 0; int n = nums.length; while(i &lt; n)&#123; if(nums[i] == val)&#123; nums[i] = nums[n - 1]; n--; &#125;else&#123; i++; &#125; &#125; return n;&#125; 复杂度分析：&gt; 时间复杂度：O(n)空间复杂度：O(1) 在这个方法中，赋值操作的次数等于要删除的元素的数量。因此，如果要移除的元素很少，效率会更高。"},{"title":"【论文笔记】Knowledge-Based Weak Supervision for Information Extraction","date":"2019-10-09T14:36:31.000Z","path":"2019/10/09/hoffmann-2011-note/","text":"摘要基于知识的弱监督（远程监督），使用结构化的数据启发性地标记训练语料库，最近，研究人员开发了多实例学习算法来对抗可能来自启发式标签的嘈杂训练数据，但是他们的模型假定关系是不相交的，例如，他们无法提取成对的Founded(Jobs, Apple) 和 CEO-of（Jobs,Apple). 本文提出一种具有重叠关系的多实例学习的新方法，该方法将句子级的抽取模型和简单的语料库级的组件结合，以汇总单个情况。我们使用Freebase的弱监督，将模型应用于学习纽约时报文本的抽取器上。实验表明，该方法可以快速运行，并且在汇总和句子级别都有令人惊讶的准确率。 引言弱监督（or 远程监督）方法通过启发式地将数据库内容与相应文本进行匹配来创建自己的训练数据，例如假设r(e1,e2） = Founded(Jobs, Apple)是数据库中的3元组，而s = &quot;Steve Jobs founded Apple, Inc.&quot;是包含e1=Jobs 和 e2=Apple的同义词的句子,则s可能是r(e1,e2)事实的自然语言表达，并可能是一个有用的训练样例。 当文本语料库和数据库内容紧密对齐时，弱监督能很好地工作，Riedel et al.(2010)观察到，当该方法被广泛应用时（例如，将Freebase记录与纽约时报文章相匹配),会产生大量的噪声数据和较差的抽取性能。为了解决这个问题，为了解决这个问题，他们将弱监督作为多实例学习的一种形式，假设包含e1和e2的句子中至少有一个表示r（e1，e2），并且他们的方法在提取性能上有了实质性的改进。 但是，Riedel等人的模型（如先前系统的模型（Mintz等人，2009））假设关系不重叠，即不能存在两个事实r(e1,e2)和q(e1,e2)，对任何一对实体都正确。不幸的是，这个假设经常被违反，例如， Founded(Jobs, Apple) 和 CEO-of（Jobs,Apple)都是正确的，事实上，Freebase与纽约时报2007年语料库匹配的句子中，有18.3%存在重叠关系。 本文介绍了MULTIR，这是一种新型的弱监督模型，有以下作用： MULTIR引入了多实例学习的概率图形模型，该模型可以处理重叠的关系。 MULTIR还可以产生准确的句子级别预测，解码单个句子以及进行语料库级别提取。 MULTIR在计算上易于处理。推理归结为加权集覆盖，为此它使用贪婪近似，并使用最坏情况下的运行时间O(|R| · |S|)，其中R是可能的关系的集合，S是任何实体对的最大句子集合。实际上，MULTIR运行非常快。 我们提出的实验表明MULTIR优于Riedel等人的重新实现。额外的实验可以表征MULTIR的性能。 来自数据库的弱监督给定语料库，我们从中抽取有关实体的事实，例如公司Apple，城市Boston。基本的事实（或者说是关系实例），是一个表达式 $r(e)$,其中r是关系名，$e = e_1,…,e_n$是实体列表。 entity mention是表示一个实体的文本标记的连续序列。在本文中，我们假设有一个预言家能够识别语料库中所有entity mentions，但预言家并没有规范或者消除这些mentions的歧义，我们用$e_i\\in E$来表示实体及其名称。 relation mention是一连串的文本（包括一个或多个entity mentions）,它说明一些基本事实$r(e)$是真实的。例如，”Steve Ballmer, CEO of Microsoft, spoke recently at CES.”对于CEO-of(Steve Ballmer, Microsoft)包含3个entity mention和一个relation mention，在本文中，我们将关注二元关系，此外，我们假设两个实体mention都在单个句子中显示为名词短语。 聚合抽取的任务需要两个输入。$\\sum$,一组由语料组成的句子和一个抽取模型；作为输出，它应该产生一组基础事实，$I$,以便每个事实$r(e)\\in I$都在语料库中有所表达。 句法抽取采用相同的输入，并产生同样的$I$,但除此之外他还会产生函数$\\Gamma:I \\to P(\\sum)$,该函数为每个$r(e)\\in I$标识$\\sum$中包含的mention（描述为$r(e)$）的句子集合。通常语料库级别的抽取问题比较容易，因为它只需要进行聚合预测即可，也许使用语料库范围的统计信息即可。相反，句子级别的抽取必须用表达事实的每个句子来证明每次抽取的合理性。 总结基于知识的弱监督学习问题由(1)$\\sum$,训练语料库(2)$ E $, 一组在该语料库中提到的实体(3)$ R $, 一组关系名称(4)$ \\Delta $, R中一组关系的基本事实作为输入。 抽取模型作为输出。 重叠关系建模定义了一个无向图模型，允许对汇总（语料库级）和句子级抽取决策进行联合推理。图(a)显示了模型。 随机变量每对实体$ e = (e_1,e_2) \\in E \\times E$都有一个连接的组件，该组件为该对实体的所有提取决策建模。每个关系名称$ r \\in R $都有一个布尔输出变量 $ Y^r $ ，它表示$r(e)$是否为真。包括这组二值随机变量使我们的模型能够提取重叠关系。 令 $ S_(e_1,e_2) \\subset \\sum $为包含e1、e2 mention的句子集合，对每个句子$ x_i \\in S_(e_1,e_2)$，都存在一个隐变量$Z_i$，该变量在关系名称$r \\in R $范围内。仅当$x_i$表达基本事实$r(e)$时才应为$Z_i$分配值$r \\in R$，从而对句子级抽取进行建模。 算法 总结针对wrong label问题，使用弱监督方法提取重叠关系。"},{"title":"【论文笔记】Distant supervision for relation extraction without labeled data","date":"2019-10-09T07:25:06.000Z","path":"2019/10/09/mintz-2009-note/","text":"关系抽取关系抽取就是抽取一个句子中实体对之间的关系。要训练一个关系抽取器，给它一个句子两个实体，首先它需要知道为这两个实体之间的关系打什么标签，模型无法自己为关系取名，因此就需要人工标注这两个实体之间的关系是什么。当模型训练好之后，再遇到这样的实体对，就会知道是这个关系并把它抽取出来。 但问题是，人工标注是耗时耗力的一件事，而且数量实在有限，数据规模对模型训练又有影响，因此本篇论文的作者mintz首次提出了不依赖人工标注的关系抽取，也就是将远程监督应用到关系抽取上。 摘要对于像ACE这样的任务，关系抽取的现代模型是基于关系的有监督学习，使用小型人工标注的语料库。 本文提出不依赖人工标注，将远程监督应用在关系抽取中。 实验使用Freebase数据集（大型语义数据集，有数千个关系）提供远程监督。对于出现在Freebase关系中的每一对实体，我们在大型未标注语料库中寻找包含这些实体的所有句子，并且提取文本特征以训练关系分类器。 算法结合了监督IE（在概率分类器中结合400,000个噪声模式特征）和无监督IE（从任何域的大型语料库中提取大量关系)的优点。模型能够以67.6%的精度提取102个关系中的10000个实例。 论文还分析了feature performance，表明语法分析特征对于表达中含糊不清或者此法遥远的关系特别有用。 引言在有监督学习中，语料库中的句子首先被标记为实体的存在以及他们之间的关系。例如，NIST ACE RDC2003和2004语料库，包含超过1000个文档，其中实体对由5-7个主要关系类型以及23-24个子类型，共计16771个关系实例。ACE系统提取各种各样的词汇、句法和语义特征，并使用监督分类器来标记测试集句子中给定的一对实体之间的关系。 然而有监督的关系抽取也存在问题 人工标注代价昂贵，因此数量有限 关系被标注在特定语料库上，因此产生的分类器往往偏向于该文本域。 另一种方法是纯粹的无监督信息抽取，在大量文本的实体之间提取单词串，聚类并简化这些单词串以产生关系串。**无监督的方法可以使用非常大的数据，并且抽取非常大量的关系，但是产生的关系可能不容易映射到特定知识库所需要的关系。 第三种方法就是半监督学习，使用Bootstrapping进行关系抽取。对于要抽取的关系，该方法首先手工设定若干种子实例，然后迭代地从数据中抽取关系对应的关系模板和更多的实例。这种方法产生的模板往往是低精度且语义漂移的。 语义漂移：在迭代过程中会产生一些与种子不相关的实例，然后这些不相关实例再次进入迭代，频繁产生其他不相关实例。参考文献：Komachi M, Kudo T, Shimbo M, et al. Graph-based Analysis of Semantic Drift in Espresso-like Bootstrapping Algorithms.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2008, Proceedings of the Conference, 25-27 October 2008, Honolulu, Hawaii, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. 2008:1011-1020. 本文提出另一种范例-远程监督，他结合了这些方法的一些优点。本文的算法使用Freebase为关系抽取提供远程监督。Freebase包含900万个实体之间的7300个关系的1.16亿个实例。远程监督的假设是，任何包含一对在Freebase关系中的实体的句子都可以以某种方式表达这种关系。 也就是说，如果两个实体在某个知识图谱中存在关系，那么所有包含这两个实体的句子都已某种方式表述这种关系 由于可能有许多含有给定实体对的句子，因此我们可以提取非常大量的（可能有噪声的）特征，这些特征被合并到逻辑回归分类器中。 我们的范例提供了整合多个句子数据的自然方式，以决定两个实体之间是否存在关系。由于我们的算法可能会使用大量未标记的数据，因此一对实体可能会在测试集中多次出现。对于每一对实体，我们将来自许多不同语句的特征汇总到一个特征向量中，从而使我们能够为分类器提供更多信息，从而产生更准确的标签。 Freebase后文中，使用‘relation’来指代实体之间的有序二元关系，我们将这个关系中的单个有序对称为’relation instance’。 例如，在实体’John Steinbeck’和 ‘United States’之间存在关系’person-nationality’，因此将(John Steinbeck, United States)作为一个instance。 我们使用Freebase的关系和关系实例，Freebase是一个免费的结构化语义数据在线数据库。Freebase中的数据是从各种来源收集的。一个主要来源是维基百科的文本框和其他表格数据。数据还来自NNDB（传记信息），MusicBrainz（音乐），SEC（财务和公司数据）以及直接的维基风格用户编辑。经过对2008年7月链接导出的一些基本处理，将Freebase的数据表示转换为二元关系后，我们有900万个实体之间的7,300个关系，有1.16亿个实例。接下来我们将筛选出无用和不感兴趣的的实体，例如用户配置文件和音乐曲目。 Freebase也包含了许多关系的逆转（bookauthor v.author-book），并将这些关系合并。 过滤和删除除最大关系以外的所有关系，筛选出了94万个实体，102种关系和180万实体对。 关系种类相当于分类的类别，那么有102类；每种关系对应的所有实体对就是样本；从Wikipedia中所有包含某实体对的句子中抽取特征，拼接成这个样本的特征向量。最后训练LR多分类器，用One-vs-Rest，而不是softmax，也就是训练102个LR二分类器——把某种关系视为正类，把其他所有的关系视为负类。补充：在One-vs-Rest策略中，假设有n个类别，那么就会建立n个二项分类器，每个分类器针对其中一个类别和剩余类别进行分类。进行预测时，利用这n个二项分类器进行分类，得到数据属于当前类的概率，选择其中概率最大的一个类别作为最终的预测结果。 架构我们的远程监督方法的核心是使用Freebase构建我们关系的训练集，以及参与这些关系的实体对。 训练阶段 1.在训练阶段，所有实体都使用标记persons，organizations和locations的命名实体标记器(NET)在句子中标识。也就是用命名实体识别工具把语料库中句子的实体识别出来。 2.如果句子包含两个实体，并且这些实体是Freebase关系之一的实例，则从该句子中提取特征并将其添加到关系的特征向量中。训练多类逻辑回归分类器，在训练中，来自不同句子的相同元组（关系，实体1，实体2）的特征相结合，创建更丰富的特征向量。也就是从这些包含这俩实体的句子中提取文本特征，拼接成一个向量，作为这种关系的一个样本的特征向量，用来训练分类器。 远程监督的假设是,如果两个实体之间存在一种关系，那么包含这两个实体的任何句子都可能表达这种关系。因为任何一个单独的句子都可能给出错误提示，所以我们的算法会训练一个多类逻辑回归分类器，为每个噪音特征学习权重。在训练中，来自不同句子的相同元组（关系，实体1，实体2）的特征相结合，创造更丰富的特征向量。 测试阶段 再次使用NET标记实体，这一次，在句子中同时出现的每对实体都被认为是潜在的关系实例，并且当这些实体同时出现时，将在该句子上提取特征并且添加到该实体对的特征向量中。例如，如果一组实体出现在测试集的10个句子中，并且每个句子都有3个特征是从它里边提取的，那么这个实体对将有30个关联特征。测试语料中的每个句子中的每个实体对都通过特征提取运行，回归分类器根据每个实体对出现的所有句子的特征预测一个关系名称。 考虑位置包含（location-contains)关系，假设Freebase中我们有两个这种关系的实例：(Virginia,Richmond)(France,Nantes)当我们遇到句子&#39;Richmond, the capital of Virginia&#39;以及&#39;Henry&#39;s Edict of Nantes helped the Protestants of France&#39;（亨利的南特诏书帮助了法国的新教徒）,我们将从这些句子中提取特征。有的特征可能会非常有用，例如Richmond这个句子中的特征；有的可能不那么有用，例如Nantes这个句子中的特征。在测试中，如果我们能够发现&#39;Vienna, the capital of Austria&#39;这样的句子，它的一个或多个特征将会匹配到Richond句子的特征，从而证明(Austria, Vieena)属于位置包含关系。 请注意，我们体系结构的主要优点之一是它能够将来自许多不同mention（描述）的相同关系的信息结合起来。从以下两个句子中考虑实体对(Steven Spielberg, Saving Private Ryan)，作为film-director关系的证据。 [Steven Spielberg]’s film [Saving Private Ryan] is loosely based on the brothers’ stroy.([史蒂文·斯皮尔伯格]的电影[拯救大兵瑞恩]大致是根据兄弟俩的故事改编的。) Allison co-produced the Academy Award-winning [Saving Private Ryan], directed by [Steven Spielberg](艾莉森联合制作了奥斯卡获奖影片《拯救大兵瑞恩》，导演是史蒂文·斯皮尔伯格) 第一句虽然为film-director 电影导演提供了证据，但却可以作为filmwrite电影编剧或film-producer 电影制片人的证据。第二句话并没有提到拯救大兵瑞恩是个电影，因此可以成为CEO关系的证据（考虑，’Robert Mueller directed the FBI’).孤立的说，这些特征都不是决定性的，但他们组合起来则是决定性的。 特征特征提取中用到的特征包含词法特征、句法特征和命名实体标签特征。 词法特征： 词法特征描述了出现在句子中两个实体之间和周围的特定词语，这些特定词语出现在： 两个实体之间的单词序列 这些单词的词性标签 标志位表示哪个实体出现在前面 实体1左侧的k个单词及其词性标签 实体2右侧的k个单词及其词性标签 $ k \\in {0,1,2} $ 每个词法特征由所有这些组件连接而成。我们为每个$k \\in {0,1,2} $生成一个连接特征。词性标签由宾夕法尼亚州立大学训练的最大熵标记器分配，然后简化为7类：nouns名词，verbs动词，adverbs副词，adjectives形容词，numbers数词，foreign words外来词以及其他。 为了逼近句法特征，我们还测试了词法特征的变化：（1）忽略所有不是动词的词（2）省略所有的功能词结合其他词汇特征，它们对精度有了小的提升，但还不足以证明对计算资源的需求增加。 这里就是获取这些特征，然后把这些特征表示成向量再拼接起来，比如词袋模型，把词语和词性都标识为向量。 句法特征 参照：https://blog.csdn.net/u014422406/article/details/53954530利用依存句法解析器MINIPAR对句子进行解析，然后从解析树中提取实体的依赖路径。 a) 两个实体之间的最短依存路径； b) 两个实体的左右窗口。 命名实体标注特征使用斯坦福的NET进行命名实体标记（人名、地名、组织名和其他） NET是一个标记器，只是用来对实体进行标记的。 总结，论文中使用的特征不是单个特征，而是多种特征拼接起来的。有多个句子包含某实体对，可以从每个句子中抽取出词法特征、句法特征和实体特征，拼接起来，得到一个句子的特征向量，最后把多个句子的特征向量再拼接起来，得到某实体对（一个样本）的特征向量。 训练阶段假设freebase中存在（ Edwin Hubble，Marshfield，place of birth ）这一实例。 训练数据句子为“Astronomer Edwin Hubble was born in Marshfield, Missouri.” 1）首先使用NET将‘Edwin Hubble’标记为person，将‘Marshfield’标记为location. 2）实体Edwin Hubble和实体Marshfield是freebase关系之一的实例。所以对句子进行特征提取： 词汇特征与命名实体标注特征： 句法特征： 3）使用联合特征训练多类逻辑回归分类器。 测试阶段测试数据句子为“Galileo was born in Pisa.” 1）首先使用NET将‘Galileo’标记为person，将‘Pisa’标记为location. 2）对句子进行特征提取 3）判断‘Galileo’与‘Pisa’是place of birth 关系。 实现Text 文本对于非结构化文本，我们使用Freebase Wikipedia Extraction，它是所有维基百科文章（不包括讨论和用户页面）全文的转储，由Freebase（Metaweb，2008）的开发者Metaweb Technologies进行了句子标记。这个转储包含大约180万篇文章，平均每篇文章14.3个句子。单词总数（标点符号）为601,600,703。对于我们的实验，我们使用大约一半的文章：训练集80万和测试集40万。(留出法进行自动模型评估，一半实体用来训练，另一半实体用于对模型评估） 构造负样本由于有102种关系，每种关系都需要训练一个LR二分类器，所以需要构造负样本。这里的负样本不是其他101种关系的训练样本，而是从训练集中的句子中抽取实体时，如果实体不在Freebase中，那么就随机挑选这样的句子作为负样本。 解析和分块这个非结构化文本的每个句子都是由MINIPAR解析的依存关系来产生一个依存关系图。 训练和测试训练过程 LR分类器以实体对的特征向量作为输入，输出关系名和概率。每种关系训练一个二分类器，一共训练102个分类器。 训练好分类器，对测试集中的所有实体对的关系进行预测，并得到概率值。然后对所有实体对按概率降序排列，从中挑选出概率最高的N个实体对（概率值&gt;0.5)作为发现的新实体对。 测试方法 指标是准确率，方法采用留出法（自动评估）和人工评估两种方法。留出法的做法是，将Freebase中180万实体对的一半作为测试集（另一半用于训练）。新发现的N个实体对中，如果有n对在Freebase的测试集中，那么准确率为$\\frac nN$.人工评估则采用多数投票的方式。 模型评估结果表明远程监督是一种较好的方法。在文本特征的比较上，词法特征和句法特征拼接而成的特征向量，优于单独使用其中一种特征的情况。此外，句法特征在远程监督中比词法特征更有效，尤其对于依存句法结构比较短而实体对之间的词语非常多的句子。 评价这篇论文把远程监督的思想引入了关系抽取中，充分利用未标注的非结构化文本，从词法、句法和实体三方面构造特征，最后用留出法和人工校验两种方法进行模型评估，是一种非常完整规范的关系抽取范式。 优点 1.使用知识库提供训练数据来取代人工标注获取训练数据，没有过拟合的问题和领域依赖的问题，而且不需要很多已经标注好的数据，可以使用更大的数据集； 2.综合一种关系的多种描述instance：(Steven Spielberg, Saving Private Ryan)relation: film-director这个例子（可以参考前面架构一节的介绍） 缺点 1.基于给出的假设，训练集会产生大量的 wrong labels（噪声问题），比如两个实体有多种关系或者根本在这句话中没有任何关系。这样的训练数据会对关系抽取器产生影响。（例如，”Steve Jobs”, “Apple”在 Freebase 中存在 founder 的关系，如”Steven Jobs passed away the day before Apple unveiled iPhone 4s in late 2011.”句话中并没有表示出 Steven Jobs 与 Apple 之间存在 founder 的关系） 2.NLP 工具带来的误差，比如 实体标注器和依存句法解析器，中间过程出错会造成错误传播问题,也就是说越多的 feature engineering 就会带来越多的误差，在整个任务的 pipeline 上会产生误差的传播和积累，从而影响后续关系抽取的精度。 关于问题1中 wrong labels 的问题，有的工作将关系抽取定义为一个 Multi-instance Multi-label 学习问题，比如工作 Multi-instance Multi-label Learning for Relation Extraction，训练集中的每个 instance 都可能是一种 label。 而有的工作（本文的工作）则是将问题定义为 Multi-instance Single-label 问题，假设共现的 entity 对之间只存在一种关系或者没有关系，一组包括同一对 entities 的 instances 定义为一个 Bag，每一个 Bag 具有一个 label，最终训练的目标是优化 Bag Label 的准确率。第一种假设更加接近于实际情况，研究难度也相对更大一些。 关于问题2中的 pipeline 问题，用深度学习的思路来替代特征工程是一个非常自然的想法，用 word embedding 来表示句子中的 entity 和 word，用 RNN 或者 CNN 以及各种 RNN 和 CNN 的变种模型来对句子进行建模，将训练句子表示成一个 sentence vector，然后进行关系分类，近几年有几个工作都是类似的思路，比如：[3] Relation Classification via Convolutional Deep Neural Network [4] Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks [5] Neural Relation Extraction with Selective Attention over Instances [6] Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Deions 参考博客远程监督浅谈Freebase Data Dump结构初探一周论文 | 关于远程监督，我们来推荐几篇值得读的论文 关系抽取之远程监督算法"},{"title":"【LeetCode】283.移动零","date":"2019-10-09T01:54:36.000Z","path":"2019/10/09/leetcode-283/","text":"283.移动零给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。 示例:&gt; 输入: [0,1,0,3,12] 输出: [1,3,12,0,0] 说明: 必须在原数组上操作，不能拷贝额外的数组。尽量减少操作次数。 题解分析题目要求把0移动到数组末尾，那么我们很自然就可以想到，可以将非0元素都移动到数组左端，剩下的就填0，那么怎么做呢？ 原始做法 - 使用辅助数组我们可以将原数组中非0的元素复制到新数组中，然后将新数组的元素赋值给旧数组。 123456789101112131415public void moveZeroes(int[] nums)&#123; if(nums == null || nums.length == 0) return; int[] newArray = new int[nums.length]; int index = 0; //step1 : 复制非0元素 for(int i = 0; i &lt; nums.length; i++)&#123; if(nums[i] != 0)&#123; newArray[index++] = nums[i]; &#125; &#125; //step2：更新原数组 for(int i = 0; i &lt; newArray.length; i++)&#123; nums[i] = newArray[i]; &#125;&#125; 复杂度分析：&gt; 时间复杂度：O(n)空间复杂度：O(n) 这种方法不符合题目的要求，因为使用了辅助数组。 改进1 - 将非0元素填充到前一个0的位置，最后用0填充后面的位置由上一种方法可以想到，我们可以遍历一遍数组的时候，用index记录0元素的位置，遇到非0元素就将它填充到前一个0的位置上，使得[0,index)区间内都是非0元素，[index,nums.length)区间用0填充。1234567891011121314public void moveZeroes(int[] nums)&#123; if(nums == null || nums.length == 0) return; int index = 0;//0元素初始位置 //step1:复制非0元素到前一个0元素位置 for(int i = 0; i &lt; nums.length; i++)&#123; if(nums[i] != 0)&#123; nums[index++] = nums[i]; &#125; &#125; //step2：将index之后的元素全部填充为0 while(index &lt; nums.length)&#123; nums[index++] = 0; &#125;&#125; 复杂度分析： 时间复杂度：O(n)空间复杂度：O(1) 考虑到最终还需要填充0，是否可以减少操作次数呢？ 改进2-将0元素与非0元素交换可以将0元素与非0元素交换，那么，就不存在最后一步填充0了。 这里还需要考虑到的一个极端情况是，数组没有0元素时，不管是改进1还是改进2都会对当前非0元素与自身进行复制/交换，可以通过添加一个小判断，减少操作的次数。 12345678910111213141516171819public void moveZeroes(int[] nums)&#123; if(nums == null || nums.length == 0) return; int index = 0; for(int i = 0; i &lt; nums.length;i++)&#123; if(nums[i] != 0)&#123;//非0元素 if(i != index)&#123;//如果i不等于index，才交换 swap(nums,i,index++); &#125;else&#123;//否则index++ index++; &#125; &#125; &#125;&#125;private void swap(int[] nums,int i, int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp;&#125; 复杂度分析： 时间复杂度：O(n)空间复杂度：O(1)"},{"title":"【关系抽取】相关概念-1","date":"2019-10-08T13:09:20.000Z","path":"2019/10/08/related-work-for-relations-extraction-adai/","text":"CIPS青工委学术专栏第3期 | 基于深度学习的关系抽取基于深度学习的关系抽取 https://blog.csdn.net/qq_35203425/article/details/81138978 现有主流的关系抽取技术分为有监督的学习方法、半监督的学习方法和无监督的学习方法三种： 1、有监督的学习方法将关系抽取任务当做分类问题，根据训练数据设计有效的特征，从而学习各种分类模型，然后使用训练好的分类器预测关系。该方法的问题在于需要大量的人工标注训练语料，而语料标注工作通常非常耗时耗力。 2、半监督的学习方法主要采用Bootstrapping进行关系抽取。对于要抽取的关系，该方法首先手工设定若干种子实例，然后迭代地从数据中抽取关系对应的关系模板和更多的实例。 3、无监督的学习方法假设拥有相同语义关系的实体对拥有相似的上下文信息。因此可以利用每个实体对对应上下文信息来代表该实体对的语义关系，并对所有实体对的语义关系进行聚类。 与其他两种方法相比，有监督的学习方法能够抽取更有效的特征，其准确率和召回率都更高。因此有监督的学习方法受到了越来越多学者的关注，本文也将重点介绍该类方法。 【基于有监督学习的关系抽取】 有监督的关系抽取系统通常需要大量人工标注的训练数据，从训练数据中自动学习关系对应的抽取模式。有监督关系抽取方法主要包括：基于核函数的方法[Zhao and Grishman 2005; Bunescu and Mooney 2006]，基于逻辑回归的方法[Kambhatla 2004]，基于句法解析增强的方法[Miller et al. 2000]和基于条件随机场的方法[Culotta et al. 2006]。然而，阻碍这些系统效果继续提升的主要问题在于，人工标注训练数据需要花费大量的时间和精力。 针对这个局限性，Mintz等人[Mintz et al. 2009]提出了远程监督（Distant Supervision）的思想。作者们将纽约时报新闻文本与大规模知识图谱Freebase（包含7300多个关系和超过9亿的实体）进行实体对齐。远程监督假设，一个同时包含两个实体的句子蕴含了该实体对在Freebase中的关系，并将该句子作为该实体对所对应关系的训练正例。作者在远程监督标注的数据上提取文本特征并训练关系分类模型，有效解决了关系抽取的标注数据规模问题。之后许多研究者从各个角度对远程监督技术提出了改进方案。例如Takamatsu等人[Takamatsu et al. 2012]改进了实体对齐的技术，降低了数据噪音，提高了关系抽取的总体效果。Yao等人[Yao et al. 2010]提出了基于无向图模型的关系抽取方法。Riedel等人[Riedel et al. 2010]则增强了远程监督的假设，与 [Mintz et al.2009]相比错误率减少了31%。 以上远程监督技术都假设一个实体对只对应一种关系。但是，很多实体之间具有多种关系。例如，“Steve Jobs founded Apple”和“Steve Jobs is the CEO of Apple”。因此，Hoffmann等人[Hoffmann et al. 2011]提出采用多实例多标签（Multi-Instance Multi-label）方法来对关系抽取进行建模，刻画一个实体对可能存在多种关系的情况。类似地，Surdeanu等人[Surdeanu et al. 2012]也提出利用多实例多标签和贝叶斯网络来进行关系抽取。 【基于深度学习的关系抽取】现有的有监督学习关系抽取方法已经取得了较好的效果，但它们严重依赖词性标注、句法解析等自然语言处理标注提供分类特征。而自然语言处理标注工具往往存在大量错误，这些错误将会在关系抽取系统中不断传播放大，最终影响关系抽取的效果。 最近，很多研究人员开始将深度学习的技术应用到关系抽取中。[Socher et al. 2012] 提出使用递归神经网络来解决关系抽取问题。该方法首先对句子进行句法解析，然后为句法树上的每个节点学习向量表示。通过递归神经网络，可以从句法树最低端的词向量开始，按照句子的句法结构迭代合并，最终得到该句子的向量表示，并用于关系分类。该方法能够有效地考虑句子的句法结构信息，但同时该方法无法很好地考虑两个实体在句子中的位置和语义信息。 [Zeng et al. 2014] 提出采用卷积神经网络进行关系抽取。他们采用词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。后来，[Santos et al. 2015]还提出了一种新的卷积神经网络进行关系抽取，其中采用了新的损失函数，能够有效地提高不同关系类别之间的区分性。 [Miwa et al. 2016] 提出了一种基于端到端神经网络的关系抽取模型。该模型使用双向LSTM（Long-Short Term Memory，长短时记忆模型）和树形LSTM同时对实体和句子进行建模。目前，基于卷积神经网络的方法在关系抽取的标准数据集SemEval-2010 Task 8上取得了最好的效果。 上面介绍的神经网络模型在人工标注的数据集上取得了巨大成功。然而，与之前基于特征的关系抽取系统类似，神经网络关系抽取模型也面临着人工标注数据较少的问题。对此，[Zeng et al. 2015]尝试将基于卷积神经网络的关系抽取模型扩展到远程监督数据上。[Zeng et al. 2015]假设每个实体对的所有句子中至少存在一个句子反映该实体对的关系，提出了一种新的学习框架：以实体对为单位，对于每个实体对只考虑最能反映其关系的那个句子。该方法在一定程度上解决了神经网络关系抽取模型在远程监督数据上的应用，在NYT10数据集上取得了远远高于基于特征的关系抽取模型的预测效果。但是，该方法仍然存在一定的缺陷：该模型对于每个实体对只能选用一个句子进行学习和预测，损失了来自其他大量的有效句子的信息。 我们有没有可能把实体对对应的有噪音的句子过滤掉，然后利用所有有效句子进行学习和预测呢？ [Lin et al. 2016] 提出了一种基于句子级别注意力机制的神经网络模型来解决这个问题，该方法能够根据特定关系为实体对的每个句子分配权重，通过不断学习能够使有效句子获得较高的权重，而有噪音的句子获得较小的权重。与之前的模型相比，该方法效果取得较大提升。我们也将相关代码发布在Github上：https://github.com/thunlp/NRE。 【总结及未来趋势】 近年来，深度学习在自然语言处理领域的很多方向取得了巨大成功，本文以关系抽取为例，介绍了如何利用深度学习的语义表示和学习能力，自动地从训练数据中学习分类特征，从而取得比传统方法更好的关系抽取效果。当然，关系抽取系统性能还有很大提升空间，仍然有很多问题亟待解决。 首先，基于句法树的树形LSTM神经网络模型在关系抽取上取得了不错的效果，这说明句法信息的引入对于关系抽取有一定帮助。然而，目前的句法分析仍然存在较多错误，在考虑句法信息的同时也引入了大量错误噪音。有研究表明，如果对于一个句子考虑其最可能的多个句法分析树，分析结果准确率可以得到较大提升。因此，一个重要的研究方向是，如何有效地将句子的多个可能句法树信息结合起来，用于关系抽取。 其次，目前的神经网络关系抽取主要用于预先设定好的关系集合。而面向开放领域的关系抽取，仍然是基于模板等比较传统的方法。因此，我们需要探索如何将神经网络引入开放领域的关系抽取，自动发现新的关系及其事实。此外，对现有神经网络模型如何对新增关系和样例进行快速学习也是值得探索的实用问题。 最后，目前关系抽取主要基于单语言文本。事实上，人类知识蕴藏于不同模态和类型的信息源中。我们需要探索如何利用多语言文本、图像和音频信息进行关系抽取。 ADai的Related-Work 关系抽取有两种框架，一种是pipeline方法，另一种是联合抽取方法（主要做的）。 传统的pipeline方法把抽取任务看作两个独立的抽取任务。首先利用然语言处理工具对文本中的实体进行识别，然后对实体之间的关系进行分类。 为了解决人工标注问题，神经模型在关系分类中得到了广泛的应用，包括卷积神经网络（CNN）、循环神经网络（RNN）和长期短期记忆网络（LSTM）。 mintz[5]首先提出了利用远程监控进行关系抽取。Zeng[6]提出了多实例学习来解决数据标注错误问题。近年来，强化学习也被应用于信息抽取领域。Qin[7]提出了一个强化学习框架，并重建了一个更为纯粹的训练集。此外，hoffmann[8]使用弱监督方法提取重叠关系。yu[9]提出了一个具有任意图形结构的联合判别概率模型来同时优化所有相关子任务。li[10]采用基于特征工程的结构化系统方法提取关系。yu[9]和li[10]应用非常复杂的特征工程方法提取关系。虽然pipeline方法使得每个提取组件更灵活，但下游结果容易受到上游任务的影响，导致错误的累积。 为了解决这一问题，人们提出了许多联合提取模型。miwa[11]采用了一种树形结构来联合提取实体和关系，并通过参数共享进行联合学习。Zheng[12]还分享了用于联合学习的神经网络的基本参数。Zheng[1]提出将关系抽取问题转化为序列标注问题，利用递归神经网络联合抽取实体和关系。但是，该模型仅在三元组没有重叠实体，并且具有重叠实体的三元组不能很好地处理的情况下使用。针对这一问题，Zeng[3]提出利用复制机制来解决重叠关系，将重叠实体复制到多个解码器中，解码器生成不同的三元组，但这种方法很大程度上依赖于训练数据的标注。该方法无法处理具有多个单词的实体。takanobu[13]使用强化学习来提取关系，并将关系提取任务视为半马尔可夫决策过程，通过高级强化学习（RL）过程检测关系，并通过低级rl过程来识别关系的参与实体。该模型在NYT数据集上取得了很好的效果。 在编码器的使用中，由于Bi-LSTM在语义提取方面的优越性能，大多数模型都采用Bi-LSTM进行编码。但是除了获取词义信息外，语义单元信息的提取也非常重要。多级扩展卷积[17]用于捕获词与词之间的局部和长期依赖关系，生成语义单元的表示，在文本情感分析中表现出优异的性能[2]。扩张卷积可以增加接收场，而不需要汇聚层，从而造成信息丢失，使得每个卷积的输出包含了大量的信息。扩展卷积，例如图像分割、语音合成[18]和机器翻译[16]，可以很好地应用于需要全局信息的图像处理任务或需要依赖长序列信息的语音和文本处理。 在上述的联合提取方法中，只有Zeng[3]和Takanobu[13]考虑重叠关系问题。在关系抽取中，Zeng[3]使用解码器直接解码并添加全连接层来获得关系类型，Takanobu[13]是基于强化学习来获得关系类型的。与这些方法不同的是，我们将关系的提取看作是一个多标签分类问题，并利用分类器链的方法来获取关系类型。并利用扩展卷积结合Bi-LSTM对文本进行编码，更充分地提取文本中的语义。 论文整理Relation Classification via Convolutional Deep Neural Network（2014） 远程监督pipeline方法 远程监督： Distant supervision for relation extraction without labeled data （2009） 本文提出不依赖人工标注，将远程监督应用在关系抽取中。使用Freebase提供远程监督。训练阶段一开始是NET标记实体，然后拿这两个实体去freebase中找，找到了就提取特征（词法特征：词性标签是由宾夕法尼亚大学训练的最大熵标记器分配，然后简化为7类，句法特征：依存句法解析器MINIPAR)，然后再把这些特征扔到LR里去训练分类器。 wrong label问题：基于给定的假设，训练集会产生大量wrong label产生一个改进方向：多实例多标签。 误差传播问题：在特征提取的过程中由于使用了NLP工具，工具在标注和解析中产生的误差会影响到特征的准确性，也就是说会产生误差，造成误差传播问题。产生一个改进方向：用深度学习思路代替特征工程，也就是用神经网络模型来做特征的提取。 多实例-多标签上面的远程监督假设一对实体对应一种关系，但是，很多实体之间具有多种关系。因此这两篇采用多实例-多标签进行关系抽取： Knowledge-Based Weak Supervision for Information Extraction（2011） 使用弱监督方法处理重叠关系。引入多实例学习的概率图模型，可用来处理重叠关系。 Multi-instance Multi-label Learning for Relation Extraction（2012） 利用多实例多标签和贝叶斯网络来进行关系抽取。"},{"title":"【知识图谱】基础-1","date":"2019-10-08T03:06:51.000Z","path":"2019/10/08/KG-basic/","text":"什么是知识图谱？学术角度：知识图谱本质上是语义网络（Semantic Network）的知识库。 实际应用角度：多关系图（Multi-relational Graph）。 什么是多关系图？包含多种类型的节点和多种类型的边。 在知识图谱里，我们通常用“实体（Entity）”来表达图里的节点、用“关系（Relation）”来表达图里的“边”。实体指的是现实世界中的事物比如人、地名、概念、药物、公司等，关系则用来表达不同实体之间的某种联系。 知识图谱的表示知识图谱应用的前提是已经构建好了知识图谱，也可以把它认为是一个知识库。 属性图：当一个知识图谱拥有属性时，我们可以用属性图（Property Graph）来表示。 RDF：由很多的三元组（Triples）来组成。RDF 在设计上的主要特点是易于发布和分享数据，但不支持实体或关系拥有属性，如果非要加上属性，则在设计上需要做一些修改。 目前来看，RDF 主要还是用于学术的场景，在工业界我们更多的还是采用图数据库（比如用来存储属性图）的方式。 知识抽取知识图谱的构建是后续应用的基础，而且构建的前提是需要把数据从不同的数据源中抽取出来。对于垂直领域的知识图谱来说，它们的数据源主要来自两种渠道：一种是业务本身的数据，这部分数据通常包含在公司内的数据库表并以结构化的方式存储；另一种是网络上公开、抓取的数据，这些数据通常是以网页的形式存在所以是非结构化的数据。 前者一般只需要简单预处理即可以作为后续 AI 系统的输入，但后者一般需要借助于自然语言处理等技术来提取出结构化信息。比如在上面的搜索例子里，Bill Gates 和 Malinda Gate 的关系就可以从非结构化数据中提炼出来，比如维基百科等数据源。 信息抽取的难点在于处理非结构化数据. 在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术： a. 实体命名识别（Name Entity Recognition） b. 关系抽取（Relation Extraction） c. 实体统一（Entity Resolution） d. 指代消解（Coreference Resolution） 在下面的图中，我们给出了一个实例。左边是一段非结构化的英文文本，右边是从这些文本中抽取出来的实体和关系。下面针对每一项技术解决的问题做简单的描述，以至于这些是具体怎么实现的，不在这里一一展开. 首先是实体命名识别，就是从文本里提取出实体并对每个实体做分类(打标签)：比如从上述文本里，我们可以提取出实体 -“NYC”，并标记实体类型为 “Location”；我们也可以从中提取出“Virgil’s BBQ”，并标记实体类型为“Restarant”。这种过程称之为实体命名识别，这是一项相对比较成熟的技术，有一些现成的工具可以用来做这件事情。 其次，我们可以通过关系抽取技术，把实体间的关系从文本中提取出来，比如实体“hotel”和“Hilton property”之间的关系为“in”；“hotel”和“Time Square”的关系为“near”等等。 另外，在实体命名识别和关系抽取过程中，有两个比较棘手的问题：一个是实体统一，也就是说有些实体写法上不一样，但其实是指向同一个实体。比如“NYC”和“New York”表面上是不同的字符串，但其实指的都是纽约这个城市，需要合并。实体统一不仅可以减少实体的种类，也可以降低图谱的稀疏性（Sparsity）. 另一个问题是指代消解，也是文本中出现的“it”, “he”, “she”这些词到底指向哪个实体，比如在本文里两个被标记出来的“it”都指向“hotel”这个实体。 实体统一和指代消解问题相对于前两个问题更具有挑战性 知识图谱的存储知识图谱主要有两种存储方式：一种是基于 RDF 的存储；另一种是基于图数据库的存储。 RDF 一个重要的设计原则是数据的易发布以及共享，图数据库则把重点放在了高效的图查询和搜索上。其次，RDF 以三元组的方式来存储数据而且不包含属性信息，但图数据库一般以属性图为基本的表示形式，所以实体和关系可以包含属性，这就意味着更容易表达现实的业务场景。 它们之间的区别如下图所示。 这里列出了常用的图数据库系统以及他们最新使用情况的排名。 其中 Neo4j 系统目前仍是使用率最高的图数据库，它拥有活跃的社区，而且系统本身的查询效率高，但唯一的不足就是不支持准分布式。 相反，OrientDB 和 JanusGraph（原 Titan）支持分布式，但这些系统相对较新，社区不如 Neo4j 活跃，这也就意味着使用过程当中不可避免地会遇到一些刺手的问题。如果选择使用 RDF 的存储系统，Jena 或许一个比较不错的选择。 知识图谱的体系架构"},{"title":"【移动Web】知识点-1","date":"2019-09-29T01:37:25.000Z","path":"2019/09/29/2019-9-29-mobile-web-note-1/","text":"移动Web知识点Viewport什么是css像素，物理像素？ 手机打开PC页面刚好被等比例缩放？移动设备视窗概念 layout viewport（布局视窗）：浏览器初始视窗大小和浏览器厂商有关 Visual viewport（物理视窗）：可视范围的大小 Ideal viewport（理想视窗）：没有固定尺寸，不同设备的ideal viewport不同，所有iphone的ideal viewport都是320px，只要把某个个元素的宽度设为ideal viewport的宽度，那么这个元素的宽度就是屏幕的宽度。意义在于，无论何种分辨率的屏幕，针对ideal viewport设计的网站，不需要手动缩放，也不需要横向滚动条，都可以完美呈现。 等比例缩放是一种浏览器的特性，将页面缩到visual viewport的可视区域内。这个特性实际上有不好的地方，因为图片、文字都会缩小，使用手机浏览是非常不方便的。 设备宽高和viewport有什么关系？iphone、ipad以及IE会横竖屏不分，通通以竖屏的宽度为ideal viewport的宽度。 如何使用 meta设置viewport？如何设置ideal viewport？&lt;meta name = &quot;viewport&quot; content=&quot;width=device-width,initial-scale=1.0, maximum-scale=1.0, user-scalable=0&quot;/&gt;这个meta标签的作用是，让当前viewport宽度等于设备宽度(ideal viewport的宽度)，同时不允许用户手动缩放。 meta viewport首先是苹果在safari引入的，在苹果的规范中，meta viewport有6个属性（content中）： width：设置layout viewport的宽度，正整数，或着字符串“width-device” initial-scale：页面的初始缩放值，数字，可以是小数。 minimum-scale：允许用户的最小缩放值，数字，可以是小数。 maximum-scale：允许用户的最大缩放值，数字，可以是小数。 heigh：设置layout viewport 的高度，这个属性并不重要，很少使用。 user-scalable：是否允许用户进行缩放，值“no”或“yes”，0或1. 在Android中还支持target-densitydpi这个私有属性，表示目标设备的密度等级，作用是决定css中的1px代表多少物理像素 target-densitypdi：可以是数值，或者字符串:”high-dpi”,”medium-dpi”,”low-dpi”,”device-dpi”中的一个。 当配置`target-densitypid=device-dpi”时，css中的1px会等于物理像素的1px。 因为只有android支持且Android决定放弃这个属性了，所以我们不要使用。 initial-scale &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1.0&quot;&gt; 也可以把当前viewport变为ideal viewport。设置为1说明梅索放，但却达到了ideal viewport的效果。说明缩放是相对于ideal viewport来进行缩放的 如果width和initial-scale = 1同时出现，并且冲突怎么办？ 假如width=400px，那么浏览器会取他们中的较大值。 最后，总结一下，要把当前的viewport宽度设为ideal viewport的宽度，既可以设置width=device-width，也可以设置 initial-scale=1，但这两者各有一个小缺陷，就是iphone、ipad以及IE 会横竖屏不分，通通以竖屏的ideal viewport宽度为准。所以，最完美的写法应该是，两者都写上去，这样就 initial-scale=1 解决了 iphone、ipad的毛病，width=device-width则解决了IE的毛病：&lt;meta name = &quot;viewport&quot; content=&quot;width=device-width,initial-scale=1.0, maximum-scale=1.0, user-scalable=0&quot;/&gt; 流式布局主要是这个属性：display:flex写在移动端：display:-webkit-flex; 这是CSS3中的属性。 控制子元素横着排列还是竖着排列 flex-direction:row|row-reverse|column|column-reverse; 换行，wrap强制换行,nowrap强制不换行，wrap-reverse flex-wrap:nowrap|wrap|wrap-reverse; justify-content:flex-start|flex-end|center|space-between|space-around; align-items:flex-start|flex-end|center|baseline|stretch; align-self:auto|flex-start|flex-end|center|baseline|stretch;"},{"title":"【LeetCode】 142.环形链表2","date":"2019-09-28T12:36:12.000Z","path":"2019/09/28/leetcode-cycle-list-2/","text":"142.环形链表2给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 说明：不允许修改给定的链表。 示例 1：输入：head = [3,2,0,-4], pos = 1输出：tail connects to node index 1解释：链表中有一个环，其尾部连接到第二个节点。 题解分析141题是判断链表是否有环，142题是判断入环的第一个节点。 判断链表有环的方法是：快慢指针。 那么对于入环的第一个节点怎么去找？ 假设从链表头部走到环的入口需要F步 假设两个指针相遇的时候，慢指针走了 a 步，快指针走了 a + b + a 步 由快慢的定义，我们知道f = 2s，因为快指针每次都走2步，慢指针走1步，快指针走了慢指针的两倍路程。 那么可以推出：2 (F + a) = F + a + b + aF = b 说明，入环之前这一段路程和b相同，那就意味着，我们用一个指针从头走F步，另一个指针从第一次相遇处走F步，当他们再次相遇时，说明这个点就是入口。 步骤1.快慢指针找到第一次相遇的位置2.快指针fast重新从head开始,slow继续从第一次相遇处，fast 和 slow 每次都只走一步，相遇时的位置就是环开始的位置。返回slow或者fast即可。 编码123456789101112131415161718public ListNode detectCycle(ListNode head) &#123; if(head==null || head.next == null) return null; ListNode fast = head; ListNode slow = head; while(true)&#123; if(fast == null || fast.next == null) return null; fast = fast.next.next; slow = slow.next; if(fast == slow) break; &#125; fast = head; while(fast != slow)&#123; fast = fast.next; slow = slow.next; &#125; return fast; &#125;"},{"title":"【MySQL】MySQL总结","date":"2019-09-28T10:52:22.000Z","path":"2019/09/28/Mysql-Summary/","text":"事务1.什么是事务？ 满足ACID特性的一组操作。 2.自动提交 MySQL中的事务都是自动提交的，如果不显式地使用START TRANSACTION来开启事务，那么每一个查询都会被当作一个事务。 3.事务的特性(ACID) 原子性：Atomicity，事务是一个不可再分的工作单位，事务中的操作要么都执行，要么都不执行。 一致性：Consistency，事务总是从一个一致状态转移到另一个一致状态。并发执行的事务，其结果和串行执行的结果一致（正确），那么就说这个并发事务符合一致性。 隔离性：Isolation，事务所做的修改在提交之前，对其他事务是不可见的。 持久性：Durability，事务所做的修改一旦提交，就会永久的保存在数据库中。 4.并发事务存在的问题 1.丢失更新：两个事务读取同一事务后，先后进行修改（或回滚）导致第一个提交的事务数据丢失。 2.脏读：事务读取到其他事务未提交的数据 3.不可重复读：事务A读取数据x但还没提交，事务B修改了X或删除了X并提交，事务A再次读取数据时，两次得到的结果不一致。针对update 和 delete。 4.幻读：事务A读取x = 100的数据得到了10条结果，事务B增加了一条x = 100的数据并提交，事务A再次读取时会得到11条结果，导致前后不一致。针对insert。 5.事务的隔离级别 1.未提交读 Read-Uncommited：事务中的修改即使没有提交也会被其他事务可见。(导致上述1、2、3问题) 2.提交读 Read-Commited ： 事务只会看到已提交事务做出的修改。（导致2，3） 3.可重复读 Repeatable-Read : 一个事务多次读取相同内容得到结果一致，也就是说，一个事务不会读取另一个事务修改但未提交的数据。（能解决2，不能解决3）。是MySQL默认的隔离级别。 MySQL通过MVCC（快照读） + next-key locking (当前读）解决了幻读的问题。 4.可串行化 Serializable : 强制事务串行执行。最高级别。 6.为什么MySQL默认隔离级别是RR？ MySQL在5.0之前binlog只有statement一种格式，这个格式在RC下，会出现主从不一致的情况，因此默认隔离级别是RR. 1.什么是binlog binlog记录所有数据库表结构变更（例如CREATE、ALTER TABLE…）以及表数据修改（INSERT、UPDATE、DELETE…）的二进制日志。binlog不会记录SELECT和SHOW这类操作，因为这类操作对数据本身并没有修改，但你可以通过查询通用日志来查看MySQL执行过的所有语句。 2.binlog的几种格式 statement：记录的是修改SQL语句row：记录每行实际数据变更mixed：上面两种的混合。 当binlog为statement格式，数据库隔离级别在RC情况下，在master节点先delete后insert，binlog里记录的是先insert后delete(按提交顺序,session2先提交)，slave同步的是binlog，因此从机执行的顺序和主机不一致！就会出现主从不一致。 3.如何解决？ 1)设置为RR,加入间隙锁，session2插入时会阻塞。 2)使用binlog的row格式，这个格式5.1才有。 因此MySQL的默认隔离级别是RR。 7.那既然说InnoDB通过MVCC解决了幻读，什么是MVCC？ 多版本并发控制。 通过为每一行数据保存两个隐藏的字段（创建时间，删除时间），使得大部分读操作都不需要加锁。 实际上保存的是版本号，每个事物开始都有一个版本号，递增。 SELECT InnoDB会根据以下两个条件检查每一行 行的创建版本号早于当前版本号，这样保证事物读取的数据是在事务开始之前就存在的，或者事务自己创建的。行的删除版本号大于当前版本号，或者未定义，这样保证事物发生时该行还没有被删除（修改）。 INSERT 为创建的行添加当前事务的版本号到行的创建版本号。 DELETE 为被删除的行添加当前事务的版本号到删除版本号。 UPDATE 为旧的数据添加当前版本号到删除版本号。 创建一行新纪录，保存当前版本号到创建版本号。 8.next-key locking 算法 next-key locking是一个行锁的算法，除他以外InnoDB还有两个行锁的算法，他们分别是： 1.Record Lock：锁直接加在索引记录上，锁住的是key 2.Gap Lock:间隙锁，锁定索引记录的间隙（不包括记录本身），保证间隙不变，它是针对隔离级别为RR及以上的事务来说的。 3.Next-key locking：上面这两个家伙组合起来就是next-key locking。锁定间隙且锁定记录本身。 之前说，InnoDB通过MVCC解决了幻读的问题，但他是在快照读的层面实现的（简单的select），next-key locking是在当前读层面来实现的，而且默认情况下是针对RR级别的。 在使用next-key locking的时候，当InnoDB扫描索引记录时，会先对索引加上行锁（record lock）然后在索引两边的间隙加上间隙锁（gap lock），这样就不能在间隙的位置进行修改或插入，从而防止幻读的发生。 9.锁的降级 当索引含有唯一属性时，Next-key Locking 会自动降级为Record Lock 用来减少锁定的范围，加大并发的处理速度。但是此种情况只存在于查询【所有的唯一索引列】。如果，唯一索引由多个列组成，而查询是查找多个唯一索引列中的其中一个，那么这种查询由于联合索引的特性，查询是一个范围查询，而不是点查询，所以不会降级处理。 索引1.设计索引的原则 1.最适合的索引列出现在where子句中，或者连接子句指定的列。 2.使用唯一索引。 3.使用短索引。对于varchar类型的字段，使用前缀索引效率会高。可以指定一个前缀长度，在这个长度内是唯一的。 4.利用最左前缀。 5.不要过度索引，不要建立太多的索引。 6.考虑列上进行比较的类型，索引可以用于&lt;、&lt;=、=、&gt;=、&gt;和between运算，模式具有一个直接前缀的时候可以用于like运算。 7.索引列的顺序非常重要！选择性最高的列放在最前面，选择性：不重复的索引值/总的记录数 8.索引列不能是表达式的一部分，也不能是函数的参数。 9.覆盖索引，可以充分利用二级索引中的主键值来覆盖查询 10.索引和锁： 索引可以让查询锁定更少的行 即使使用索引也有可能锁住不需要的数据，例如索引为 查询为where id &lt; 5 and id&lt;&gt;1 for update，虽然不需要id=1的数据，但mysql执行计划是索引范围扫描，因此&lt;5的都锁住了。 InnoDB在二级索引上使用共享锁，访问主键需要排他锁，并且使用select for update比lock in share mode或非锁定查询要慢 2.什么情况不会用到索引？ 1.当MySQL估计全表比索引快时，不用索引。 2.当跳过索引中的列。 当索引为 查询where a = 10 and c= 100，就只会用到索引的第一列。 3.违反最左前缀。当索引为时，where b = 100 或 where b = 100 and c = 100 或where c = 100都不会用到索引。 4.查询中有某个列的范围查询，则这个列的右边都无法用到索引。当索引为时，查询where a = 10 and b like “bbv%” and c = 98，这个查询只用到索引的前两列。 5.前导的模糊查询不会用到索引，where a like “%as”不会，但where a like “as%”可以用。 6.使用聚合函数索引会失效。 7.列类型是字符串，查询条件未加引号。 8.在查询条件中使用OR会失效，可以为每个or的字段都加索引（不太好吧） 3.InnoDB的聚簇索引是什么？优点和缺点是什么？ 更多：https://www.jianshu.com/p/fa8192853184 聚簇索引是InnoDB存储数据的方式，在同一个结构中保存了索引和数据行，数据存放在索引叶子页中。 InnoDB通过主键聚集数据。 优点： 1.可以把相关的数据保存在一起。 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问，不必访问磁盘。这样主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回了，如果按照主键Id来组织数据，获得数据更快。 2.数据访问更快，因为索引和数据是存放在一起的。（只需要一次查询） 3.使用覆盖索引扫描可以直接使用主键值。 缺点： 1.插入速度严重依赖于查询顺序，使用自增的主键是最快的插入方式。 2.更新代价很高，会强制被更新的行移动到新的位置。 3.导致页分裂。当主键值要求必须在已满（16K）的页中插入一条数据，会导致页分裂，页分裂会占用更多的空间。 4.二级索引可能会比想象的更大，因为二级索引中包含了主键，如果主键很大，那么其他索引都会很大。 5.二级索引的查找需要两次索引查找。自适应哈希索引能减少这样的重复工作。 自适应哈希索引：当InnoDB注意到某些索引值被频繁使用时，他会在内存中基于B+Tree索引之上再创建一个哈希索引。这是一个自动的内部行为，用户无法控制或配置，如果有必要也可以关闭这个功能。 4.二级索引的好处？ 二级索引存储了主键的值，虽然会消耗更多的空间，但是这样会减少出现页分裂时二级索引的维护，InnoDB在移动行时无需更新二级索引的id。 5.覆盖索引 优秀的索引应该考虑到整个查询而不单单只是where部分。如果一个索引能覆盖所有需要查询的字段（包括select部分的）那么就称之为覆盖索引。覆盖索引能极大提高性能，索引的条目通常远小于数据的数量，如果只需要读取索引那就能极大减少访问量。 由于聚簇索引，覆盖索引对InnoDB的表特别有用。InnoDB的二级索引在叶子结点保存了主键的值，如果二级索引能覆盖查询则可以避免对主键索引的二次访问。 explain的Extra列可以看到Using index，说明这个查询是索引覆盖查询。 6.哈希索引 InnoDB 引擎有一个特殊的功能叫 “自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 哈希索引能以 O(1) 时间进行查找，但是失去了有序性，它具有以下限制： 1.无法用于排序与分组； 2.只支持精确查找，无法用于部分查找和范围查找 7.索引的优点&gt; 创建唯一性索引，保证数据库表中每一行数据的唯一性 大大加快数据的检索速度，这是创建索引的最主要的原因 加速数据库表之间的连接，特别是在实现数据的参考完整性方面特别有意义 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间 通过使用索引，可以在查询中使用优化隐藏器，提高系统的性能 8.索引的缺点 -创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加-索引需要占用物理空间，除了数据表占用数据空间之外，每一个索引还要占一定的物理空间，如果建立聚簇索引，那么需要的空间就会更大-当对表中的数据进行增加、删除和修改的时候，索引也需要维护，降低数据维护的速度 9.为什么MySQL的索引要使用B+树而不是其它树形结构?比如B树？ 因为B树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少。 指针少的情况下要保存大量数据，只能增加树的高度，导致IO操作变多，查询性能变低； 锁1.快照读和当前读 快照读：简单的select操作属于快照读，不加锁（不包括select … for update,select … lock in share mode) 例如：select * from table where… 当前读：特殊的select，insert/update/delete操作，属于当前读，要加锁。 例如： select … for update select…lock in share mode insert… update… delete… 在这些操作中，都需要加锁，select … lock in share mode加的是共享锁（S），其他都是排他锁（X） 2.共享锁和排他锁 共享锁：又叫做读锁，读锁是共享的，用户可以并发读，但不能修改，也就是说，事务A对数据加上共享锁后，别的事务也可以对数据加共享锁，但不能加排他锁。 select…lock in share mode 在select语句后面加上lock in share mode，MySQL对结果的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。 排他锁：又叫写锁，写锁会阻塞其他的读锁和写锁，也就是当数据被加上排他锁后，再也无法对这个数据加任何锁，直到释放。 select … for update 在select语句后面加上for update时，MySQL会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。 3.意向锁&gt; 意向锁：意向锁是表级锁，其设计目的主要是为了在一个事务中揭示下一行将要被请求锁的类型。 解释之前先说一下，意向锁是InnoDB自动加的，不需要用户干预，而共享锁和排他锁可以用户自己加（for update和lock in share mode） 假设事务A对数据表的某一行加了读锁，那么其他事务就只能读，不能写，也就是不能加排他锁。 此时有一个事务B对表申请一个表锁，如果他申请成功，那么也就意味着事务B对整个表拥有了写的权利，那事务A锁定的那一行不是矛盾了吗？ 数据库要避免冲突，那么就需要检查这个表是否被其他事务加了表锁，要是没加，就还得检查这个表的每一行是不是被锁住，这样就需要遍历整个表，效率不高，所以就有了意向锁。 有了意向锁，事务A在给表的某一行加锁前，需要先申请一个意向共享锁，成功后，再对这一行加行锁。 那么事务B再加锁前就需要先判断表上是否加了意向共享锁，如果有，就说明有些行被加了共享锁，这时事务B就阻塞了。 所以这么看来，意向锁的意思就可以这么表达：告诉事务这个表上有其他事务加了共享锁或排他锁。 加了意向共享锁（IS）的表，还可以被其他事务加IS，但被共享锁锁定的那些记录是不能被修改的。 加了意向排他锁（IX）的表，就不能被加任何共享锁和排他锁了。 意向锁是个表级的锁，共享锁和排他锁可以是行级也可以是表级，IX和IS只会和表级的X和S冲突，不会和行级的冲突。 意向锁实现了表锁和行锁的共存（举的那个例子就说明行锁和表锁共存时会有冲突） 并存的概念是指数据库同时支持表、行锁，而不是任何情况都支持一个表中同时有一个事务A持有行锁、又有一个事务B持有表锁，因为表一旦被上了一个表级的写锁，肯定不能再上一个行级的锁。 4.表锁和行锁 刚说S和X锁的时候我们提到了表锁和行锁，现在来解释一下： 表锁（table lock）：是MySQL中最基本的锁策略，且开销最小，他会锁定整张表 行锁（row lock）：给某一行加锁，行锁可以最大程度支持并发处理（同时带来最大的锁开销），行锁只在存储引擎层实现，而MySQL服务器层没有实现。 表锁和行锁是从锁的粒度来分的，X和S是从类型来分的，所以我们之前说，可以有行级的S和X锁，也可以有表级的S和X锁。 存储引擎1.InnoDB InnoDB是MySQL默认事务型引擎。 InnoDB采用MVCC来支持高并发，并且实现了四个标准的隔离级别。默认级别是RR，并通过间隙锁（next-key locking）策略防止幻读的出现。 InnoDB是基于聚簇索引建立的。 调优1.慢查询 查询性能低下的最基本原因是访问的数据太多。 分析步骤： 1.确认应用程序是否在检索大量不需要的数据。 2.确认MySQL服务器层时候在分析大量超过需要的行。 典型&lt;检索不需要数据&gt;情况：1.SELECT * 每次看到这个的时候都需要想想，是不是真的需要表里全部的数据。 我们那个查找下载论文学生的查询，就SELECT * FROM STUDENT。 2.重复查询相同的数据 有的查询总是返回相同的结果，可以考虑用缓存。 2.向MySQL发送一个请求，MySQL到底做了什么？ 1.客户端发送一条查询给服务器 2.服务器先检查查询缓存，如果命中缓存，则立刻返回缓存中的结果。否则进入下一阶段。 解析SQL语句之前，如果查询缓存开启，则会优先检查这个缓存，是通过对大小写敏感的哈希查找实现的。如果命中，则在返回结果前会再检查一下用户权限，如果权限没有问题，则会跳过后面的阶段直接返回结果。 3.服务器进行SQL解析、预处理，再由查询优化器生成对应的执行计划。 SQL解析：通过关键字进行SQL语句解析，生成一颗“解析树”。预处理器：根据MySQL规则进一步检查解析树是否合法。（表和列是否存在，名字和别名是否有歧义）查询优化器：一条查询可以有很多种执行方式，优化器就是找到最好的执行计划。 4.MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询。 5.返回结果导到客户端。 3.EXPLAIN 分析查询EXPLAIN可以对SELECT查询进行分析，所有字段如下： id：SELECT查询的标识符，每个SELECT都会分配一个唯一的标识符 select_type：SELECT查询的类型 table：查询的是哪个表 partitions：匹配的分区 type：mysql找到需要的数据行的方式 possible_keys：可能用到的索引 key：用到的索引 key_len：用到的索引的长度。 ref：哪个字段或常数和key一起使用。 rows：查询一共扫描了多少行，估计值。 filtered：此查询条件所过滤数据的百分比。 Extra：额外的信息。-select_type - 查询类型常用的取值有： SIMPLE, 表示此查询不包含 UNION 查询或子查询 （最常见） PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. type - MySQL找到需要数据行的方式常取的值： const：针对主键或唯一索引的等值查询扫描, 最多只返回一行数据,const 查询速度非常快, 因为它仅仅读取一次即可。 eq_ref：此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 = , 查询效率较高。 ref：此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询. range：表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中。 index：表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据。index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index。 ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. ALL &lt; index &lt; range ~ &gt;index_merge &lt; ref &lt; eq_ref &lt; const &lt; system possible_keys - 可能用到的索引 表示能用的索引，并不代表真的用了，key表示用到的索引。 key - 使用的索引 查询使用的索引 key_len 使用索引的长度 表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. rows - 查询需要扫描的行数 MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. Extra - 额外信息Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. 连接1.连接的种类1.外连接左连接 left join (left outer join)结果包括左表的所有行，如果右表有无法匹配的行则显示null。右连接 right join (right outer join)包括右表所有的行，如果左表又无法匹配的行，则显示null。 全连接 full join (full outer join)左表和右表其中一个表中存在匹配，则返回行。 2.内连接：内联接是用比较运算符比较要连接列的值的联接join 或 inner join 其他1.数据库中的范式 1NF：属性不可分 2NF：属性完全依赖于主键 [消除部分子函数依赖] 3NF：属性不依赖于其它非主属性 [消除传递依赖] 2.char 和 varchar的区别 char是定长的，varchar是变长的。 varchar在修改的时候可能会比原来长，原页面无法容纳时可能会导致分页。 varchar会保留字符串末尾的空格，char不保留。 3.分页查询怎么写？ select * from employee limit 3, 7; // 返回4-11行 select * from employee limit 3,1; // 返回第4行 SELECT * FROM message ORDER BY id DESC LIMIT 10000, 20 每次都要扫描10020行，然后只要20条记录，性能是不好的。 怎么写效率更好呢？ 如果只需要上一页和下一页，可以这么写： select * from message where id &gt; 10020 order by id desc limit 20;//下一页select * from message where id &lt; 10000 order by id desc limit 20;//上一页 这样就只需要扫描20行。 上一页 1 2 3 4 5 6 7 8 9 下一页 怎么实现呢？ 还是按照SELECT * FROM message ORDER BY id DESC，按id降序分页，每页20条 假设当前是第10页，当前页条目id最大的是2519，最小的是2500; 当前第10页的sql可以这么写: SELECT * FROM tb_goods_info WHERE auto_id &gt;=2500 ORDER BY auto_id ASC LIMIT 0,20 第9页： SELECT * FROM tb_goods_info WHERE auto_id &lt;2500 ORDER BY auto_id desc LIMIT 0,20 第8页： SELECT * FROM tb_goods_info WHERE auto_id &lt;2500 ORDER BY auto_id desc LIMIT 20,20 第11页： SELECT * FROM tb_goods_info WHERE auto_id &gt;2519 ORDER BY auto_id asc LIMIT 0,20 原理还是一样，记录住当前页id的最大值和最小值，计算跳转页面和当前页相对偏移，由于页面相近，这个偏移量不会很大，这样的话m值相对较小，大大减少扫描的行数。其实传统的limit m,n，相对的偏移一直是第一页，这样的话越翻到后面，效率越差，而上面给出的方法就没有这样的问题。 注意SQL语句里面的ASC和DESC，如果是ASC取出来的结果，显示的时候记得倒置一下。 4.数据库字段类型为什么建议尽量避免NULL 高性能MySQL中明确提到“尽量避免NULL” 原因：NOT IN、!= 等负向条件查询在有 NULL 值的情况下返回非空行的结果&gt;集。Count(*)会统计包括NULL的行，count(colName)不会统计此列为null的行NULL 列需要更多的存储空间，需要一个额外的字节作为判断是否为 NULL 的标志位。"},{"title":"【日记】不知道纪念什么的纪念日记","date":"2019-09-28T08:12:59.000Z","path":"2019/09/28/four/","text":"1当事情发生的时候，没有人会问你：“嗨，准备好了吗？” 而是就那样，发生了。 对工作是这样，对学业是这样，对感情也是这样。 2时常反省自己，做一个温柔的人，做一个认真的人，做一个努力生活的人。 虽然生活最近对我不太好，但我还是愿意热烈的爱着它。 我喜欢一个人走路，喜欢一个人吃饭。 我想谈一场简简单单的恋爱。 这些都很难吗？ 不知道。 尽人事，听天命吧。 3他日重逢，要等来生。 4《我用什么才能留住你》博尔赫斯 我用什么才能留住你？我给你瘦落的街道、绝望的落日、荒郊的月亮。我给你一个久久地望着孤月的人的悲哀。 我给你我已死去的祖辈，后人们用大理石祭奠的先魂：我父亲的父亲，阵亡于布宜诺斯艾利斯的边境，两颗子弹射穿了他的胸膛，死的时候蓄着胡子，尸体被士兵们用牛皮裹起； 我母亲的祖父——那年才二十四岁——在秘鲁率领三百人冲锋，如今都成了消失的马背上的亡魂。 我给你我的书中所能蕴含的一切悟力，以及我生活中所能有的男子气概和幽默。我给你一个从未有过信仰的人的忠诚。 我给你我设法保全的我自己的核心——不营字造句，不和梦交易，不被时间、欢乐和逆境触动的核心。 我给你早在你出生前多年的一个傍晚看到的一朵黄玫瑰的记忆。我给你关于你生命的诠释，关于你自己的理论，你的真实而惊人的存在。 我给你我的寂寞、我的黑暗、我心的饥渴；我试图用困惑、危险、失败来打动你。 5《如果我不曾见过太阳》狄金森 我本可忍受黑暗 如果我不曾见过太阳 然而阳光却把我的荒凉 照耀的更加荒凉 当你拥有过幸福，那么任何一点点的苦难都会变得分外强烈。"},{"title":"Hello World","date":"2019-09-28T02:09:06.924Z","path":"2019/09/28/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]