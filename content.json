[{"title":"【论文笔记】Relation Classification via Convolutional Deep Neural Network","date":"2019-10-11T14:24:24.000Z","path":"2019/10/11/Zeng-2014-note/","text":"CNN：https://blog.csdn.net/kane7csdn/article/details/83617086https://blog.csdn.net/liangchunjiang/article/details/79030681论文模型调试：https://blog.csdn.net/u014586129/article/details/90142875https://www.jianshu.com/p/28fb2aed876f https://blog.csdn.net/manmanxiaowugun/article/details/81157105https://www.sohu.com/a/165856071_465975https://blog.csdn.net/u013414502/article/details/82110202#1%E5%A6%82%E4%BD%95%E6%8F%92%E5%85%A5%E5%85%AC%E5%BC%8Fhttps://www.cnblogs.com/Luv-GEM/p/11598294.html"},{"title":"【论文笔记】Multi-instance Multi-label Learning for Relation Extraction","date":"2019-10-10T08:42:51.000Z","path":"2019/10/10/MIMR-2012-note/","text":"摘要提出了一种用于关系抽取的多实例-多标签学习方法，该方法使用具有隐变量的图模型共同对文本中一对实体的所有实例及其所有标签进行建模。该模型在两个困难领域具有竞争优势。 引言远程监督在关系抽取中，主要解决的是训练数据标注的问题，可以通过将文本与知识库对齐来自动生成训练数据。 在本文中，我们关注关系抽取中的远程监督，这是信息抽取的一个子问题，用于解决两个命名实体间关系的抽取。上面的图1展示了带有两个标签的关系抽取域的简单示例。 远程监督带来两个建模挑战：（1）通过这种方式获取的一些训练实例是无效的，例如图1，最后一句对于元组的任何已知标签都不是正确的。此类误报的百分比可能很高，Riedel et al.(2010)报告了Freebase中的关系与纽约时报文章语料库对气候的误报率高达31%。（2）一组相同的实体可能会有多个标签，并且不清楚哪个标签是由给定元组的任何文本描述实例化的。例如，图1中元组(Barack Obama, United States)有两个有效标签：BornIn和EmployedBy，每个（潜在的）被实例化成不同的句子。在Riedel语料库中，训练分区中7.5%的实体元组具有多个标签。 多实例多标签学习概述，传统监督学习中，一个对象只有一个实例和一个标签，对于关系抽取，对象是一个包含两个命名实体的元组，文本中对于这个元组的每个描述都会生成一个不同的实例。 图2中总结了这个多实例多标签（MIML)学习问题.在本文中，我们提出一种新颖的图模型，称为MIML-RE，该模型针对MIML学习进行关系抽取。我们的工作做出以下贡献: (1)MIML-RE是第一个将多个实例（通过对分配给实例的潜在标签建模）和多标签（通过提供一种简单的方法来捕获标签之间的依赖关系）联合建模的RE方法。例如，我们的模型能学习某些标签倾向与联合生成，而其他标签则不能联合联合分配给同一元组。(2)我们证明MIML-RE在两个困难领域中有竞争性。"},{"title":"【leetCode】27.移动元素","date":"2019-10-10T02:25:11.000Z","path":"2019/10/10/leetcode-27/","text":"27.移动元素给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。 不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 示例 1: 给定 nums = [3,2,2,3], val = 3, 函数应该返回新的长度 2, 并且 nums 中的前两个元素均为 2。 你不需要考虑数组中超出新长度后面的元素。 示例 2: 给定 nums = [0,1,2,2,3,0,4,2], val = 2, 函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。 注意这五个元素可为任意顺序。 你不需要考虑数组中超出新长度后面的元素。 题解分析解法1 - 双指针交换元素第一个想法是，用两个指针分别指向等于val的元素和其他元素，然后交换这两个位置的元素。 有一点类似快排的partition。123456789101112131415161718192021222324252627282930313233343536 public int removeElement(int[] nums, int val) &#123; if(nums == null || nums.length == 0) return 0; int p = 0;//val的指针 int q = 0;//非val的指针 int i = 0; //寻找数组中第一个等于val元素的位置，用p记录。 while(i &lt; nums.length)&#123; if(nums[i] == val)&#123; p = i; break; &#125;else i++; &#125; // 当i 等于数组长度时，说明没找到第一个等于val的元素，也就意味着数组中没有等于val的元素 // 那么不需要remove，直接return i，即数组长度即可。 if(i == nums.length)&#123; return i; &#125; //q从p的下一个位置开始进行遍历。 q = p + 1; //比较p和q位置的元素，将不等于val的q位置元素与p位置元素交换。 while(p &lt; nums.length || q &lt; nums.length)&#123; while(q &lt; nums.length &amp;&amp; nums[q] == val) q++; if(q == nums.length) break; swap(nums,p,q); while(nums[p] != val) p++; &#125; return p;&#125;private void swap(int[] nums, int i, int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp;&#125; 复杂度分析：&gt; 时间复杂度：O(n)空间复杂度：O(1) 解法2 - 双指针复制通过上面的方法我们进行了元素的交换，实际上我们不需要等于val的元素，也就意味我们并不需要交换，只需要将非val元素复制到val元素的位置上即可。 12345678910111213 // 解法2：双指针，当j指向的元素等于val时，就递增j（j++） // 当j指向的元素不等于val时，将j位置的元素复制到i位置，i++，j++public int removeElementV2(int[] nums, int val)&#123; if(nums == null || nums.length == 0) return 0; int i = 0; for(int j = 0; j &lt; nums.length; j++)&#123; if(nums[j] != val)&#123; nums[i] = nums[j]; i++; &#125; &#125; return i; &#125; 复杂度分析：&gt; 时间复杂度：O(n)假设数组总共有 n个元素，i和 j 至少遍历 2n 步。空间复杂度：O(1) 解法3 - 双指针复制，少量复制上面的方法我们发现，当数组为[1,2,3,4]，且val=4时，会将1，2，3都原地复制，这是没必要的。因此我们可以让双指针位于数组的首部和尾部，当前指针指向的元素等于val时，就将后指针指向的元素复制到前指针的位置，后指针前移。 如果后指针指向的元素也等于val怎么办？实际上不用担心，交换之后我们还是会对前指针的元素进行判断的。即，复制之后我们不会后移前指针，只有当前指针指向的元素不等于val才会后移。 123456789101112131415//解法3：双指针，头尾交换public int removeElementV3(int[] nums,int val)&#123; if(nums == null || nums.length == 0) return 0; int i = 0; int n = nums.length; while(i &lt; n)&#123; if(nums[i] == val)&#123; nums[i] = nums[n - 1]; n--; &#125;else&#123; i++; &#125; &#125; return n;&#125; 复杂度分析：&gt; 时间复杂度：O(n)空间复杂度：O(1) 在这个方法中，赋值操作的次数等于要删除的元素的数量。因此，如果要移除的元素很少，效率会更高。"},{"title":"【论文笔记】Knowledge-Based Weak Supervision for Information Extraction","date":"2019-10-09T14:36:31.000Z","path":"2019/10/09/hoffmann-2011-note/","text":"摘要基于知识的弱监督（远程监督），使用结构化的数据启发性地标记训练语料库，最近，研究人员开发了多实例学习算法来对抗可能来自启发式标签的嘈杂训练数据，但是他们的模型假定关系是不相交的，例如，他们无法提取成对的Founded(Jobs, Apple) 和 CEO-of（Jobs,Apple). 本文提出一种具有重叠关系的多实例学习的新方法，该方法将句子级的抽取模型和简单的语料库级的组件结合，以汇总单个情况。我们使用Freebase的弱监督，将模型应用于学习纽约时报文本的抽取器上。实验表明，该方法可以快速运行，并且在汇总和句子级别都有令人惊讶的准确率。 引言弱监督（or 远程监督）方法通过启发式地将数据库内容与相应文本进行匹配来创建自己的训练数据，例如假设r(e1,e2） = Founded(Jobs, Apple)是数据库中的3元组，而s = &quot;Steve Jobs founded Apple, Inc.&quot;是包含e1=Jobs 和 e2=Apple的同义词的句子,则s可能是r(e1,e2)事实的自然语言表达，并可能是一个有用的训练样例。 当文本语料库和数据库内容紧密对齐时，弱监督能很好地工作，Riedel et al.(2010)观察到，当该方法被广泛应用时（例如，将Freebase记录与纽约时报文章相匹配),会产生大量的噪声数据和较差的抽取性能。为了解决这个问题，为了解决这个问题，他们将弱监督作为多实例学习的一种形式，假设包含e1和e2的句子中至少有一个表示r（e1，e2），并且他们的方法在提取性能上有了实质性的改进。 但是，Riedel等人的模型（如先前系统的模型（Mintz等人，2009））假设关系不重叠，即不能存在两个事实r(e1,e2)和q(e1,e2)，对任何一对实体都正确。不幸的是，这个假设经常被违反，例如， Founded(Jobs, Apple) 和 CEO-of（Jobs,Apple)都是正确的，事实上，Freebase与纽约时报2007年语料库匹配的句子中，有18.3%存在重叠关系。 本文介绍了MULTIR，这是一种新型的弱监督模型，有以下作用： MULTIR引入了多实例学习的概率图形模型，该模型可以处理重叠的关系。 MULTIR还可以产生准确的句子级别预测，解码单个句子以及进行语料库级别提取。 MULTIR在计算上易于处理。推理归结为加权集覆盖，为此它使用贪婪近似，并使用最坏情况下的运行时间O(|R| · |S|)，其中R是可能的关系的集合，S是任何实体对的最大句子集合。实际上，MULTIR运行非常快。 我们提出的实验表明MULTIR优于Riedel等人的重新实现。额外的实验可以表征MULTIR的性能。 来自数据库的弱监督给定语料库，我们从中抽取有关实体的事实，例如公司Apple，城市Boston。基本的事实（或者说是关系实例），是一个表达式 $r(e)$,其中r是关系名，$e = e_1,…,e_n$是实体列表。 entity mention是表示一个实体的文本标记的连续序列。在本文中，我们假设有一个预言家能够识别语料库中所有entity mentions，但预言家并没有规范或者消除这些mentions的歧义，我们用$e_i\\in E$来表示实体及其名称。 relation mention是一连串的文本（包括一个或多个entity mentions）,它说明一些基本事实$r(e)$是真实的。例如，”Steve Ballmer, CEO of Microsoft, spoke recently at CES.”对于CEO-of(Steve Ballmer, Microsoft)包含3个entity mention和一个relation mention，在本文中，我们将关注二元关系，此外，我们假设两个实体mention都在单个句子中显示为名词短语。 聚合抽取的任务需要两个输入。$\\sum$,一组由语料组成的句子和一个抽取模型；作为输出，它应该产生一组基础事实，$I$,以便每个事实$r(e)\\in I$都在语料库中有所表达。 句法抽取采用相同的输入，并产生同样的$I$,但除此之外他还会产生函数$\\Gamma:I \\to P(\\sum)$,该函数为每个$r(e)\\in I$标识$\\sum$中包含的mention（描述为$r(e)$）的句子集合。通常语料库级别的抽取问题比较容易，因为它只需要进行聚合预测即可，也许使用语料库范围的统计信息即可。相反，句子级别的抽取必须用表达事实的每个句子来证明每次抽取的合理性。 总结基于知识的弱监督学习问题由(1)$\\sum$,训练语料库(2)$ E $, 一组在该语料库中提到的实体(3)$ R $, 一组关系名称(4)$ \\Delta $, R中一组关系的基本事实作为输入。 抽取模型作为输出。 重叠关系建模定义了一个无向图模型，允许对汇总（语料库级）和句子级抽取决策进行联合推理。图(a)显示了模型。 随机变量每对实体$ e = (e_1,e_2) \\in E \\times E$都有一个连接的组件，该组件为该对实体的所有提取决策建模。每个关系名称$ r \\in R $都有一个布尔输出变量 $ Y^r $ ，它表示$r(e)$是否为真。包括这组二值随机变量使我们的模型能够提取重叠关系。 令 $ S_(e_1,e_2) \\subset \\sum $为包含e1、e2 mention的句子集合，对每个句子$ x_i \\in S_(e_1,e_2)$，都存在一个隐变量$Z_i$，该变量在关系名称$r \\in R $范围内。仅当$x_i$表达基本事实$r(e)$时才应为$Z_i$分配值$r \\in R$，从而对句子级抽取进行建模。 算法 总结针对wrong label问题，使用弱监督方法提取重叠关系。"},{"title":"【论文笔记】Distant supervision for relation extraction without labeled data","date":"2019-10-09T07:25:06.000Z","path":"2019/10/09/mintz-2009-note/","text":"关系抽取关系抽取就是抽取一个句子中实体对之间的关系。要训练一个关系抽取器，给它一个句子两个实体，首先它需要知道为这两个实体之间的关系打什么标签，模型无法自己为关系取名，因此就需要人工标注这两个实体之间的关系是什么。当模型训练好之后，再遇到这样的实体对，就会知道是这个关系并把它抽取出来。 但问题是，人工标注是耗时耗力的一件事，而且数量实在有限，数据规模对模型训练又有影响，因此本篇论文的作者mintz首次提出了不依赖人工标注的关系抽取，也就是将远程监督应用到关系抽取上。 摘要对于像ACE这样的任务，关系抽取的现代模型是基于关系的有监督学习，使用小型人工标注的语料库。 本文提出不依赖人工标注，将远程监督应用在关系抽取中。 实验使用Freebase数据集（大型语义数据集，有数千个关系）提供远程监督。对于出现在Freebase关系中的每一对实体，我们在大型未标注语料库中寻找包含这些实体的所有句子，并且提取文本特征以训练关系分类器。 算法结合了监督IE（在概率分类器中结合400,000个噪声模式特征）和无监督IE（从任何域的大型语料库中提取大量关系)的优点。模型能够以67.6%的精度提取102个关系中的10000个实例。 论文还分析了feature performance，表明语法分析特征对于表达中含糊不清或者此法遥远的关系特别有用。 引言在有监督学习中，语料库中的句子首先被标记为实体的存在以及他们之间的关系。例如，NIST ACE RDC2003和2004语料库，包含超过1000个文档，其中实体对由5-7个主要关系类型以及23-24个子类型，共计16771个关系实例。ACE系统提取各种各样的词汇、句法和语义特征，并使用监督分类器来标记测试集句子中给定的一对实体之间的关系。 然而有监督的关系抽取也存在问题 人工标注代价昂贵，因此数量有限 关系被标注在特定语料库上，因此产生的分类器往往偏向于该文本域。 另一种方法是纯粹的无监督信息抽取，在大量文本的实体之间提取单词串，聚类并简化这些单词串以产生关系串。**无监督的方法可以使用非常大的数据，并且抽取非常大量的关系，但是产生的关系可能不容易映射到特定知识库所需要的关系。 第三种方法就是半监督学习，使用Bootstrapping进行关系抽取。对于要抽取的关系，该方法首先手工设定若干种子实例，然后迭代地从数据中抽取关系对应的关系模板和更多的实例。这种方法产生的模板往往是低精度且语义漂移的。 语义漂移：在迭代过程中会产生一些与种子不相关的实例，然后这些不相关实例再次进入迭代，频繁产生其他不相关实例。参考文献：Komachi M, Kudo T, Shimbo M, et al. Graph-based Analysis of Semantic Drift in Espresso-like Bootstrapping Algorithms.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2008, Proceedings of the Conference, 25-27 October 2008, Honolulu, Hawaii, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. 2008:1011-1020. 本文提出另一种范例-远程监督，他结合了这些方法的一些优点。本文的算法使用Freebase为关系抽取提供远程监督。Freebase包含900万个实体之间的7300个关系的1.16亿个实例。远程监督的假设是，任何包含一对在Freebase关系中的实体的句子都可以以某种方式表达这种关系。 也就是说，如果两个实体在某个知识图谱中存在关系，那么所有包含这两个实体的句子都已某种方式表述这种关系 由于可能有许多含有给定实体对的句子，因此我们可以提取非常大量的（可能有噪声的）特征，这些特征被合并到逻辑回归分类器中。 我们的范例提供了整合多个句子数据的自然方式，以决定两个实体之间是否存在关系。由于我们的算法可能会使用大量未标记的数据，因此一对实体可能会在测试集中多次出现。对于每一对实体，我们将来自许多不同语句的特征汇总到一个特征向量中，从而使我们能够为分类器提供更多信息，从而产生更准确的标签。 Freebase后文中，使用‘relation’来指代实体之间的有序二元关系，我们将这个关系中的单个有序对称为’relation instance’。 例如，在实体’John Steinbeck’和 ‘United States’之间存在关系’person-nationality’，因此将(John Steinbeck, United States)作为一个instance。 我们使用Freebase的关系和关系实例，Freebase是一个免费的结构化语义数据在线数据库。Freebase中的数据是从各种来源收集的。一个主要来源是维基百科的文本框和其他表格数据。数据还来自NNDB（传记信息），MusicBrainz（音乐），SEC（财务和公司数据）以及直接的维基风格用户编辑。经过对2008年7月链接导出的一些基本处理，将Freebase的数据表示转换为二元关系后，我们有900万个实体之间的7,300个关系，有1.16亿个实例。接下来我们将筛选出无用和不感兴趣的的实体，例如用户配置文件和音乐曲目。 Freebase也包含了许多关系的逆转（bookauthor v.author-book），并将这些关系合并。 过滤和删除除最大关系以外的所有关系，筛选出了94万个实体，102种关系和180万实体对。 关系种类相当于分类的类别，那么有102类；每种关系对应的所有实体对就是样本；从Wikipedia中所有包含某实体对的句子中抽取特征，拼接成这个样本的特征向量。最后训练LR多分类器，用One-vs-Rest，而不是softmax，也就是训练102个LR二分类器——把某种关系视为正类，把其他所有的关系视为负类。补充：在One-vs-Rest策略中，假设有n个类别，那么就会建立n个二项分类器，每个分类器针对其中一个类别和剩余类别进行分类。进行预测时，利用这n个二项分类器进行分类，得到数据属于当前类的概率，选择其中概率最大的一个类别作为最终的预测结果。 架构我们的远程监督方法的核心是使用Freebase构建我们关系的训练集，以及参与这些关系的实体对。 训练阶段 1.在训练阶段，所有实体都使用标记persons，organizations和locations的命名实体标记器(NET)在句子中标识。也就是用命名实体识别工具把语料库中句子的实体识别出来。 2.如果句子包含两个实体，并且这些实体是Freebase关系之一的实例，则从该句子中提取特征并将其添加到关系的特征向量中。训练多类逻辑回归分类器，在训练中，来自不同句子的相同元组（关系，实体1，实体2）的特征相结合，创建更丰富的特征向量。也就是从这些包含这俩实体的句子中提取文本特征，拼接成一个向量，作为这种关系的一个样本的特征向量，用来训练分类器。 远程监督的假设是,如果两个实体之间存在一种关系，那么包含这两个实体的任何句子都可能表达这种关系。因为任何一个单独的句子都可能给出错误提示，所以我们的算法会训练一个多类逻辑回归分类器，为每个噪音特征学习权重。在训练中，来自不同句子的相同元组（关系，实体1，实体2）的特征相结合，创造更丰富的特征向量。 测试阶段 再次使用NET标记实体，这一次，在句子中同时出现的每对实体都被认为是潜在的关系实例，并且当这些实体同时出现时，将在该句子上提取特征并且添加到该实体对的特征向量中。例如，如果一组实体出现在测试集的10个句子中，并且每个句子都有3个特征是从它里边提取的，那么这个实体对将有30个关联特征。测试语料中的每个句子中的每个实体对都通过特征提取运行，回归分类器根据每个实体对出现的所有句子的特征预测一个关系名称。 考虑位置包含（location-contains)关系，假设Freebase中我们有两个这种关系的实例：(Virginia,Richmond)(France,Nantes)当我们遇到句子&#39;Richmond, the capital of Virginia&#39;以及&#39;Henry&#39;s Edict of Nantes helped the Protestants of France&#39;（亨利的南特诏书帮助了法国的新教徒）,我们将从这些句子中提取特征。有的特征可能会非常有用，例如Richmond这个句子中的特征；有的可能不那么有用，例如Nantes这个句子中的特征。在测试中，如果我们能够发现&#39;Vienna, the capital of Austria&#39;这样的句子，它的一个或多个特征将会匹配到Richond句子的特征，从而证明(Austria, Vieena)属于位置包含关系。 请注意，我们体系结构的主要优点之一是它能够将来自许多不同mention（描述）的相同关系的信息结合起来。从以下两个句子中考虑实体对(Steven Spielberg, Saving Private Ryan)，作为film-director关系的证据。 [Steven Spielberg]’s film [Saving Private Ryan] is loosely based on the brothers’ stroy.([史蒂文·斯皮尔伯格]的电影[拯救大兵瑞恩]大致是根据兄弟俩的故事改编的。) Allison co-produced the Academy Award-winning [Saving Private Ryan], directed by [Steven Spielberg](艾莉森联合制作了奥斯卡获奖影片《拯救大兵瑞恩》，导演是史蒂文·斯皮尔伯格) 第一句虽然为film-director 电影导演提供了证据，但却可以作为filmwrite电影编剧或film-producer 电影制片人的证据。第二句话并没有提到拯救大兵瑞恩是个电影，因此可以成为CEO关系的证据（考虑，’Robert Mueller directed the FBI’).孤立的说，这些特征都不是决定性的，但他们组合起来则是决定性的。 特征特征提取中用到的特征包含词法特征、句法特征和命名实体标签特征。 词法特征： 词法特征描述了出现在句子中两个实体之间和周围的特定词语，这些特定词语出现在： 两个实体之间的单词序列 这些单词的词性标签 标志位表示哪个实体出现在前面 实体1左侧的k个单词及其词性标签 实体2右侧的k个单词及其词性标签 $ k \\in {0,1,2} $ 每个词法特征由所有这些组件连接而成。我们为每个$k \\in {0,1,2} $生成一个连接特征。词性标签由宾夕法尼亚州立大学训练的最大熵标记器分配，然后简化为7类：nouns名词，verbs动词，adverbs副词，adjectives形容词，numbers数词，foreign words外来词以及其他。 为了逼近句法特征，我们还测试了词法特征的变化：（1）忽略所有不是动词的词（2）省略所有的功能词结合其他词汇特征，它们对精度有了小的提升，但还不足以证明对计算资源的需求增加。 这里就是获取这些特征，然后把这些特征表示成向量再拼接起来，比如词袋模型，把词语和词性都标识为向量。 句法特征 参照：https://blog.csdn.net/u014422406/article/details/53954530利用依存句法解析器MINIPAR对句子进行解析，然后从解析树中提取实体的依赖路径。 a) 两个实体之间的最短依存路径； b) 两个实体的左右窗口。 命名实体标注特征使用斯坦福的NET进行命名实体标记（人名、地名、组织名和其他） NET是一个标记器，只是用来对实体进行标记的。 总结，论文中使用的特征不是单个特征，而是多种特征拼接起来的。有多个句子包含某实体对，可以从每个句子中抽取出词法特征、句法特征和实体特征，拼接起来，得到一个句子的特征向量，最后把多个句子的特征向量再拼接起来，得到某实体对（一个样本）的特征向量。 训练阶段假设freebase中存在（ Edwin Hubble，Marshfield，place of birth ）这一实例。 训练数据句子为“Astronomer Edwin Hubble was born in Marshfield, Missouri.” 1）首先使用NET将‘Edwin Hubble’标记为person，将‘Marshfield’标记为location. 2）实体Edwin Hubble和实体Marshfield是freebase关系之一的实例。所以对句子进行特征提取： 词汇特征与命名实体标注特征： 句法特征： 3）使用联合特征训练多类逻辑回归分类器。 测试阶段测试数据句子为“Galileo was born in Pisa.” 1）首先使用NET将‘Galileo’标记为person，将‘Pisa’标记为location. 2）对句子进行特征提取 3）判断‘Galileo’与‘Pisa’是place of birth 关系。 实现Text 文本对于非结构化文本，我们使用Freebase Wikipedia Extraction，它是所有维基百科文章（不包括讨论和用户页面）全文的转储，由Freebase（Metaweb，2008）的开发者Metaweb Technologies进行了句子标记。这个转储包含大约180万篇文章，平均每篇文章14.3个句子。单词总数（标点符号）为601,600,703。对于我们的实验，我们使用大约一半的文章：训练集80万和测试集40万。(留出法进行自动模型评估，一半实体用来训练，另一半实体用于对模型评估） 构造负样本由于有102种关系，每种关系都需要训练一个LR二分类器，所以需要构造负样本。这里的负样本不是其他101种关系的训练样本，而是从训练集中的句子中抽取实体时，如果实体不在Freebase中，那么就随机挑选这样的句子作为负样本。 解析和分块这个非结构化文本的每个句子都是由MINIPAR解析的依存关系来产生一个依存关系图。 训练和测试训练过程 LR分类器以实体对的特征向量作为输入，输出关系名和概率。每种关系训练一个二分类器，一共训练102个分类器。 训练好分类器，对测试集中的所有实体对的关系进行预测，并得到概率值。然后对所有实体对按概率降序排列，从中挑选出概率最高的N个实体对（概率值&gt;0.5)作为发现的新实体对。 测试方法 指标是准确率，方法采用留出法（自动评估）和人工评估两种方法。留出法的做法是，将Freebase中180万实体对的一半作为测试集（另一半用于训练）。新发现的N个实体对中，如果有n对在Freebase的测试集中，那么准确率为$\\frac nN$.人工评估则采用多数投票的方式。 模型评估结果表明远程监督是一种较好的方法。在文本特征的比较上，词法特征和句法特征拼接而成的特征向量，优于单独使用其中一种特征的情况。此外，句法特征在远程监督中比词法特征更有效，尤其对于依存句法结构比较短而实体对之间的词语非常多的句子。 评价这篇论文把远程监督的思想引入了关系抽取中，充分利用未标注的非结构化文本，从词法、句法和实体三方面构造特征，最后用留出法和人工校验两种方法进行模型评估，是一种非常完整规范的关系抽取范式。 优点 1.使用知识库提供训练数据来取代人工标注获取训练数据，没有过拟合的问题和领域依赖的问题，而且不需要很多已经标注好的数据，可以使用更大的数据集； 2.综合一种关系的多种描述instance：(Steven Spielberg, Saving Private Ryan)relation: film-director这个例子（可以参考前面架构一节的介绍） 缺点 1.基于给出的假设，训练集会产生大量的 wrong labels（噪声问题），比如两个实体有多种关系或者根本在这句话中没有任何关系。这样的训练数据会对关系抽取器产生影响。（例如，”Steve Jobs”, “Apple”在 Freebase 中存在 founder 的关系，如”Steven Jobs passed away the day before Apple unveiled iPhone 4s in late 2011.”句话中并没有表示出 Steven Jobs 与 Apple 之间存在 founder 的关系） 2.NLP 工具带来的误差，比如 实体标注器和依存句法解析器，中间过程出错会造成错误传播问题,也就是说越多的 feature engineering 就会带来越多的误差，在整个任务的 pipeline 上会产生误差的传播和积累，从而影响后续关系抽取的精度。 关于问题1中 wrong labels 的问题，有的工作将关系抽取定义为一个 Multi-instance Multi-label 学习问题，比如工作 Multi-instance Multi-label Learning for Relation Extraction，训练集中的每个 instance 都可能是一种 label。 而有的工作（本文的工作）则是将问题定义为 Multi-instance Single-label 问题，假设共现的 entity 对之间只存在一种关系或者没有关系，一组包括同一对 entities 的 instances 定义为一个 Bag，每一个 Bag 具有一个 label，最终训练的目标是优化 Bag Label 的准确率。第一种假设更加接近于实际情况，研究难度也相对更大一些。 关于问题2中的 pipeline 问题，用深度学习的思路来替代特征工程是一个非常自然的想法，用 word embedding 来表示句子中的 entity 和 word，用 RNN 或者 CNN 以及各种 RNN 和 CNN 的变种模型来对句子进行建模，将训练句子表示成一个 sentence vector，然后进行关系分类，近几年有几个工作都是类似的思路，比如：[3] Relation Classification via Convolutional Deep Neural Network [4] Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks [5] Neural Relation Extraction with Selective Attention over Instances [6] Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Deions 参考博客远程监督浅谈Freebase Data Dump结构初探一周论文 | 关于远程监督，我们来推荐几篇值得读的论文 关系抽取之远程监督算法"},{"title":"【leetCode】283.移动零","date":"2019-10-09T01:54:36.000Z","path":"2019/10/09/leetcode-283/","text":"283.移动零给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。 示例:&gt; 输入: [0,1,0,3,12] 输出: [1,3,12,0,0] 说明: 必须在原数组上操作，不能拷贝额外的数组。尽量减少操作次数。 题解分析题目要求把0移动到数组末尾，那么我们很自然就可以想到，可以将非0元素都移动到数组左端，剩下的就填0，那么怎么做呢？ 原始做法 - 使用辅助数组我们可以将原数组中非0的元素复制到新数组中，然后将新数组的元素赋值给旧数组。 123456789101112131415public void moveZeroes(int[] nums)&#123; if(nums == null || nums.length == 0) return; int[] newArray = new int[nums.length]; int index = 0; //step1 : 复制非0元素 for(int i = 0; i &lt; nums.length; i++)&#123; if(nums[i] != 0)&#123; newArray[index++] = nums[i]; &#125; &#125; //step2：更新原数组 for(int i = 0; i &lt; newArray.length; i++)&#123; nums[i] = newArray[i]; &#125;&#125; 复杂度分析：&gt; 时间复杂度：O(n)空间复杂度：O(n) 这种方法不符合题目的要求，因为使用了辅助数组。 改进1 - 将非0元素填充到前一个0的位置，最后用0填充后面的位置由上一种方法可以想到，我们可以遍历一遍数组的时候，用index记录0元素的位置，遇到非0元素就将它填充到前一个0的位置上，使得[0,index)区间内都是非0元素，[index,nums.length)区间用0填充。1234567891011121314public void moveZeroes(int[] nums)&#123; if(nums == null || nums.length == 0) return; int index = 0;//0元素初始位置 //step1:复制非0元素到前一个0元素位置 for(int i = 0; i &lt; nums.length; i++)&#123; if(nums[i] != 0)&#123; nums[index++] = nums[i]; &#125; &#125; //step2：将index之后的元素全部填充为0 while(index &lt; nums.length)&#123; nums[index++] = 0; &#125;&#125; 复杂度分析： 时间复杂度：O(n)空间复杂度：O(1) 考虑到最终还需要填充0，是否可以减少操作次数呢？ 改进2-将0元素与非0元素交换可以将0元素与非0元素交换，那么，就不存在最后一步填充0了。 这里还需要考虑到的一个极端情况是，数组没有0元素时，不管是改进1还是改进2都会对当前非0元素与自身进行复制/交换，可以通过添加一个小判断，减少操作的次数。 12345678910111213141516171819public void moveZeroes(int[] nums)&#123; if(nums == null || nums.length == 0) return; int index = 0; for(int i = 0; i &lt; nums.length;i++)&#123; if(nums[i] != 0)&#123;//非0元素 if(i != index)&#123;//如果i不等于index，才交换 swap(nums,i,index++); &#125;else&#123;//否则index++ index++; &#125; &#125; &#125;&#125;private void swap(int[] nums,int i, int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp;&#125; 复杂度分析： 时间复杂度：O(n)空间复杂度：O(1)"},{"title":"【关系抽取】相关概念-1","date":"2019-10-08T13:09:20.000Z","path":"2019/10/08/related-work-for-relations-extraction-adai/","text":"CIPS青工委学术专栏第3期 | 基于深度学习的关系抽取基于深度学习的关系抽取 https://blog.csdn.net/qq_35203425/article/details/81138978 现有主流的关系抽取技术分为有监督的学习方法、半监督的学习方法和无监督的学习方法三种： 1、有监督的学习方法将关系抽取任务当做分类问题，根据训练数据设计有效的特征，从而学习各种分类模型，然后使用训练好的分类器预测关系。该方法的问题在于需要大量的人工标注训练语料，而语料标注工作通常非常耗时耗力。 2、半监督的学习方法主要采用Bootstrapping进行关系抽取。对于要抽取的关系，该方法首先手工设定若干种子实例，然后迭代地从数据中抽取关系对应的关系模板和更多的实例。 3、无监督的学习方法假设拥有相同语义关系的实体对拥有相似的上下文信息。因此可以利用每个实体对对应上下文信息来代表该实体对的语义关系，并对所有实体对的语义关系进行聚类。 与其他两种方法相比，有监督的学习方法能够抽取更有效的特征，其准确率和召回率都更高。因此有监督的学习方法受到了越来越多学者的关注，本文也将重点介绍该类方法。 【基于有监督学习的关系抽取】 有监督的关系抽取系统通常需要大量人工标注的训练数据，从训练数据中自动学习关系对应的抽取模式。有监督关系抽取方法主要包括：基于核函数的方法[Zhao and Grishman 2005; Bunescu and Mooney 2006]，基于逻辑回归的方法[Kambhatla 2004]，基于句法解析增强的方法[Miller et al. 2000]和基于条件随机场的方法[Culotta et al. 2006]。然而，阻碍这些系统效果继续提升的主要问题在于，人工标注训练数据需要花费大量的时间和精力。 针对这个局限性，Mintz等人[Mintz et al. 2009]提出了远程监督（Distant Supervision）的思想。作者们将纽约时报新闻文本与大规模知识图谱Freebase（包含7300多个关系和超过9亿的实体）进行实体对齐。远程监督假设，一个同时包含两个实体的句子蕴含了该实体对在Freebase中的关系，并将该句子作为该实体对所对应关系的训练正例。作者在远程监督标注的数据上提取文本特征并训练关系分类模型，有效解决了关系抽取的标注数据规模问题。之后许多研究者从各个角度对远程监督技术提出了改进方案。例如Takamatsu等人[Takamatsu et al. 2012]改进了实体对齐的技术，降低了数据噪音，提高了关系抽取的总体效果。Yao等人[Yao et al. 2010]提出了基于无向图模型的关系抽取方法。Riedel等人[Riedel et al. 2010]则增强了远程监督的假设，与 [Mintz et al.2009]相比错误率减少了31%。 以上远程监督技术都假设一个实体对只对应一种关系。但是，很多实体之间具有多种关系。例如，“Steve Jobs founded Apple”和“Steve Jobs is the CEO of Apple”。因此，Hoffmann等人[Hoffmann et al. 2011]提出采用多实例多标签（Multi-Instance Multi-label）方法来对关系抽取进行建模，刻画一个实体对可能存在多种关系的情况。类似地，Surdeanu等人[Surdeanu et al. 2012]也提出利用多实例多标签和贝叶斯网络来进行关系抽取。 【基于深度学习的关系抽取】现有的有监督学习关系抽取方法已经取得了较好的效果，但它们严重依赖词性标注、句法解析等自然语言处理标注提供分类特征。而自然语言处理标注工具往往存在大量错误，这些错误将会在关系抽取系统中不断传播放大，最终影响关系抽取的效果。 最近，很多研究人员开始将深度学习的技术应用到关系抽取中。[Socher et al. 2012] 提出使用递归神经网络来解决关系抽取问题。该方法首先对句子进行句法解析，然后为句法树上的每个节点学习向量表示。通过递归神经网络，可以从句法树最低端的词向量开始，按照句子的句法结构迭代合并，最终得到该句子的向量表示，并用于关系分类。该方法能够有效地考虑句子的句法结构信息，但同时该方法无法很好地考虑两个实体在句子中的位置和语义信息。 [Zeng et al. 2014] 提出采用卷积神经网络进行关系抽取。他们采用词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。后来，[Santos et al. 2015]还提出了一种新的卷积神经网络进行关系抽取，其中采用了新的损失函数，能够有效地提高不同关系类别之间的区分性。 [Miwa et al. 2016] 提出了一种基于端到端神经网络的关系抽取模型。该模型使用双向LSTM（Long-Short Term Memory，长短时记忆模型）和树形LSTM同时对实体和句子进行建模。目前，基于卷积神经网络的方法在关系抽取的标准数据集SemEval-2010 Task 8上取得了最好的效果。 上面介绍的神经网络模型在人工标注的数据集上取得了巨大成功。然而，与之前基于特征的关系抽取系统类似，神经网络关系抽取模型也面临着人工标注数据较少的问题。对此，[Zeng et al. 2015]尝试将基于卷积神经网络的关系抽取模型扩展到远程监督数据上。[Zeng et al. 2015]假设每个实体对的所有句子中至少存在一个句子反映该实体对的关系，提出了一种新的学习框架：以实体对为单位，对于每个实体对只考虑最能反映其关系的那个句子。该方法在一定程度上解决了神经网络关系抽取模型在远程监督数据上的应用，在NYT10数据集上取得了远远高于基于特征的关系抽取模型的预测效果。但是，该方法仍然存在一定的缺陷：该模型对于每个实体对只能选用一个句子进行学习和预测，损失了来自其他大量的有效句子的信息。 我们有没有可能把实体对对应的有噪音的句子过滤掉，然后利用所有有效句子进行学习和预测呢？ [Lin et al. 2016] 提出了一种基于句子级别注意力机制的神经网络模型来解决这个问题，该方法能够根据特定关系为实体对的每个句子分配权重，通过不断学习能够使有效句子获得较高的权重，而有噪音的句子获得较小的权重。与之前的模型相比，该方法效果取得较大提升。我们也将相关代码发布在Github上：https://github.com/thunlp/NRE。 【总结及未来趋势】 近年来，深度学习在自然语言处理领域的很多方向取得了巨大成功，本文以关系抽取为例，介绍了如何利用深度学习的语义表示和学习能力，自动地从训练数据中学习分类特征，从而取得比传统方法更好的关系抽取效果。当然，关系抽取系统性能还有很大提升空间，仍然有很多问题亟待解决。 首先，基于句法树的树形LSTM神经网络模型在关系抽取上取得了不错的效果，这说明句法信息的引入对于关系抽取有一定帮助。然而，目前的句法分析仍然存在较多错误，在考虑句法信息的同时也引入了大量错误噪音。有研究表明，如果对于一个句子考虑其最可能的多个句法分析树，分析结果准确率可以得到较大提升。因此，一个重要的研究方向是，如何有效地将句子的多个可能句法树信息结合起来，用于关系抽取。 其次，目前的神经网络关系抽取主要用于预先设定好的关系集合。而面向开放领域的关系抽取，仍然是基于模板等比较传统的方法。因此，我们需要探索如何将神经网络引入开放领域的关系抽取，自动发现新的关系及其事实。此外，对现有神经网络模型如何对新增关系和样例进行快速学习也是值得探索的实用问题。 最后，目前关系抽取主要基于单语言文本。事实上，人类知识蕴藏于不同模态和类型的信息源中。我们需要探索如何利用多语言文本、图像和音频信息进行关系抽取。 ADai的Related-Work 关系抽取有两种框架，一种是pipeline方法，另一种是联合抽取方法（主要做的）。 传统的pipeline方法把抽取任务看作两个独立的抽取任务。首先利用然语言处理工具对文本中的实体进行识别，然后对实体之间的关系进行分类。 为了解决人工标注问题，神经模型在关系分类中得到了广泛的应用，包括卷积神经网络（CNN）、循环神经网络（RNN）和长期短期记忆网络（LSTM）。 mintz[5]首先提出了利用远程监控进行关系抽取。Zeng[6]提出了多实例学习来解决数据标注错误问题。近年来，强化学习也被应用于信息抽取领域。Qin[7]提出了一个强化学习框架，并重建了一个更为纯粹的训练集。此外，hoffmann[8]使用弱监督方法提取重叠关系。yu[9]提出了一个具有任意图形结构的联合判别概率模型来同时优化所有相关子任务。li[10]采用基于特征工程的结构化系统方法提取关系。yu[9]和li[10]应用非常复杂的特征工程方法提取关系。虽然pipeline方法使得每个提取组件更灵活，但下游结果容易受到上游任务的影响，导致错误的累积。 为了解决这一问题，人们提出了许多联合提取模型。miwa[11]采用了一种树形结构来联合提取实体和关系，并通过参数共享进行联合学习。Zheng[12]还分享了用于联合学习的神经网络的基本参数。Zheng[1]提出将关系抽取问题转化为序列标注问题，利用递归神经网络联合抽取实体和关系。但是，该模型仅在三元组没有重叠实体，并且具有重叠实体的三元组不能很好地处理的情况下使用。针对这一问题，Zeng[3]提出利用复制机制来解决重叠关系，将重叠实体复制到多个解码器中，解码器生成不同的三元组，但这种方法很大程度上依赖于训练数据的标注。该方法无法处理具有多个单词的实体。takanobu[13]使用强化学习来提取关系，并将关系提取任务视为半马尔可夫决策过程，通过高级强化学习（RL）过程检测关系，并通过低级rl过程来识别关系的参与实体。该模型在NYT数据集上取得了很好的效果。 在编码器的使用中，由于Bi-LSTM在语义提取方面的优越性能，大多数模型都采用Bi-LSTM进行编码。但是除了获取词义信息外，语义单元信息的提取也非常重要。多级扩展卷积[17]用于捕获词与词之间的局部和长期依赖关系，生成语义单元的表示，在文本情感分析中表现出优异的性能[2]。扩张卷积可以增加接收场，而不需要汇聚层，从而造成信息丢失，使得每个卷积的输出包含了大量的信息。扩展卷积，例如图像分割、语音合成[18]和机器翻译[16]，可以很好地应用于需要全局信息的图像处理任务或需要依赖长序列信息的语音和文本处理。 在上述的联合提取方法中，只有Zeng[3]和Takanobu[13]考虑重叠关系问题。在关系抽取中，Zeng[3]使用解码器直接解码并添加全连接层来获得关系类型，Takanobu[13]是基于强化学习来获得关系类型的。与这些方法不同的是，我们将关系的提取看作是一个多标签分类问题，并利用分类器链的方法来获取关系类型。并利用扩展卷积结合Bi-LSTM对文本进行编码，更充分地提取文本中的语义。 论文整理Relation Classification via Convolutional Deep Neural Network（2014） 远程监督pipeline方法 远程监督： Distant supervision for relation extraction without labeled data （2009） 本文提出不依赖人工标注，将远程监督应用在关系抽取中。使用Freebase提供远程监督。训练阶段一开始是NET标记实体，然后拿这两个实体去freebase中找，找到了就提取特征（词法特征：词性标签是由宾夕法尼亚大学训练的最大熵标记器分配，然后简化为7类，句法特征：依存句法解析器MINIPAR)，然后再把这些特征扔到LR里去训练分类器。 wrong label问题：基于给定的假设，训练集会产生大量wrong label产生一个改进方向：多实例多标签。 误差传播问题：在特征提取的过程中由于使用了NLP工具，工具在标注和解析中产生的误差会影响到特征的准确性，也就是说会产生误差，造成误差传播问题。产生一个改进方向：用深度学习思路代替特征工程，也就是用神经网络模型来做特征的提取。 多实例-多标签上面的远程监督假设一对实体对应一种关系，但是，很多实体之间具有多种关系。因此这两篇采用多实例-多标签进行关系抽取： Knowledge-Based Weak Supervision for Information Extraction（2011） 使用弱监督方法处理重叠关系。引入多实例学习的概率图模型，可用来处理重叠关系。 Multi-instance Multi-label Learning for Relation Extraction（2012） 利用多实例多标签和贝叶斯网络来进行关系抽取。"},{"title":"【知识图谱】基础-1","date":"2019-10-08T03:06:51.000Z","path":"2019/10/08/KG-basic/","text":"什么是知识图谱？学术角度：知识图谱本质上是语义网络（Semantic Network）的知识库。 实际应用角度：多关系图（Multi-relational Graph）。 什么是多关系图？包含多种类型的节点和多种类型的边。 在知识图谱里，我们通常用“实体（Entity）”来表达图里的节点、用“关系（Relation）”来表达图里的“边”。实体指的是现实世界中的事物比如人、地名、概念、药物、公司等，关系则用来表达不同实体之间的某种联系。 知识图谱的表示知识图谱应用的前提是已经构建好了知识图谱，也可以把它认为是一个知识库。 属性图：当一个知识图谱拥有属性时，我们可以用属性图（Property Graph）来表示。 RDF：由很多的三元组（Triples）来组成。RDF 在设计上的主要特点是易于发布和分享数据，但不支持实体或关系拥有属性，如果非要加上属性，则在设计上需要做一些修改。 目前来看，RDF 主要还是用于学术的场景，在工业界我们更多的还是采用图数据库（比如用来存储属性图）的方式。 知识抽取知识图谱的构建是后续应用的基础，而且构建的前提是需要把数据从不同的数据源中抽取出来。对于垂直领域的知识图谱来说，它们的数据源主要来自两种渠道：一种是业务本身的数据，这部分数据通常包含在公司内的数据库表并以结构化的方式存储；另一种是网络上公开、抓取的数据，这些数据通常是以网页的形式存在所以是非结构化的数据。 前者一般只需要简单预处理即可以作为后续 AI 系统的输入，但后者一般需要借助于自然语言处理等技术来提取出结构化信息。比如在上面的搜索例子里，Bill Gates 和 Malinda Gate 的关系就可以从非结构化数据中提炼出来，比如维基百科等数据源。 信息抽取的难点在于处理非结构化数据. 在构建类似的图谱过程当中，主要涉及以下几个方面的自然语言处理技术： a. 实体命名识别（Name Entity Recognition） b. 关系抽取（Relation Extraction） c. 实体统一（Entity Resolution） d. 指代消解（Coreference Resolution） 在下面的图中，我们给出了一个实例。左边是一段非结构化的英文文本，右边是从这些文本中抽取出来的实体和关系。下面针对每一项技术解决的问题做简单的描述，以至于这些是具体怎么实现的，不在这里一一展开. 首先是实体命名识别，就是从文本里提取出实体并对每个实体做分类(打标签)：比如从上述文本里，我们可以提取出实体 -“NYC”，并标记实体类型为 “Location”；我们也可以从中提取出“Virgil’s BBQ”，并标记实体类型为“Restarant”。这种过程称之为实体命名识别，这是一项相对比较成熟的技术，有一些现成的工具可以用来做这件事情。 其次，我们可以通过关系抽取技术，把实体间的关系从文本中提取出来，比如实体“hotel”和“Hilton property”之间的关系为“in”；“hotel”和“Time Square”的关系为“near”等等。 另外，在实体命名识别和关系抽取过程中，有两个比较棘手的问题：一个是实体统一，也就是说有些实体写法上不一样，但其实是指向同一个实体。比如“NYC”和“New York”表面上是不同的字符串，但其实指的都是纽约这个城市，需要合并。实体统一不仅可以减少实体的种类，也可以降低图谱的稀疏性（Sparsity）. 另一个问题是指代消解，也是文本中出现的“it”, “he”, “she”这些词到底指向哪个实体，比如在本文里两个被标记出来的“it”都指向“hotel”这个实体。 实体统一和指代消解问题相对于前两个问题更具有挑战性 知识图谱的存储知识图谱主要有两种存储方式：一种是基于 RDF 的存储；另一种是基于图数据库的存储。 RDF 一个重要的设计原则是数据的易发布以及共享，图数据库则把重点放在了高效的图查询和搜索上。其次，RDF 以三元组的方式来存储数据而且不包含属性信息，但图数据库一般以属性图为基本的表示形式，所以实体和关系可以包含属性，这就意味着更容易表达现实的业务场景。 它们之间的区别如下图所示。 这里列出了常用的图数据库系统以及他们最新使用情况的排名。 其中 Neo4j 系统目前仍是使用率最高的图数据库，它拥有活跃的社区，而且系统本身的查询效率高，但唯一的不足就是不支持准分布式。 相反，OrientDB 和 JanusGraph（原 Titan）支持分布式，但这些系统相对较新，社区不如 Neo4j 活跃，这也就意味着使用过程当中不可避免地会遇到一些刺手的问题。如果选择使用 RDF 的存储系统，Jena 或许一个比较不错的选择。 知识图谱的体系架构"},{"title":"【移动Web】知识点-1","date":"2019-09-29T01:37:25.000Z","path":"2019/09/29/2019-9-29-mobile-web-note-1/","text":"移动Web知识点Viewport什么是css像素，物理像素？ 手机打开PC页面刚好被等比例缩放？移动设备视窗概念 layout viewport（布局视窗）：浏览器初始视窗大小和浏览器厂商有关 Visual viewport（物理视窗）：可视范围的大小 Ideal viewport（理想视窗）：没有固定尺寸，不同设备的ideal viewport不同，所有iphone的ideal viewport都是320px，只要把某个个元素的宽度设为ideal viewport的宽度，那么这个元素的宽度就是屏幕的宽度。意义在于，无论何种分辨率的屏幕，针对ideal viewport设计的网站，不需要手动缩放，也不需要横向滚动条，都可以完美呈现。 等比例缩放是一种浏览器的特性，将页面缩到visual viewport的可视区域内。这个特性实际上有不好的地方，因为图片、文字都会缩小，使用手机浏览是非常不方便的。 设备宽高和viewport有什么关系？iphone、ipad以及IE会横竖屏不分，通通以竖屏的宽度为ideal viewport的宽度。 如何使用 meta设置viewport？如何设置ideal viewport？&lt;meta name = &quot;viewport&quot; content=&quot;width=device-width,initial-scale=1.0, maximum-scale=1.0, user-scalable=0&quot;/&gt;这个meta标签的作用是，让当前viewport宽度等于设备宽度(ideal viewport的宽度)，同时不允许用户手动缩放。 meta viewport首先是苹果在safari引入的，在苹果的规范中，meta viewport有6个属性（content中）： width：设置layout viewport的宽度，正整数，或着字符串“width-device” initial-scale：页面的初始缩放值，数字，可以是小数。 minimum-scale：允许用户的最小缩放值，数字，可以是小数。 maximum-scale：允许用户的最大缩放值，数字，可以是小数。 heigh：设置layout viewport 的高度，这个属性并不重要，很少使用。 user-scalable：是否允许用户进行缩放，值“no”或“yes”，0或1. 在Android中还支持target-densitydpi这个私有属性，表示目标设备的密度等级，作用是决定css中的1px代表多少物理像素 target-densitypdi：可以是数值，或者字符串:”high-dpi”,”medium-dpi”,”low-dpi”,”device-dpi”中的一个。 当配置`target-densitypid=device-dpi”时，css中的1px会等于物理像素的1px。 因为只有android支持且Android决定放弃这个属性了，所以我们不要使用。 initial-scale &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1.0&quot;&gt; 也可以把当前viewport变为ideal viewport。设置为1说明梅索放，但却达到了ideal viewport的效果。说明缩放是相对于ideal viewport来进行缩放的 如果width和initial-scale = 1同时出现，并且冲突怎么办？ 假如width=400px，那么浏览器会取他们中的较大值。 最后，总结一下，要把当前的viewport宽度设为ideal viewport的宽度，既可以设置width=device-width，也可以设置 initial-scale=1，但这两者各有一个小缺陷，就是iphone、ipad以及IE 会横竖屏不分，通通以竖屏的ideal viewport宽度为准。所以，最完美的写法应该是，两者都写上去，这样就 initial-scale=1 解决了 iphone、ipad的毛病，width=device-width则解决了IE的毛病：&lt;meta name = &quot;viewport&quot; content=&quot;width=device-width,initial-scale=1.0, maximum-scale=1.0, user-scalable=0&quot;/&gt; 流式布局主要是这个属性：display:flex写在移动端：display:-webkit-flex; 这是CSS3中的属性。 控制子元素横着排列还是竖着排列 flex-direction:row|row-reverse|column|column-reverse; 换行，wrap强制换行,nowrap强制不换行，wrap-reverse flex-wrap:nowrap|wrap|wrap-reverse; justify-content:flex-start|flex-end|center|space-between|space-around; align-items:flex-start|flex-end|center|baseline|stretch; align-self:auto|flex-start|flex-end|center|baseline|stretch;"},{"title":"【LeetCode】 142.环形链表2","date":"2019-09-28T12:36:12.000Z","path":"2019/09/28/leetcode-cycle-list-2/","text":"142.环形链表2给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 说明：不允许修改给定的链表。 示例 1：输入：head = [3,2,0,-4], pos = 1输出：tail connects to node index 1解释：链表中有一个环，其尾部连接到第二个节点。 题解分析141题是判断链表是否有环，142题是判断入环的第一个节点。 判断链表有环的方法是：快慢指针。 那么对于入环的第一个节点怎么去找？ 假设从链表头部走到环的入口需要F步 假设两个指针相遇的时候，慢指针走了 a 步，快指针走了 a + b + a 步 由快慢的定义，我们知道f = 2s，因为快指针每次都走2步，慢指针走1步，快指针走了慢指针的两倍路程。 那么可以推出：2 (F + a) = F + a + b + aF = b 说明，入环之前这一段路程和b相同，那就意味着，我们用一个指针从头走F步，另一个指针从第一次相遇处走F步，当他们再次相遇时，说明这个点就是入口。 步骤1.快慢指针找到第一次相遇的位置2.快指针fast重新从head开始,slow继续从第一次相遇处，fast 和 slow 每次都只走一步，相遇时的位置就是环开始的位置。返回slow或者fast即可。 编码123456789101112131415161718public ListNode detectCycle(ListNode head) &#123; if(head==null || head.next == null) return null; ListNode fast = head; ListNode slow = head; while(true)&#123; if(fast == null || fast.next == null) return null; fast = fast.next.next; slow = slow.next; if(fast == slow) break; &#125; fast = head; while(fast != slow)&#123; fast = fast.next; slow = slow.next; &#125; return fast; &#125;"},{"title":"【MySQL】MySQL总结","date":"2019-09-28T10:52:22.000Z","path":"2019/09/28/Mysql-Summary/","text":"事务1.什么是事务？ 满足ACID特性的一组操作。 2.自动提交 MySQL中的事务都是自动提交的，如果不显式地使用START TRANSACTION来开启事务，那么每一个查询都会被当作一个事务。 3.事务的特性(ACID) 原子性：Atomicity，事务是一个不可再分的工作单位，事务中的操作要么都执行，要么都不执行。 一致性：Consistency，事务总是从一个一致状态转移到另一个一致状态。并发执行的事务，其结果和串行执行的结果一致（正确），那么就说这个并发事务符合一致性。 隔离性：Isolation，事务所做的修改在提交之前，对其他事务是不可见的。 持久性：Durability，事务所做的修改一旦提交，就会永久的保存在数据库中。 4.并发事务存在的问题 1.丢失更新：两个事务读取同一事务后，先后进行修改（或回滚）导致第一个提交的事务数据丢失。 2.脏读：事务读取到其他事务未提交的数据 3.不可重复读：事务A读取数据x但还没提交，事务B修改了X或删除了X并提交，事务A再次读取数据时，两次得到的结果不一致。针对update 和 delete。 4.幻读：事务A读取x = 100的数据得到了10条结果，事务B增加了一条x = 100的数据并提交，事务A再次读取时会得到11条结果，导致前后不一致。针对insert。 5.事务的隔离级别 1.未提交读 Read-Uncommited：事务中的修改即使没有提交也会被其他事务可见。(导致上述1、2、3问题) 2.提交读 Read-Commited ： 事务只会看到已提交事务做出的修改。（导致2，3） 3.可重复读 Repeatable-Read : 一个事务多次读取相同内容得到结果一致，也就是说，一个事务不会读取另一个事务修改但未提交的数据。（能解决2，不能解决3）。是MySQL默认的隔离级别。 MySQL通过MVCC（快照读） + next-key locking (当前读）解决了幻读的问题。 4.可串行化 Serializable : 强制事务串行执行。最高级别。 6.为什么MySQL默认隔离级别是RR？ MySQL在5.0之前binlog只有statement一种格式，这个格式在RC下，会出现主从不一致的情况，因此默认隔离级别是RR. 1.什么是binlog binlog记录所有数据库表结构变更（例如CREATE、ALTER TABLE…）以及表数据修改（INSERT、UPDATE、DELETE…）的二进制日志。binlog不会记录SELECT和SHOW这类操作，因为这类操作对数据本身并没有修改，但你可以通过查询通用日志来查看MySQL执行过的所有语句。 2.binlog的几种格式 statement：记录的是修改SQL语句row：记录每行实际数据变更mixed：上面两种的混合。 当binlog为statement格式，数据库隔离级别在RC情况下，在master节点先delete后insert，binlog里记录的是先insert后delete(按提交顺序,session2先提交)，slave同步的是binlog，因此从机执行的顺序和主机不一致！就会出现主从不一致。 3.如何解决？ 1)设置为RR,加入间隙锁，session2插入时会阻塞。 2)使用binlog的row格式，这个格式5.1才有。 因此MySQL的默认隔离级别是RR。 7.那既然说InnoDB通过MVCC解决了幻读，什么是MVCC？ 多版本并发控制。 通过为每一行数据保存两个隐藏的字段（创建时间，删除时间），使得大部分读操作都不需要加锁。 实际上保存的是版本号，每个事物开始都有一个版本号，递增。 SELECT InnoDB会根据以下两个条件检查每一行 行的创建版本号早于当前版本号，这样保证事物读取的数据是在事务开始之前就存在的，或者事务自己创建的。行的删除版本号大于当前版本号，或者未定义，这样保证事物发生时该行还没有被删除（修改）。 INSERT 为创建的行添加当前事务的版本号到行的创建版本号。 DELETE 为被删除的行添加当前事务的版本号到删除版本号。 UPDATE 为旧的数据添加当前版本号到删除版本号。 创建一行新纪录，保存当前版本号到创建版本号。 8.next-key locking 算法 next-key locking是一个行锁的算法，除他以外InnoDB还有两个行锁的算法，他们分别是： 1.Record Lock：锁直接加在索引记录上，锁住的是key 2.Gap Lock:间隙锁，锁定索引记录的间隙（不包括记录本身），保证间隙不变，它是针对隔离级别为RR及以上的事务来说的。 3.Next-key locking：上面这两个家伙组合起来就是next-key locking。锁定间隙且锁定记录本身。 之前说，InnoDB通过MVCC解决了幻读的问题，但他是在快照读的层面实现的（简单的select），next-key locking是在当前读层面来实现的，而且默认情况下是针对RR级别的。 在使用next-key locking的时候，当InnoDB扫描索引记录时，会先对索引加上行锁（record lock）然后在索引两边的间隙加上间隙锁（gap lock），这样就不能在间隙的位置进行修改或插入，从而防止幻读的发生。 9.锁的降级 当索引含有唯一属性时，Next-key Locking 会自动降级为Record Lock 用来减少锁定的范围，加大并发的处理速度。但是此种情况只存在于查询【所有的唯一索引列】。如果，唯一索引由多个列组成，而查询是查找多个唯一索引列中的其中一个，那么这种查询由于联合索引的特性，查询是一个范围查询，而不是点查询，所以不会降级处理。 索引1.设计索引的原则 1.最适合的索引列出现在where子句中，或者连接子句指定的列。 2.使用唯一索引。 3.使用短索引。对于varchar类型的字段，使用前缀索引效率会高。可以指定一个前缀长度，在这个长度内是唯一的。 4.利用最左前缀。 5.不要过度索引，不要建立太多的索引。 6.考虑列上进行比较的类型，索引可以用于&lt;、&lt;=、=、&gt;=、&gt;和between运算，模式具有一个直接前缀的时候可以用于like运算。 7.索引列的顺序非常重要！选择性最高的列放在最前面，选择性：不重复的索引值/总的记录数 8.索引列不能是表达式的一部分，也不能是函数的参数。 9.覆盖索引，可以充分利用二级索引中的主键值来覆盖查询 10.索引和锁： 索引可以让查询锁定更少的行 即使使用索引也有可能锁住不需要的数据，例如索引为 查询为where id &lt; 5 and id&lt;&gt;1 for update，虽然不需要id=1的数据，但mysql执行计划是索引范围扫描，因此&lt;5的都锁住了。 InnoDB在二级索引上使用共享锁，访问主键需要排他锁，并且使用select for update比lock in share mode或非锁定查询要慢 2.什么情况不会用到索引？ 1.当MySQL估计全表比索引快时，不用索引。 2.当跳过索引中的列。 当索引为&lt;a,b,c&gt; 查询where a = 10 and c= 100，就只会用到索引的第一列。 3.违反最左前缀。当索引为&lt;a,b,c&gt;时，where b = 100 或 where b = 100 and c = 100 或where c = 100都不会用到索引。 4.查询中有某个列的范围查询，则这个列的右边都无法用到索引。当索引为&lt;a,b,c&gt;时，查询where a = 10 and b like “bbv%” and c = 98，这个查询只用到索引的前两列&lt;a,b&gt;。 5.前导的模糊查询不会用到索引，where a like “%as”不会，但where a like “as%”可以用。 6.使用聚合函数索引会失效。 7.列类型是字符串，查询条件未加引号。 8.在查询条件中使用OR会失效，可以为每个or的字段都加索引（不太好吧） 3.InnoDB的聚簇索引是什么？优点和缺点是什么？ 更多：https://www.jianshu.com/p/fa8192853184 聚簇索引是InnoDB存储数据的方式，在同一个结构中保存了索引和数据行，数据存放在索引叶子页中。 InnoDB通过主键聚集数据。 优点： 1.可以把相关的数据保存在一起。 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问，不必访问磁盘。这样主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回了，如果按照主键Id来组织数据，获得数据更快。 2.数据访问更快，因为索引和数据是存放在一起的。（只需要一次查询） 3.使用覆盖索引扫描可以直接使用主键值。 缺点： 1.插入速度严重依赖于查询顺序，使用自增的主键是最快的插入方式。 2.更新代价很高，会强制被更新的行移动到新的位置。 3.导致页分裂。当主键值要求必须在已满（16K）的页中插入一条数据，会导致页分裂，页分裂会占用更多的空间。 4.二级索引可能会比想象的更大，因为二级索引中包含了主键，如果主键很大，那么其他索引都会很大。 5.二级索引的查找需要两次索引查找。自适应哈希索引能减少这样的重复工作。 自适应哈希索引：当InnoDB注意到某些索引值被频繁使用时，他会在内存中基于B+Tree索引之上再创建一个哈希索引。这是一个自动的内部行为，用户无法控制或配置，如果有必要也可以关闭这个功能。 4.二级索引的好处？ 二级索引存储了主键的值，虽然会消耗更多的空间，但是这样会减少出现页分裂时二级索引的维护，InnoDB在移动行时无需更新二级索引的id。 5.覆盖索引 优秀的索引应该考虑到整个查询而不单单只是where部分。如果一个索引能覆盖所有需要查询的字段（包括select部分的）那么就称之为覆盖索引。覆盖索引能极大提高性能，索引的条目通常远小于数据的数量，如果只需要读取索引那就能极大减少访问量。 由于聚簇索引，覆盖索引对InnoDB的表特别有用。InnoDB的二级索引在叶子结点保存了主键的值，如果二级索引能覆盖查询则可以避免对主键索引的二次访问。 explain的Extra列可以看到Using index，说明这个查询是索引覆盖查询。 6.哈希索引 InnoDB 引擎有一个特殊的功能叫 “自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。 哈希索引能以 O(1) 时间进行查找，但是失去了有序性，它具有以下限制： 1.无法用于排序与分组； 2.只支持精确查找，无法用于部分查找和范围查找 7.索引的优点 创建唯一性索引，保证数据库表中每一行数据的唯一性 大大加快数据的检索速度，这是创建索引的最主要的原因 加速数据库表之间的连接，特别是在实现数据的参考完整性方面特别有意义 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间 通过使用索引，可以在查询中使用优化隐藏器，提高系统的性能 8.索引的缺点 -创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加-索引需要占用物理空间，除了数据表占用数据空间之外，每一个索引还要占一定的物理空间，如果建立聚簇索引，那么需要的空间就会更大-当对表中的数据进行增加、删除和修改的时候，索引也需要维护，降低数据维护的速度 9.为什么MySQL的索引要使用B+树而不是其它树形结构?比如B树？ 因为B树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少。 指针少的情况下要保存大量数据，只能增加树的高度，导致IO操作变多，查询性能变低； 锁1.快照读和当前读 快照读：简单的select操作属于快照读，不加锁（不包括select … for update,select … lock in share mode) 例如：select * from table where… 当前读：特殊的select，insert/update/delete操作，属于当前读，要加锁。 例如： select … for update select…lock in share mode insert… update… delete… 在这些操作中，都需要加锁，select … lock in share mode加的是共享锁（S），其他都是排他锁（X） 2.共享锁和排他锁 共享锁：又叫做读锁，读锁是共享的，用户可以并发读，但不能修改，也就是说，事务A对数据加上共享锁后，别的事务也可以对数据加共享锁，但不能加排他锁。 select…lock in share mode 在select语句后面加上lock in share mode，MySQL对结果的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。 排他锁：又叫写锁，写锁会阻塞其他的读锁和写锁，也就是当数据被加上排他锁后，再也无法对这个数据加任何锁，直到释放。 select … for update 在select语句后面加上for update时，MySQL会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。 3.意向锁 意向锁：意向锁是表级锁，其设计目的主要是为了在一个事务中揭示下一行将要被请求锁的类型。 解释之前先说一下，意向锁是InnoDB自动加的，不需要用户干预，而共享锁和排他锁可以用户自己加（for update和lock in share mode） 假设事务A对数据表的某一行加了读锁，那么其他事务就只能读，不能写，也就是不能加排他锁。 此时有一个事务B对表申请一个表锁，如果他申请成功，那么也就意味着事务B对整个表拥有了写的权利，那事务A锁定的那一行不是矛盾了吗？ 数据库要避免冲突，那么就需要检查这个表是否被其他事务加了表锁，要是没加，就还得检查这个表的每一行是不是被锁住，这样就需要遍历整个表，效率不高，所以就有了意向锁。 有了意向锁，事务A在给表的某一行加锁前，需要先申请一个意向共享锁，成功后，再对这一行加行锁。 那么事务B再加锁前就需要先判断表上是否加了意向共享锁，如果有，就说明有些行被加了共享锁，这时事务B就阻塞了。 所以这么看来，意向锁的意思就可以这么表达：告诉事务这个表上有其他事务加了共享锁或排他锁。 加了意向共享锁（IS）的表，还可以被其他事务加IS，但被共享锁锁定的那些记录是不能被修改的。 加了意向排他锁（IX）的表，就不能被加任何共享锁和排他锁了。 意向锁是个表级的锁，共享锁和排他锁可以是行级也可以是表级，IX和IS只会和表级的X和S冲突，不会和行级的冲突。 意向锁实现了表锁和行锁的共存（举的那个例子就说明行锁和表锁共存时会有冲突） 并存的概念是指数据库同时支持表、行锁，而不是任何情况都支持一个表中同时有一个事务A持有行锁、又有一个事务B持有表锁，因为表一旦被上了一个表级的写锁，肯定不能再上一个行级的锁。 4.表锁和行锁 刚说S和X锁的时候我们提到了表锁和行锁，现在来解释一下： 表锁（table lock）：是MySQL中最基本的锁策略，且开销最小，他会锁定整张表 行锁（row lock）：给某一行加锁，行锁可以最大程度支持并发处理（同时带来最大的锁开销），行锁只在存储引擎层实现，而MySQL服务器层没有实现。 表锁和行锁是从锁的粒度来分的，X和S是从类型来分的，所以我们之前说，可以有行级的S和X锁，也可以有表级的S和X锁。 存储引擎1.InnoDB InnoDB是MySQL默认事务型引擎。 InnoDB采用MVCC来支持高并发，并且实现了四个标准的隔离级别。默认级别是RR，并通过间隙锁（next-key locking）策略防止幻读的出现。 InnoDB是基于聚簇索引建立的。 调优1.慢查询 查询性能低下的最基本原因是访问的数据太多。 分析步骤： 1.确认应用程序是否在检索大量不需要的数据。 2.确认MySQL服务器层时候在分析大量超过需要的行。 典型&lt;检索不需要数据&gt;情况： 1.SELECT * 每次看到这个的时候都需要想想，是不是真的需要表里全部的数据。 我们那个查找下载论文学生的查询，就SELECT * FROM STUDENT。 2.重复查询相同的数据 有的查询总是返回相同的结果，可以考虑用缓存。 2.向MySQL发送一个请求，MySQL到底做了什么？ 1.客户端发送一条查询给服务器 2.服务器先检查查询缓存，如果命中缓存，则立刻返回缓存中的结果。否则进入下一阶段。 解析SQL语句之前，如果查询缓存开启，则会优先检查这个缓存，是通过对大小写敏感的哈希查找实现的。如果命中，则在返回结果前会再检查一下用户权限，如果权限没有问题，则会跳过后面的阶段直接返回结果。 3.服务器进行SQL解析、预处理，再由查询优化器生成对应的执行计划。 SQL解析：通过关键字进行SQL语句解析，生成一颗“解析树”。预处理器：根据MySQL规则进一步检查解析树是否合法。（表和列是否存在，名字和别名是否有歧义）查询优化器：一条查询可以有很多种执行方式，优化器就是找到最好的执行计划。 4.MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询。 5.返回结果导到客户端。 3.EXPLAIN 分析查询 EXPLAIN可以对SELECT查询进行分析，所有字段如下： id：SELECT查询的标识符，每个SELECT都会分配一个唯一的标识符 select_type：SELECT查询的类型 table：查询的是哪个表 partitions：匹配的分区 type：mysql找到需要的数据行的方式 possible_keys：可能用到的索引 key：用到的索引 key_len：用到的索引的长度。 ref：哪个字段或常数和key一起使用。 rows：查询一共扫描了多少行，估计值。 filtered：此查询条件所过滤数据的百分比。 Extra：额外的信息。 -select_type - 查询类型 常用的取值有： SIMPLE, 表示此查询不包含 UNION 查询或子查询 （最常见） PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. type - MySQL找到需要数据行的方式 常取的值： const：针对主键或唯一索引的等值查询扫描, 最多只返回一行数据,const 查询速度非常快, 因为它仅仅读取一次即可。 eq_ref：此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 = , 查询效率较高。 ref：此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询. range：表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中。 index：表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据。index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index。 ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. ALL &lt; index &lt; range ~ &gt;index_merge &lt; ref &lt; eq_ref &lt; const &lt; system possible_keys - 可能用到的索引 表示能用的索引，并不代表真的用了，key表示用到的索引。 key - 使用的索引 查询使用的索引 key_len 使用索引的长度 表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n 字节长度 varchar(n): 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. rows - 查询需要扫描的行数 MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. Extra - 额外信息 Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. 连接1.连接的种类1.外连接左连接 left join (left outer join)结果包括左表的所有行，如果右表有无法匹配的行则显示null。右连接 right join (right outer join)包括右表所有的行，如果左表又无法匹配的行，则显示null。 全连接 full join (full outer join)左表和右表其中一个表中存在匹配，则返回行。 2.内连接：内联接是用比较运算符比较要连接列的值的联接join 或 inner join 其他1.数据库中的范式 1NF：属性不可分 2NF：属性完全依赖于主键 [消除部分子函数依赖] 3NF：属性不依赖于其它非主属性 [消除传递依赖] 2.char 和 varchar的区别 char是定长的，varchar是变长的。 varchar在修改的时候可能会比原来长，原页面无法容纳时可能会导致分页。 varchar会保留字符串末尾的空格，char不保留。 3.分页查询怎么写？ select * from employee limit 3, 7; // 返回4-11行 select * from employee limit 3,1; // 返回第4行 SELECT * FROM message ORDER BY id DESC LIMIT 10000, 20 每次都要扫描10020行，然后只要20条记录，性能是不好的。 怎么写效率更好呢？ 如果只需要上一页和下一页，可以这么写： select * from message where id &gt; 10020 order by id desc limit 20;//下一页select * from message where id &lt; 10000 order by id desc limit 20;//上一页 这样就只需要扫描20行。 上一页 1 2 3 4 5 6 7 8 9 下一页 怎么实现呢？ 还是按照SELECT * FROM message ORDER BY id DESC，按id降序分页，每页20条 假设当前是第10页，当前页条目id最大的是2519，最小的是2500; 当前第10页的sql可以这么写: SELECT * FROM tb_goods_info WHERE auto_id &gt;=2500 ORDER BY auto_id ASC LIMIT 0,20 第9页： SELECT * FROM tb_goods_info WHERE auto_id &lt;2500 ORDER BY auto_id desc LIMIT 0,20 第8页： SELECT * FROM tb_goods_info WHERE auto_id &lt;2500 ORDER BY auto_id desc LIMIT 20,20 第11页： SELECT * FROM tb_goods_info WHERE auto_id &gt;2519 ORDER BY auto_id asc LIMIT 0,20 原理还是一样，记录住当前页id的最大值和最小值，计算跳转页面和当前页相对偏移，由于页面相近，这个偏移量不会很大，这样的话m值相对较小，大大减少扫描的行数。其实传统的limit m,n，相对的偏移一直是第一页，这样的话越翻到后面，效率越差，而上面给出的方法就没有这样的问题。 注意SQL语句里面的ASC和DESC，如果是ASC取出来的结果，显示的时候记得倒置一下。 4.数据库字段类型为什么建议尽量避免NULL 高性能MySQL中明确提到“尽量避免NULL” 原因： NOT IN、!= 等负向条件查询在有 NULL 值的情况下返回非空行的结果&gt;集。Count(*)会统计包括NULL的行，count(colName)不会统计此列为null的行NULL 列需要更多的存储空间，需要一个额外的字节作为判断是否为 NULL 的标志位。"},{"title":"【日记】不知道纪念什么的纪念日记","date":"2019-09-28T08:12:59.000Z","path":"2019/09/28/four/","text":"1当事情发生的时候，没有人会问你：“嗨，准备好了吗？” 而是就那样，发生了。 对工作是这样，对学业是这样，对感情也是这样。 2时常反省自己，做一个温柔的人，做一个认真的人，做一个努力生活的人。 虽然生活最近对我不太好，但我还是愿意热烈的爱着它。 我喜欢一个人走路，喜欢一个人吃饭。 我想谈一场简简单单的恋爱。 这些都很难吗？ 不知道。 尽人事，听天命吧。 3他日重逢，要等来生。 4《我用什么才能留住你》博尔赫斯 我用什么才能留住你？我给你瘦落的街道、绝望的落日、荒郊的月亮。我给你一个久久地望着孤月的人的悲哀。 我给你我已死去的祖辈，后人们用大理石祭奠的先魂：我父亲的父亲，阵亡于布宜诺斯艾利斯的边境，两颗子弹射穿了他的胸膛，死的时候蓄着胡子，尸体被士兵们用牛皮裹起； 我母亲的祖父——那年才二十四岁——在秘鲁率领三百人冲锋，如今都成了消失的马背上的亡魂。 我给你我的书中所能蕴含的一切悟力，以及我生活中所能有的男子气概和幽默。我给你一个从未有过信仰的人的忠诚。 我给你我设法保全的我自己的核心——不营字造句，不和梦交易，不被时间、欢乐和逆境触动的核心。 我给你早在你出生前多年的一个傍晚看到的一朵黄玫瑰的记忆。我给你关于你生命的诠释，关于你自己的理论，你的真实而惊人的存在。 我给你我的寂寞、我的黑暗、我心的饥渴；我试图用困惑、危险、失败来打动你。 5《如果我不曾见过太阳》狄金森 我本可忍受黑暗 如果我不曾见过太阳 然而阳光却把我的荒凉 照耀的更加荒凉 当你拥有过幸福，那么任何一点点的苦难都会变得分外强烈。"},{"title":"Hello World","date":"2019-09-28T02:09:06.924Z","path":"2019/09/28/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"}]